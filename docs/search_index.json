[["index.html", "3 Multivariate 1 Prerequisites", " 3 Multivariate Kim Cuddington 05/07/2021 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["intro.html", "2 Introduction", " 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2021) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],["literature.html", "3 Literature", " 3 Literature Here is a review of existing methods. "],["introduction-to-git-and-github.html", "4 Introduction to Git and GitHub 4.1 Motivation 4.2 Getting set up for the first time 4.3 Using Git and GitHub 4.4 Beyond the basics", " 4 Introduction to Git and GitHub 4.1 Motivation As a biology graduate student or a professional biologist in a university or government setting, why might you want to use Git and GitHub? And what are Git and GitHub anyway? Scientists (including students) are working far more collaboratively than in the past This involves both sharing code and writing up results There is a push towards open science – including your code as part of a scientific paper We have called this a TTT approach: Transparent – a clear and open way to show data, code, and results, enabling reproducibility Traceable – a clear link from database queries and code to final results (numbers, tables, and graphs in a document) Transferable – it should be feasible for another person to reproduce work and build upon it with a minimal learning curve Using Git and GitHub in your workflow greatly enables this, both when working alone and in a team. Git keeps track of the latest versions of your files, such as computer code or write up for results as you work on them. It also allows you to go back to any previous version of your files (this is ‘version control’). (It also does much more). GitHub is a website that hosts a ‘repository’ of your work code and enables users to easily collaborate. Your repositories can be either public or private. We use Git and GitHub extensively: to collaborate on writing code and producing documents (such as this entire document!). to easily share code publically for scientific papers, and update it if necessary. when working alone to retain a methodical workflow. Example application – Pacific Hake stock assessment Under a formal Agreement between the Canadian and US governments, a team of four of us (two from each country) conduct an annual stock assessment for Pacific Hake (Merluccius productus) off the west coast of Canada and the US. The assessment is used to manage the stock, which is of important ecological and economic value ($100 million export value in Canada). We fit complex population models to data to make projections about future health of the stock under different levels of catch. There is a very short turnaround (five weeks) between getting the final data, doing the analyses (model runs can take many hours of computer time) and submitting the assessment document, which is typically &gt;200 pages and contains numerous figures and tables available here. Prior to 2016, the document was assembled in Word, requiring lots of editing and amaglamating of files, often late at night. Now we share our code via GitHub, automate a lot of the document production using knitr (similar to Rmarkdown, covered in Module 2). So with four people constantly working on the same large document, we need to ensure we are keeping up-to-date with each other, can all produce the latest version, and have identical folder structures on each other’s computers. The alternative of emailing files back and forth is: very inefficient, prone to errors, just painful. What we can avoid Using GitHub it is easy to see what text/code collaborators have changed, avoiding things like the following, for which it hard to see where to get started: We may want to keep old versions of files (and email them back and forth), but without GitHub we can end up with a veritable gong show: We can avoid having to co-ordinate having only one person working on the latest version of a document, so we don’t get things like: Can avoid multiple versions of a file that then have to be carefully merged: While GoogleDocs, for example, is fine for collaborating on a short document, it isn’t suitable for sharing complex code, or complex documents that are somewhat automatically updated. GitHub advantages Say you’ve off on a two-week hike while your collaborators have been diligently working away and they have edited 15 new files of code in five folders, added four data sets, and created five new pages of text towards a manuscript. You can easily catch up with them (get all their changes onto your computer) with a few simple commands. You don’t even have to pester them to ask what they’ve done, as you can check it yourself. So rather than this conversation: You: “Hey, I’m back from my awesome trip and saw some bears. What have you been doing with the project?” Likely reply: “Glad you had fun. I’m busy on something else right now. Er, where were we at when you left?” You can have this one: You: “Hey, I’m back from my awesome trip and saw some bears. I went through your commits on GitHub and everything looks great. Shall I get on those Methods Issues you assigned me?” Likely reply: “Glad you had fun, looking forward to hearing about it. I’m busy on something else right now so, yes, resolving those issues will be great, thanks.” And the project keeps moving in an efficient way. By having code shared publically, it is easy to answer questions, such as this one I received: Rather than go searching on my laptop for the code that I hadn’t looked at for six months, I could click on the link the questioner sent and answer very quickly, with a simple link to the file I am referring to (there is no ambiguity): You can even ask who last edited a particular line of code/text (GitHub amusingly calls it ‘Blame’): You can properly keep track of (and discuss) ‘Issues’ to be thought about or fixed, rather than having things in emails that get forgotten: Important: You still have all your work locally on your computer. So if your internet access goes down or GitHub is unavailable (which of course will only happen when you have a deadline) you can still carry on with your work. Why this course? Delving into the Git and GitHub world online it can feel like you need a computer science degree to get started. This may not be surprising as Git was writting by the guy who wrote the operating system Linux, to help people collaborate on writing the operating system Linux. But it means that, for example, the second paragraph of the Wikipedia Git page says: “As with most other distributed version control systems, and unlike most client–server systems, every Git directory on every computer is a full-fledged repository with complete history and full version-tracking abilities, independent of network access or a central server.” Say what??? That is fairly incomprehensible to those without strong computer science backgrounds. The aim of this module is to introduce biologists to the world of Git and GitHub, while avoiding a lot of the technical details. However, once you have mastered the basics then it should be easier to delve deeper. Our target audience is: graduate level biology students biology faculty government scientists scientists in non-governmental organisations in fact anyone wanting to learn these tools This work is extended from lectures and exercises developed by Chris Grandin and myself as part of a Fisheries and Oceans Canada workshop. (Luckily Chris does have a computer science degree, and so was able to get some of us going with Git and GitHub several years ago). These tools are now widely used within our organisation. Computer language For sharing code, it doesn’t matter what language your code is in (R, Matlab, Python, C, …), as we will just be sharing text files. There is a learning curve, but once you get going you only really need a few main commands. Unfortunately the hardest bit is actually getting everything set up…. 4.2 Getting set up for the first time Before you start using Git you need to set up your computer to use it, and install a few other programs that are useful. This is a one-time setup and once it is done, you will be able to easily create new projects or join others in collaboration. We have tested the installations as much as feasible. If you have an issue then search the internet, as it may be due to some configuration on your particular computer. This module is for any operating system: Windows, MacOS, Linux or Unix. 4.2.1 What you will end up having installed These are programs/things you will need (instructions are on the next slides). Obviously skip any that you already have working. A GitHub account A text editor that isn’t Notepad Git on your computer Diffmerge or something similar for comparing changes to files (not completely necessary) Markdown Pad 2 or Chrome extension or something similar for viewing Markdown files (not completely necessary) 4.2.2 Get a GitHub account Sign up for GitHub: http://github.com If possible, choose a user name that will make sense to colleagues, e.g. andrew-edwards or cgrandin, not pink-unicorn. Desirable: attach a photo (headshot) to your profile. This makes it easy for collaborators to identify you. 4.2.3 Text Editor You must have a text editor that is aware of outside changes in a file. This is necessary because if you have a file open in the editor and you download an updated version of the file, you want the editor to ask you if you want to use the updated version. We know that Emacs, Xemacs and maybe Vim are okay, as is RStudiofor.R (and other) files. Notepad is not okay. But you can download and install Notepad++ which is fine: https://notepad-plus-plus.org/download/v7.3.3.html 4.2.4 Install the Git application on your machine See https://git-scm.com/downloads for downloading instructions for Windows, MAC and Linux/Unix It seems best to accept the default options, except NOT Notepad or Vim (unless you use Vim) as the default editor. 4.2.5 Git shell For this course we will use a simple git shell to type commands (rather than a point-and-click Graphical User Interface). This is for several reasons: Commands are the same across operating systems. It is easier to demonstrate (and remember) a few simple commands, rather than follow a cursor moving across a screen. Learning the text commands will give you a good understanding of how Git and GitHub work. It is easier to Google for help when you get stuck or want to learn about more advanced options. 4.2.6 Git shell, RStudio There are many Graphical User Interfaces that are available, as described at https://git-scm.com/downloads/guis. Many (but not all) biologists use R in RStudio for their analyses. There is Git functionality built in to RStudio that we (TODO: SOMEONE?) will demonstrate later. I use magit which works in the text editor emacs (which for years I have used for pretty much everything, such as editing files, running R, Matlab, etc.). But I would not have been able to learn magit without first knowing the Git commands from using the shell. For now we will stick with the Git shell for the aforementioned reasons. It will also give you a better understanding of Git and GitHub, and emphasise that you can use Git for any files, not just R code. 4.2.7 Powershell and posh-git Download a Powershell (a shell window in which you can type commands, presumably the ‘power’ part means it’s more powerful than a basic version) and then posh-git following the instructions at https://github.com/dahlbyk/posh-git Do the ‘Installation’ and ‘Using posh-git’ sections. If you don’t understand some options (I don’t!) just pick the simplest, usually the first. The next slides are from our course about three years ago (and were for Windows). So they may be out of date (though first one is recent tips from a colleague). [Maybe we should see https://upg-dh.newtfire.org/explainGitShell.html] 4.2.8 One-time authentication The first time you get set up or start using Git, there will be some one-timeauthentication to connect to your GitHub account. Follow any instructions. 4.2.9 Configure the Git application Windows Create a github directory, such as C:. It is fine to put it in a differentpath, but make sure there are no spaces or special charactersanywherein the fullpath. This is where you want to be saving your work that you are tracking with Git. TODO: Andy has to reinstall anyway and will write something here. Think it’s just following instructions. MAC Create the directory ~/github Enjoy a beverage TODO: Check with Luwen if it is that simple 4.2.10 “Cloning” the git-course repository For these instructions, replace GITHUB-USER with your GitHub account name. On the GitHub webpage, sign into your account and navigate to: https://github.com/quantitative-biology/module-1-git Windows: Open the Git shell and run the following command to clone the repository (‘clone’ means copy all files in the repository to your computer): git clone https://github.com/quantitative-biology/module-1-git MAC: Open terminal and change to the GitHub directory: cd ~/github then run the clone command: git clone https://github.com/quantitative-biology/module-1-git You now have the files for the GitHub course on your computer 4.2.11 Copy the .gitignore file Git uses a configuration file for your account info, name to use when committing, aliases for commands, and other things. Open up the misc sub-directory in the git-module-1 directory and copy the file .gitconfig. For Windows, copy this file (overwrite the existing file) to: C:-COMPUTER-USER-NAME.gitconfig, where YOUR-COMPUTER-USER-NAME is your username on your computer, not your GitHub account name. For MAC, copy this file (overwrite the existing file) to: ~/.gitconfig 4.2.12 Edit the .gitconfig file Use your favourite editor to edit the new file (not the one ingit-course/misc). Change the [user] settings to reflect your information. Change the [difftool] and [diffmerge] directories so they point to the location where you have DiffMerge. For Windows the location should be: C:Files.exe For MAC the location should be: /usr/local/bin/diffmerge If you could not install [difftool] or [diffmerge] then delete those lines in your .gitconfig file. 4.2.13 MAC only: make your output pretty On the MAC, change the ~/github directory and run the following command: git config –global color.ui.auto This will make your git output colored in a similar way to the Windows powershell version. 4.2.14 Install the difftool The difftool will be used to examine differences between different versions of files andalso to simplify merging of branches and collaborator’s code. There are many programsthat can be used but for consistency we will use Diffmerge. It is nice to have but not essential if you have trouble installing it. Install Diffmerge: https://sourcegear.com/diffmerge/downloads.php The configuration for directing git to use Diffmerge will be done in a later step. 4.2.15 Markdown Pad Each project has an associated README.md file that appears on its GitHub homepage.The extension .md stands for Markdown and is just an ASCii text file that containssimple formatting (such as bold or italics). There are two options we have used to readmarkdown files, choose one: The Markdown Pad 2 editor/viewer which is easy to use: http://markdownpad.com. Just get the free version. The Chrome extension for markdown viewing: https://chrome.google.com/webstore/detail/markdown-viewer/ckkdlimhmcjmikdlpkmbgfkaikojcbjk?hl=en. 4.3 Using Git and GitHub This section also has a recorded lecture to demonstrate the main concepts and ideas. The lecture is available here (TODO) and the slides from the talk are here (TODO), though the notes below mostly replicate the slides. 4.3.1 Definitions Let’s start with some definitions: Repository – essentially a directory containing all your files for a project (plus some files that Git uses). Git – a program that allows you to efficiently save ongoing versions of your files (`version control’). GitHub – a website that hosts your repositories so that you can easily share code and collaborate with colleagues. Basically, the idea is that you work on your files in a repository on your computer, use Git on your computer when you are happy to keep your changes, and use GitHub to easily share the files. Here you will learn the important steps: Creating – create a new repository on GitHub Cloning – copying it to your local computer Committing – the crux of working with Git Collaborating – efficiently work with colleagues Conflicts – fixing conflict changes (happens rarely) 4.3.2 Creating a new repository Sign into your GitHub account, click on the Repositories tab, and press the New button. Give your repository a name. Let’s call it test. Check Initialize this repository with a README. Leave Add .gitignore and Add a license set to None Click Create repository. You now have a new repository on the GitHub website. Next we will clone it onto your computer. 4.3.3 Cloning your new repository Copy the full URL (web address) of your test repository. Open the Git shell and navigate to your C://github directory (or whatever you called it when you created it in the setup instructions – it’s the place you are going to save all your Git repositories). Run the following command to clone your repository: git clone URL where URL is the url of your newly created repository (paste should work). You should now have a subdirectory called github/test on your computer. In Git shell, change to that directory (with cd test). So ‘clone’ is Git speak for copying something from GitHub onto your local computer. This example has just one file (README.md). But the process is the same for a repository with multiple files and multiple directories, and the complate file sturcture is fully preserved. Windows only: Storing your credentials When you are using the Git shell for the very first time on Windows, issue the following command: git config --global credential.helper wincred This means that you don’t have to repeatedly enter you GitHub password (just do it when you are first prompted). 4.3.4 Committing Create a new file, newFile.txt, in the github/test directory. Add a line of text at the start of the file and save it. Check the status of your (test) repository: git status It should say that you have an ‘Untracked file’ called newFile.txt. You want to tell Git to start tracking it, by using: git add. gitignore Type git status again. You should see that the file is listed as a ‘new file’ under ‘Changes to be commited.’ Let’s now ‘commit’ it: git commit -a -m \"Add newFile.txt.\" The commit message (in the quotes) should be a useful message saying what the commit encapsulates (more on that later). Push the commit to GitHub: git push Check (refresh) the GitHub webpage and see your commit and the uploaded file. What just happened? We just used three of the main Git commands: git add &lt;filename&gt; – tell Git to start keeping track of changes to this file. You only need to tell Git this once. git commit -a -m \"Message.\" – committing your changes, which means tell Git you are happy with your edits and want to save them. git push – this sends your commit to the GitHub website. You always have your files stored locally on your computer (as usual), even if you don’t add them or commit changes. When you push to GitHub then your colleagues can easily fetch (retrieve) them. Keyboard aliases (shortcuts) Now, git commit -a -m \"Message.\" is a bit much to type, so we have an alias for it: git com \"Message.\" This is defined in the .gitconfig file you installed in the git-setup instructions into C:\\Users\\YOUR-USER-NAME\\.gitconfig (for Windows). YOu can also add your own commands to that file. The -a means ‘commit all changes of files that Git is tracking,’ and -m is to include a message. Since we usually want to do both of these, git com \"Message.\" is a useful shortcut. But it is important to realise it is an alias if searching online for help. Similarly: git s – for git status git p – for git push git d – for git diff git f – for git fetch From now on we will mostly use the aliases. Use the full commands if the .gitconfig file didn’t work for you. Edit Readme.md Edit the Readme.md file. Add some simple comments describing the project such as: “A test repository for learning Git.” Look over the changes, commit them, and push them to your GitHub repository: git s git d (or git diff) – this gives a simple look at the differences between the last committed version and your current version (of all files; only one in this case) git com “Initial edit of Readme.md” git p (or git push) Refresh your GitHub web page and you should see your text (the Readme.md file is what is shown on the main page of your repo). 4.3.5 Exercise 1: create, edit and commit simpleText.txt Create a text file simpleText.txt in your local test repository. Add a line of text at the start and save it. Predict what git s will tell you, then type it in the Git shell to check. Add the file to the repository using the git commands: git add simpleText.txt git s – not necessary but useful to check you understand what is changing before you commit git com \"Adding simpleText.txt\" git p Add some more test to simpleText.txt then git com \"Message.\" and git p. Repeat this a few times to get the hang of it. git com frequently and git p occasionally (you do not have to push every commit), while intermittently doing git s and git d to understand what’s changing. Keep and eye on your commits by refreshing the GitHub page. In reality when writing code/text you won’t be committing quite so frequently, as your focus will be on the writing. Adding multiple files at once Often you add multiple files in a new directory. When you run git s, you will see a large list of Untracked files. They can be all added at once by simply adding the whole directory. 4.3.6 Exercise 2: multiple files Do the following, to get the idea of creating multiple files in a folder and committing that folder. Create a new directory in your test repository, using your normal method. Call it new-stuff. Add a few new test files to that directory called test1.txt, test2.txt, etc. Put some example text in one or more of them if you want. On the command line, check the status: git s You will see a listing showing the new-stuff/ directory in Untracked files. To add all the new files in preparation for a commit, issue the command: git add new-stuff/ Check the status of the repository again with git s It will now show all files in Changes to be committed Commit the changes: git com \"Added new-stuff directory.\" Push the changes to GitHub: git p Check your GitHub webpage and see your commit and that the files have been uploaded. That works no matter how many files are in your new-stuff directory. There could be a hundred and it’s the same command. Wildcard symbol * This is useful to know (no need to do it as part of the exercise): To add multiple files with similar names you can use the wildcard * symbol. You just added (told Git to keep track of) the new files in your new-stuff/ directory. If you add more new files to that directory, you will have to tell Git to track those also. This is because they are new – you haven’t told Git about them yet. Say you have 10 new files called idea1.txt, idea2.txt, …, idea 10.txt. Instead of typing git add new-stuff/idea1.txt git add new-stuff/idea2.txt etc. you can just use the wildcard *: git add new-stuff/idea*.txt or even just git add new-stuff/*.txt or git add new-stuff/*.*. The .gitignore file But what if you don’t want to add all the files that you create? Each repository can have a .gitignore file, in the root directory of the repository. Such a file has names of files (such as my-secret-notes.txt) or wildcard names (such as _*.pdf_ or _*.doc_ ) that will be completely ignored by Git. For an example, see https://github.com/pacific-hake/hake-assessment/blob/master/.gitignore, noting that the # can be used for comments. When sharing a repository with others, you want to share your code (for example, R, Python or Matlab code) and maybe data, but generally not share the output (such as figures that the code generates; more on this later). For reproducible research your colleague (or anyone) should be able to run your code to generate the results. Some programs you run may make temporary files that don’t need to be tracked by Git, the names of which should also be included in your .gitignore. When sharing code or collaborating you want to keep your repository as clean as possible and not clutter it up with files that other people don’t need. So when you run git s and see untracked files that you don’t want to be tracked, add them (or a suitable wildcard expression) to your .gitignore file so that they are not added inadvertently. This will also simplify your workflow (you don’t need to keep being reminded that you have untracked files). If you are on MacOS and you find that folders have a .DS_Store file in them, then create (and add and commit) a .gitignore file with .DS_Store as a line. Git Workflow You have now learnt the basics of using Git. By creating a public repository on GitHub you can now release your code to the world! You can also choose the private repository option when creating a repository, so that you can control who can see it. Go into Settings--Manage Access to add collaborators. 4.3.7 Collaborating Now we will show how to collaborate with colleagues, which is where the usefulness of Git will become more apparent. There are a few different ways to collaborate using Git and GitHub. We will focus on the following one since it is the simplest, and is what you need to collaborate with colleagues. Concept: there is a project where people contribute to a main repository that is considered the ‘master copy.’ Everyone clones directly from the creator’s repository. All collaborators push their commits to the main repository (the creator has to add them as collaborators once on GitHub). Since the creator has to grant permission, you won’t have just anyone contributing to (and maybe messing up your work), just your trusted collaborators. But you have to trust your team to not mess things up (more on that later!). Okay, so in the video we demonstrated the following: Kim creates new repo called collaborate (and clones it to her computer). Andy clones it also. On GitHub, Kim gives Andy ‘push access’ to her collaborate repo. Both do some edits (create some new simple text files). For Andy to get Kim’s updates (and vice versa), it was just: git fetch (or just git f) – fetches the latest version of the repository from GitHub onto your computer. Your local files have not yet changed (check them), but Git has the changes stored on your computer (?!?). git rebase – updates your local repository (the committed files on your computer) with the changes you have just fetched, merging both people’s work together. git p – pushes the merged changes back up to GitHub so that the other person can get them. That is the basic workflow. We also showed an example of git p not being allowed for Person A because there are recent commits on GitHub (by Person B) that Person A has not yet merged into their local version of the repository. Here is an example of the error message you get: While a bit lengthy, the error message is useful. It forces you to get the other person’s work before you push yours. You do this by: git f git rebase. So to be allowed to push, just fetch to get the new commits onto your computer, and then rebase to combine the commits into your local version. Then you can git push. Here is a full screenshot (‘g’ is just a shortcut for ‘git’). The green up arrow number 8 tells me I have 8 commits to push to GitHub. The yellow arrows I think of as just implying I need to do a rebase (before doing that I might browse through the other person’s commits on GitHub): After the rebase I was allowed to push and then everything is up to date. A bit more about git rebase Andy commits local changes, tries to git push but is told to first git fetch (to get Kim’s changes from GitHub). Andy does git fetch and then git rebase. What git rebase does is basically add Andy’s commits to Kim’s commits. (TODO: check it’s not the othe way around!) Andy then does git push to push his commits to GitHub (from where Kim will fetch them when she’s ready). Providing there are no conflicts, this will work fine. Another option is to do a git merge, which basically creates a new commit that merges both people’s work together. Our groups used to use git merge and now use git rebase; some people don’t like git merge because it adds extra commits. For a more in-depth understanding see https://reflectoring.io/git-rebase-merge/ for one of the clearer explanations out there concerning rebase v merge. Note that the error in the above screenshot (when I could not git push) told me that I might want to do git pull. This is basically git fetch git merge in one command, but it seems preferable to do git fetch git rebase. Fixing a conflict A conflict happens when two people have edited the same line(s) of the same file. Conflicts happen relatively rarely and can be generally avoided by co-ordinating with collaborators so that you are working on different files. But, they will happen and you need to know how to resolve them. Git forces you to explicitly decide whose changes to keep – this is a good thing, since you want a human to make such a decision. In the video we demonstrated a conflict. Fixing a conflict The best approach I have found to fixing a conflict is the following: Trying git rebase will tell you there is a conflict. git rebase --abort – do this to abort the rebase attempt. git merge – this will tell you there is a conflict. Open the file(s) with the conflict and edit the text (see below). git add &lt;filename(s)&gt; – you have to then add the files that had the conflict (I am not sure why this is necessary, I just do it). git com \"&lt;message&gt;\" – in your commit message you can explain how you fixed the conflict. This is useful so that your collaborators know you have resolved a conflict (they can look at the commit to see if they are happy with it). The merge message will tell you which files are conflicting. Open those files one by one, and you will see the conflicted section bracketed like the following: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Line(s) of text/code which are currently in your file. ======= Line(s) of text/code which are trying to merge in, but conflict. &gt;&gt;&gt;&gt;&gt;&gt; origin/main where origin/main refers to the version you have fetched from GitHub. All you do is remove the line(s) of text that you do not want to keep (or edit the line(s) to be something else entirely), and remove the bracketing lines &lt;&lt;&lt;... and &gt;&gt;&gt;..., and the ====== line. Save each conflicted file and then (as mentioned previously): git add &lt;filename(s)&gt; git com &quot;Kept Kim&#39;s edits as more consistent with remaining text.&quot; git p 4.3.8 Exercise 3: collaborating on a single repository If you have a colleague available, try what we just did: Person 1 creates a new repository on GitHub and clone to their computer. Give the Person 2 ‘push access’ to the repository (on the repo page on GitHub: Settings – Manage access – Invite a collaborator) Person 2 clones to their computer Both create a simple text file (use different filenames), add some text and, as usual, add, commit, and push. git fetch and git rebase to get the other person’s file. Continue editing either file, committing, and pushing. If you get the push error (shown earlier), refresh GitHub repository site to see recent commits (click on the XX commits link). You can easily spot the other person’s recent commits. Click on one (the bold message) to see details. Purposefully create a conflict (both edit the same line of the same file). Resolve it as described earlier. In practice you won’t commit so frequently when working, but this is good to get the hang of it. Congratulations Congratulations, you now know the few basic commands and functionality needed to collaborate with Git and GitHub. It takes a bit of practice, but it is very powerful. 95% of the time, this is all you are doing: Change some code. git s git d git com &quot;My commit message&quot;` git p (the git s and git d are useful to check you have changed only what you think you have changed). If GitHub does not allow you to push: git fetch git rebase If conflicts, then git rebase --abort git merge fix the conflicts manually and then git add &lt;conflicted file(s)&gt; git com &quot;Message to explain what you did&quot; git p Change some code and repeat! The next section gives slightly more advanced background that should further improve your understanding (including why Git is useful even when not collaborating or sharing your code), plus tips for improving your workflow. 4.4 Beyond the basics Here are some Git and GitHub concepts and tips that go beyond the basics that we just covered. 4.4.1 Workflow tips Realise that you still edit and save your files in the usual way on your computer. If you don’t do Git commits you will still have the latest versions of your files on your computer (as you would if you weren’t using Git). So if you do get stuck with Git you can carry on working as normal (though you probably do want to try and fix it at some point). When collaborating: If working closely with others, when you start each day (or after a break) then make sure you are up to date and have all their commits. Refresh the GitHub page for you repository, and git fetch (or just git f)and git clone if needed. (To be safe you can git f and git s to check). We find it helpful to co-ordinate our work (Slack is useful for this, or use GitHub Issues for complex discussion – see below), so that if multiple people are working at the same time, you are at least not working on exactly the same parts, just to reduce conflicts. Commit fairly frequently and write helpful commit messages (so your colleagues get an idea of what you’ve done in each commit). Push less frequently, and don’t push code that doesn’t work – that will annoy your colleagues. And then they (and probably you) may both spend time fixing it. To see who last edited a particular piece of code, when viewing the file on GitHub click Blame (you can even click on the square icon to go to the previous commit that changed that line): GitHub Issues GitHub Issues are very useful for discussing issues with your repo. For our annual Pacific Hake assessment we have used them extensively over the years: The Issues tab lists our current ‘Open’ issues – we have 20, of which five (the most recently posted) are shown here. We are currently in-between assessments (and not working on it), so we have created Issues that we want to think about or deal with for next year. This avoids forgetting about ideas or losing them in old emails. Issues are intuitive to use. There is a bright green ‘New Issues’ button to create new ones, you give a title and then write some details, people can reply, you can assign people to look at them, and you can close them. In the above screenshot you can see that we have closed 815 issues (this was over several years). Useful tip: when doing a commit that refers to an Issue, if you refer to the Issue number (with #&lt;number&gt;) in your commit message, then after pushing that commit the Issue on GitHub will automatically mention and link to the commit: git com \"Add more options to fancy_function(), #21.\" will mention the commit when you look at the issue. You can even automatically close the issue by saying closes #21 in your commit message: git com \"Add more options to fancy_function(), closes #21.\" Issues are particularly useful to avoid cluttering up code with commented notes or ideas that you may easily not come back to, or avoiding endless emails that end up getting overlooked. You don’t have to fix an Issue to close it, you can decide not to pursue, but at least you have made a decision. (We also use Slack a lot to communicate, but moreso for quick questions or bouncing ideas around – Issues are better for stuff that you want to come back to at some point). You may receive emails regarding Issues, but if you use GitHub a lot you will see Notifications (the blue dot on the bell in the top-right corner when signed in on GitHub) and that will show you new Issues of repositories you are involved with, or if anyone has updated an Issue. GitHub organizations If you will frequently collaborate with colleagues, you can create an Organization on GitHub and invite collaborators to it (click on your GitHub photo in the top-right corner, Settings, Organizations). Then they will automatically have access to all repositories created under the Organization. You can choose the security settings. 4.4.2 So I’ve made some changes but don’t really want to keep them – git stash If you’ve changed some code but have not committed it, and then maybe got in a mess and just want to go back to your last commit, you can stash your changes git stash and to include a message (for your future self): git stash save &quot;Message&quot; This stashes them away such that they can be retrieved later This is handy. You may think you don’t want to keep those changes, but sometimes you may later wish you had kept them somehwere. Note this only for files that Git is tracking (i.e. files that have been added at some point). You can have multiple stashes, seen by doing: git stash list To retrieve the last stash: git stash pop TODO: check rules for other stashed things. TODO: Often you’re working on something but it’s not finished, but you want to push it. Stash, then make branch then copy files in. Tricky to do all with git, so do manually. TODO: see my README for details, and test what to do 4.4.3 Pull requests TODO 4.4.4 The power to go back With Git you can revert back to any previous state of your repository. This is very powerful, though slightly scary at first. Do this with your test repository, that should have some files in it from the earlier excercise: git s to make sure you are all up-to-date (commit and/or push if necessary). In File Explorer (or whatever you use) look at your repository, you should see all your files, including the new-stuff\\ directory. Look at the commit tab on GitHub for your test repo and click on the clipboard icon to copy the HASH number thingy to the clipboard . In Git shell: git checkout HASH (where HASH is the pasted HASH, or git co HASH using our Alias) Look at File Explorer again – your new-stuff directory should have … disappeared!! (If it hasn’t disappeared then open it – the test files, i.e. test1.r, test2.r, etc. should be gone, but your text editor may have saved backup versions; manually delete them plus the new-stuff/ directory.) You are now back to the very first version of your repo! Powerful and scary. Now, to get your files back to the most recent version you had committed: git checkout main (it used to be git checkout master, the names have recently changed). That’s it! Check that your files are back. All this means that you can revert to any previous commit in your repository. This is very reassuring. For example you have some complex code that you realise is now a complete mess and you want to go back to yesterday’s version of everything. In practice you rarely actually do this, but it’s very comforting to know that you can. Consequently, your workflow is less cluttered and more tractable than having to save multiple versions of the same files with dates in the filename, such as this nightmare: Retrieving older work in practice I think there are fancy ways that Git can replace a current file with a version from an earlier commit. But, in practice (especially since you rarely want to do this) it is a bit safer to do the following: Say you are up-to-date (git s says all is good), but your program my_code.R just isn’t working and you want to go back to the version you had yesterday at commit number abc123. git co abc123 (or git checkout abc123) to checkout the earlier commit, which includes the old version of my_code.R that you want get. Copy my_code.R to a new file my_code_old.R. In the shell you can just do this with cp my_code.R my_code_old.R. Do NOT edit my_code.R or make any changes, as you may end up with a scary DETACHED HEAD warning. git co main to checkout the latest version again. Since you have NOT done git add my_code_old.R, Git is not tracking my_code_old.R and so it is just sitting in your folder as normal. Now you can manually copy what you want from my_code_old.R into my_code.R to fix your problem. It could be the full file, or just some part of it. Then commit as normal. At some point you can delete my_code_old.R so it is not hanging around, but you don’t have to. (Though maybe make a note in it as to which commit it was from, in case you do need it again). 4.4.5 So how does Git do all this? By now you’re probably wondering how Git keeps track of everything. Git does not keep versions of code, it keeps commits. The commits are kept track of using a HASH key which is a generated 40-digit key in hexadecimal (base 16). The hashes are what you see on GitHub and in various places when you use Git shell. By stitching all the commits back together again, Git can recreate all your code. There is a hidden .git/ directory in each repository. Look at the .git/objects/ subdirectory. Each subdirectory name is the first two digits of a HASH. The rest of the digits of the HASH are the filenames in the subdirectory. You can basically think of the hashes as representing commits (apparently they can also be blobs and trees, whatever they might be). I think of the files in the subdirectories containing the differences between each commit. Because of these structures, Git can go back and rebuild any or all files at any commit, and even have different directory structures at each commit. Since Git is keeping track of differences between files, this all works best for plain ASCii (text) files, such as .R, .txt, .Rmd, etc. Git does work for binary files, such as .xls, .docx, .RData, but since changes to the files are not easily saved (Git essentially has to resave the whole file at each commit), this is not very efficient and may make your repository large. Such files will be fully resaved every time they are changed. Think of a binary file as something that you cannot open in a text editor and read (it does not contain simple ASCii letters and numbers). Exceptions: often you may have an image or photo or other type file that you need to share for a document, but it isn’t going to keep changing. So that’s fine to commit. An example of why you should not commit binary files: A collaborator was running some R code (and correctly committed the .R files so that I could run it), but also committed the results, which included .pdf, .png and .RData files, which can get quite large. But, these latter files got updated every time the code was run. So changing one line of the .R code (which Git deals with very efficiently), and running that code and committing, resulted in the new .pdf etc. files being fully saved (since Git cannot just save the difference from the last commit because they are binary files). Even if, say, one point changes on a figure in a graph in a .pdf file, Git has to save the whole new version. This ended up with .git/objects/pack (whatever that might be!) being 2.8Gb. I needed space quickly on my computer so just deleted four files in .git/objects/pack, which freed up 1.6Gb. Note that I still had the actual final versions of files (as you would if not using Git), but just not the full repository history. However, when I tried to later do some work and then commit I got lots of ‘fatal’ errors with scary messages like bad object HEAD and the awesomely titled You are on a branch yet to be born: I just had to start again from scratch (reclone I think). Take-home message: Don’t mess with the .git directory!! 4.4.6 Git terminology At some point you will likely need to search online for some help (often questions are posted and answered on the excellent ‘stackoverflow’ website). A bit more understanding of terminology will help you. Remember that Git keeps commits. Several of these commits have pointers to them that have special names: HEAD points to the commit you are currently on in the Git shell. main or master is the default branch when you set up a repository on GitHub (there are two names because of recent changes on GitHub). 4.4.7 Branching So far we have only worked on the main branch. Sometimes you want to create a new branch that branches off from the main branch. It’s bit like a tree branching, except that at some point you want your new branch to be merged back into main. For example, you may want to try adding some new code to your project, but don’t want to break what is already there. You may do this even if working alone, but it’s especially useful if you are collaborating, or if, say, you have an R packages hosted on GitHub that anyone may be downloading – you don’t want to annoy them by pushing experimental code that doesn’t work. So you would create a new branch, work on that new branch (i.e. commit changes to the new branch), and when you are happy with your new changes you can easily merge it all back into main. Working on a new branch When creating a new branch, your starting point is identical to the branch you were when you created the new one. In the Git shell navigate into your test repository: cd test Depending on your set up, you should see main indicated somewhere (if not do git s and it should say On branch main. Make sure you are up-to-date and have committed all changes (git s, and commit if necessary). Create a new branch called temp, this will be based off the latest commit of the main branch you are currently on: git checkout -b temp (We have an alias for that: git cb temp`). You will be automatically placed in the new branch called temp, and commits you make will now occur in that branch only. Make and commit some changes (e.g. add a new file) – these will now be on your temp branch. You can push to GitHub. The first time you try git p, the Git shell will tell you that you need to type the following so that future pushes go to the new branch: git --set-upstream origin BRANCH-NAME Check the GitHub webpage to see that your branch was pushed. You repository page (that will still be looking at your main branch) may tell you that there is a temp branch with more recent commits than main. If not then if you click on the main drop-down menu: it should give you the option to look at your new temp branch. (The ‘1 branch’ in the above image should also say ‘2 branches’). You can now view your new file in your new temp branch on GitHub. A graphical way to see and understand branching is to click on Insights–Network to see the Network graph. The Network Graph is a useful visualization tool, where each commit is shown as a point on the graph (the numbers along the top are the dates). You can hover your mouse over a commit to see who committed it and the commit message. You can click to see full details of the commit. The Network Graph is particularly useful if you or others are working on multiple branches, or to check details about merges. Okay, back in your Git shell you can easily switch back to your original main branch: git checkout main (or the alias git co main). You will see that the file you just added is gone, because it only exists in the temp branch at this moment. Imagine that in your temp branch you did several commits to create a new function in your code, or have added some new text to a report. Now you are happy with what you’ve done you want to merge it back into the main branch. To view all local branches: git branch There is an asterisk next to the branch you are currently in. To switch to another branch (main in our case): git checkout main To combine the changes from the temp branch: git rebase temp or git merge temp Now the file you created in the temp branch now appears in the main branch. All commits done in the temp branch will now be in the main branch as well. If there was a merge conflict, you must fix it at this point (see earleir). Once you’ve merged your temp branch into main, you don’t really need temp any more and so it is good protocol to delete to keep things tidy: git branch -d temp If you have unmerged changes in a branch, you will not be allowed to delete it, but Git shell will tell you the command to forcibly delete it: git branch -D temp Warning – you won’t be able to get any of those changes back once you do this. To remove a branch entirely from GitHub: git p origin --delete BRANCH-NAME 4.4.8 Undoing stuff If you make a commit followed by other commits, then realize you want to undo the earlier commit, you use revert: git revert HASH where HASH is the hash for the commit you want to undo. Remember that Git shell is smart enough that you only need the first five digits: git revert 1ef1d This actually creates a new commit with the automatic message Revert \"&lt;previous commit message&gt;\". Obviously, you have to be careful with this if you’re changing something that was a few commits back, as you might mess up your code. Undoing changes not yet committed If you’ve made a mess in your working directory and you want to change everything back to the way it was on the last commit: git reset --hard HEAD If you’ve messed up a single file and just want that one file to go back to the way it was on the last commit: git checkout HEAD &lt;filename_to_restore&gt; Warning – running these commands will delete the changes you have made. Since you have not committed any changes, they will be lost. Make sure you are certain you don’t need the changes before running these commands. If you aren’t sure if you need the changes again in the future, use git stash instead. Changing the commit message in the last commit If you make a commit then realize you want to change it (add more information, fix something that will confuse your colleagues, fix something that will confuse you tomorrow), you can change the commit message: git commit --amend -m \"Correct message.\" This only works on the last commit. If you already pushed the commit before realizing that the message needs modification, do this: git p --force after making the amendment to the commit message. "],["introduction-to-rmarkdown.html", "5 Introduction to Rmarkdown 5.1 Motivation 5.2 Basic idea 5.3 Simple example", " 5 Introduction to Rmarkdown 5.1 Motivation Say you get some tree data from a colleague, and spend a month writing lots of R code to analyse the data. Your code also produces beautiful figures and tables, that you then manually copy into a Word document. You have also written lots of text, and include specific calculated numbers in the text, such as the simple The average tree height was 10.1 m. Then your colleague sheepishly tells you that someone found an error in some of the data, and so you need to redo everything. Your heart sinks with the prospect of re-running all your code and making sure you manually copy the correct new figures into your document. Oh, and you need to redo the tables and check all the numbers in your text. The alternative modern approach is to use Rmarkdown. The idea is that you can generate a ‘dynamic report.’ You write code that contains a mixture of your R code and your write up. You can use this for short analyses, scientific manuscripts, or even a complete thesis. This introduction, aimed at biologists, will get you started with the basics. You will then be in a good position to learn more details from the RStudio introduction and the online Definitive Guide to Rmarkdown. Example application A recent document we wrote in Rmarkdown is A reproducible data synopsis for over 100 species of British Columbia groundfish. For each of 113 species, we produced two pages of figures: For each species, the layout of the figures is identical (even to show no data when none are available). Producing each figure and manually inserting them into a Word document would be extremely tedious. Instead, the production of the document is automated using Rmarkdown. Furthermore, the work is transparent and traceable. Because the code produces the figures (they are not pasted in from somewhere), we can trace back from the Rmarkdown code to see the R code that: pulled the data from databases fit models generated plots. In particular, we intend to periodically update the document as new data become available. While still a lot of work, it is less daunting knowing that the code is already available. On a practical level, the report has allowed anyone to see the data available, and has consequently increased data transparency between Fisheries and Oceans Canada, the fishing industry, non-governmental organizations, and the public. This is admittedly a very advanced example with a ton of code (several new R packages) and work behind it, but the idea is to show you what is possible. 5.2 Basic idea In the above tree example you would have a sentence (written in, say, Word) that says The average tree height was 10.1 m The “10.1” is hard-wired into your text, and you typed it on from the value “10.1” that your R code calculated (in a variable you calculated as avge_height). In your Rmarkdown document, you would equivalently have: The average tree height was `r avge_height` m Instead of “10.1” you refer directly to the variable avge_height that you have already calculated. When you ‘render’ your Rmarkdown document, it automatically fills in the avge_height value as “10.1.” The `r means that the next bit of code (until the next backtick) should be evaluated using R, and the result inserted. This is the basic idea. Then, when your colleague mentions the error (or, say, provides you with extra data) you can just re-run your code and the “10.1” will be automatically updated in your document. This concept extends to your tables and figures – they can all be automatically updated. 5.3 Simple example Here we will generate some data, show some of it in a table, plot it, and show the results of fitting a simple linear regression. Read through this and then you will download and run the code in the exercise. Generate data First we’ll need some libraries: library(kableExtra) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:kableExtra&#39;: ## ## group_rows ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(xtable) Now generate some data: set.seed(42) n &lt;- 50 # sample size x &lt;- 1:n y &lt;- 10*x + rnorm(n, 0, 10) n ## [1] 50 So we are showing our R code here (we can choose to hide it if we like), and it has been executed, yielding the printed output of the value of n (because of the final line of the code). We can also embed results from R within sentences: We have a sample size of 50. This is done (as mentioned above) by the code: We have a sample size of `r n` We can also say that the maximum value of the data is 506.5564788, or round it to a whole number: the maximum value of the data is 507. These were done by: the maximum value of the data is `r max(y)` the maximum value of the data is `r round(max(y))` Show some of the data Let’s combine the data in a tibble (think of it as a data frame if you don’t know what that is): data &lt;- tibble(x, y) data ## # A tibble: 50 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 23.7 ## 2 2 14.4 ## 3 3 33.6 ## 4 4 46.3 ## 5 5 54.0 ## 6 6 58.9 ## 7 7 85.1 ## 8 8 79.1 ## 9 9 110. ## 10 10 99.4 ## # … with 40 more rows (only the first 10 rows get printed here thanks to dplyr). To have a proper table, we can do kable(data[1:10,], caption = &quot;The first rows of my data.&quot;) Table 5.1: The first rows of my data. x y 1 23.70958 2 14.35302 3 33.63128 4 46.32863 5 54.04268 6 58.93875 7 85.11522 8 79.05341 9 110.18424 10 99.37286 (If you’re running the code separately the exact style may look different because of settings we have, but pretty much everything is tweakable with the kable and kableExtra packages). Plot then fit a regression data Now to plot the data: plot(x, y) To fit and then print the summary regression output from R: fit &lt;- lm(y ~ x) print(summary(fit)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.403 -4.366 -1.193 8.319 21.072 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.7071 3.2719 1.133 0.263 ## x 9.8406 0.1117 88.124 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.39 on 48 degrees of freedom ## Multiple R-squared: 0.9939, Adjusted R-squared: 0.9937 ## F-statistic: 7766 on 1 and 48 DF, p-value: &lt; 2.2e-16 And for a report we can produce a simple table (including a caption) of output and the regression fit: kable(coefficients(summary(fit)), caption = &quot;Linear regression fit.&quot;) Table 5.2: Linear regression fit. Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.707115 3.2719100 1.133012 0.2628368 x 9.840634 0.1116686 88.123530 0.0000000 And create a plot: plot(x, y) abline(fit, col=&quot;red&quot;) You can even get a bit clever with your writing by including an R ifelse statement to somewhat automate. So the maximum value of \\(y\\) is 507, which is greater than the special value of 400. The “greater than” or “less than” part is given by `r ifelse(max(y)&gt;400, paste(&quot;greater than&quot;), paste(&quot;less than&quot;))` But be careful and think about other possibilities – what if \\(y=399.9\\) in the above example?. Now, let’s go back and change the data The big feature of dynamically generating reports is when you go back and change or update the input data. For example, changing the data in the above example and then re-running it to redo the report. The best way to demonstrate this is for you to do it in the following Exercise. 5.3.1 Exercises Download this file TODO onto your computer and put it where you want to work on this exercise. The file is an Rmarkdown (.Rmd) file that you run by either clicking the knitr button in RStudio or doing rmarkdown::render(\"****.Rmd\") in R. TODO check these. Check that this has produced a .pdf file document that looks similar to what you see above (don’t worry if the styling is not identical, but the important content should be). Note that the .Rmd file is not repeating explanations of how to do certain things TODO reword. Copy the ***.pdf to ****-orig.pdf, change n to 30 in ***.Rmd, and re-run it. Compare the two .pdf files. You have done the same analyses but on different data. Hide the output in the first chunk of the ‘Now to fit a linear regression’ section. (In a report you wouldn’t show such results, though it’s very useful when exploring data – I show the output all the time when looking at large data frames). Explain cache TODO - check if it’s like this or not (it won’t be, depends on settings I do): Look at knitr-cache-tex/ folder, order files by ‘Date modified.’ Re-run this file. Look at the times of those files. Change \\(n\\) back to 50. Re-run this file. Again look at the times of the files in knitr-cache-tex/. So knitr caches results of each chunk, but then re-runs those that have changed (or those that depend on something that has change). Very efficient – if you just update some text you don’t need all the calculations redone. Beware - if you read in, say, a .csv file and the contents of the file change, knitr will still use the cached read-in of that file, because it won’t know that the file has changed. If you’re not sure then it’s safest to delete the knitr-cache-tex/ directory to re-run all calculations (including loading in of data). "],["introduction-to-multivariate-analysis.html", "6 Introduction to multivariate analysis", " 6 Introduction to multivariate analysis In this module we’ll be disccusing multivariate quantitative methods. These methods are used for data where there is more than one response or measurement which we are trying to understand by using more than one feature or characteristic of the system. For example we may wish to understand how both precipitation and soil type are related to plant community composition. In this question, we may be tracking the abundance of over a dozen different species and many different sites with different types of soil, precipitation and other environmental factors. You can easily see that this is not a situation that ordinary univariate approaches are designed to handle! Figure 6.1: Types of multivariate analysis There are many types of maultivariate analysis, and in this module we will only describe some of the most common ones. We can think of these different types of anlaysis as laying at different ends of a spectrum of treating the data as discrete vs continuous, and relying on identfying a reponse variable a priori versus letting the “data tell us” about explanatory features, i.e., latent variables (6.1. "],["multivariate-distance-and-cluster-analysis.html", "7 Multivariate distance and cluster analysis 7.1 Thinking about resemblence 7.2 Binary Similarity metrics 7.3 Quantitative similarity &amp; dissimilarity metrics 7.4 Cluster Analysis 7.5 Exercise: Cluster analysis of isotope data", " 7 Multivariate distance and cluster analysis 7.1 Thinking about resemblence We are often interested in the question of how similar two things might be and a natural way to quantify similar is to list those characters that are shared. For example, what genetic or morphological features are the sameo or different between two species? A resemblance measure quantifies similarity by adding up in some way the similarities and differences between two things. In fact, this type of overall measure of the relationship among objects or attributes is a strating point for most multivariate analyses, and is often the most critical step. We can express the shared characters of objects as either: similarity (S), which quantifies the degree of resemblance or dissimilarity (D) which quantifies the degree of difference. Table 7.1: Table 3.1 List of shared attributes for two things Attribute Object 1 Object 2 Similarity Attribute 1 1 0 x Attribute 2 0 1 x Attribute 3 0 0 ✓ Attribute 4 1 1 ✓ Attribute 5 1 1 ✓ Attribute 6 0 0 ✓ Attribute 7 0 1 x Attribute 8 0 0 ✓ Attribute 9 1 1 ✓ Attribute 10 1 0 x 7.2 Binary Similarity metrics The simplest similarity metric just tallys the number of shared features. So we just indicate yes or no for each characteristic for each of the two things we wish to compare 7.1. Table 7.2: Table 3.2 Summary of shared and absent atributes Object 1 Object 2 Present Absent Object 1 Present a b Object 2 Absent c d We could also find a shared lack of features and indicator of similarity. The simple matching coefficient uses both shared features, and shared absent features to quantify similarity as \\(S_m=\\frac{a+d}{a+b+c+d}\\), where a refers to the number of characteristics that object 1 possesses and b is the number that object 2 possesses and so on (see 7.2). We can further categorize similarity metrics as symmetric, where we regard both shared presence and shared absence as evidence of similarity. the simple matching coefficeint would be an example of this, or asymmetric, where we regard only shared presence as evidence of similarity (that is, we ignore shared absences). Asymmetric measures are most useful in analyzing ecological community data, since it is unlikely to be informative that two temperature zone communities lack tropical data, or that aquantic environments lack terrestrial species. The Jaccard index is an asymteric binary similarity coefficient calculated as \\(S_J=\\frac{a}{a+b+c}\\), while the quite similar Sørenson index is given as \\(S_S=\\frac{2a}{2a+b+c}\\), and so gives greater weight to shared similarities.both metrics range from 0 to 1, where a value of 1 indicates complete similarity. Table 7.3: Table 3.3 Species presence and absence in lake Erie and lake Ontario (data from from Watson 1974) species erie ontario 1 1 1 2 1 1 3 1 1 4 1 1 5 1 1 6 1 1 7 1 1 8 1 1 9 1 1 10 1 1 11 1 1 12 1 1 13 1 1 14 1 1 15 1 1 16 1 1 17 1 1 18 1 1 19 1 0 20 0 1 21 0 0 22 0 0 23 0 0 24 0 0 Let’s try an example. In the 70s, Watson (1974) compared the zooplankton species present in Lake Erie and Lake Ontario. We can use this information to compare how similar the communities in the two lakes were at this time. We can see that they shared a lot of species 7.3! We can calculate the similarity metrics quite easily using the table() function and the dataframe “lksp” where I have stored the data. tlake=table(lksp[,c(&quot;erie&quot;,&quot;ontario&quot;)]) tlake ontario erie 1 0 1 18 1 0 1 4 a=tlake[1,1] b=tlake[1,2] c=tlake[2,1] d=tlake[2,2] S_j=a/(a+b+c) S_j [1] 0.9 S_s=2*a/(2*a+b+c) S_s [1] 0.9473684 We can also talk about the dissimilarity between things. When a disimilarity or similarity metric has a finite range, we can simply convert from one to the other. For example, for similarities that range from 1 (identical) to 0 (completely different), dissimilarity would simply be 1-similarity. 7.3 Quantitative similarity &amp; dissimilarity metrics While binary similarity metrics are easy to understand, there are a few problems. These metrics work best when we have a small number of characteristics and we have sampled very well (e.g., the zooplankton in Lake Erie and Ontario). The metrics are biased against maximum similarity values when we have lots of charactersitics or species and poor sampling. In addition, we sometimes have more information than just a “yes” or “no” which we could use to further charactersitize similarity. Quantiative similarity and dissimularity metrics make use of this information. Problems with Binary similarity coefficients. Work best: small number of items, heavy sampling. Some examples of quantitative similarity metrics you will see are: Percentage similarity (Renkonen index), Morisita’s index of similarity (not dispersion) and Horn’s index. There are also many quantitive dissimilarity metrics. For example, Bray Curtis dissimilarity is frequently used by ecologists to quantify differences between samples based on abundance or count data. This measure is usually applied to raw abundance data, but can be applied to relative abundances. In this case, we often talk about the “distance” between two things. Distances are of two types, either dissimilarity, converted from analogous similarity indices, or specific distance measures, such as Euclidean distance, which doesn’t have a counterpart in any similarity index. There are many, many such metrics, and obviously, you should choose the most accurate and meaningful distance measure for a given application. Legendre &amp; Legendre (2012) offers a key how to select an appropriate measure for given data and problem (check their Tables 7.4-7.6). If you uncertain then choose several distance measures and compare the results. 7.3.1 Euclidean Distance Perhaps the mostly commonly used dissimilarity or distance measure is Euclidian distance. This metric is zero for identical sampling units and has no fixed upper bound. Figure 7.1: Euclidean Distance Eclidean distance in multivariate space is derived from our understanding of distance in a cartesian plane. If we had two species measured in in two different samples, we could then plot the abundance of species 1 and species 2 for each sample, and draw a line between them. This would be our Euclidean distance: the shortest path between the two points 7.1. We know that to calculate this distance we would just use the Pythagorean theorem as \\(c=\\sqrt{a^2+b^2}\\). To generalize to n species we can say \\(D^E_{jk}=\\sqrt{\\sum^n_{i=1}(X_{ij}-X_{ik})^2}\\), where Euclidean distance between samples j and k, \\(D^E_{jk}\\), is calculated by summing over the distance in abundance of each of n species in the two samples. Table 7.4: Species abundance and distance calculations for two samples Sample j Sample k \\((X_j-X_k)^2\\) Species 1 19 35 256 Species 2 35 10 625 Species 3 0 0 0 Species 4 35 5 900 Species 5 10 50 1600 Species 6 0 0 0 Species 7 0 3 9 Species 8 0 0 0 Species 9 30 10 400 Species 10 2 0 4 TOTAL 131 113 3794 Let’s try an example. Given the species abundances in 7.4, we can calculate the squared difference in abundance for each species, and sum that quantity. Then all we need is to take the square root to obtain the Euclidean distance. Did you get the correct answer of 61.6? Of course, R makes this much easier, I can calculate Euclidan distance using the dist() function, after creating a matrix of the two columns of species abundance data from my original eu dataframe. dist(rbind(eu$j[1:10], eu$k[1:10]), method = &quot;euclidean&quot;) 1 2 61.59545 7.3.2 Comparing more than two communities/samples/sites/genes/species What about the situation where we want to compare more than two communtiies, species, samples or genes? We can simply generate a dissimilarity or similarity matrix, where each pairwise comparison is given. In the species composition matrix below 7.5, sample A and B do not share any species, while sample A and C share all species but differ in abundances (e.g. species 3 = 1 in sample A and 8 in sample C). The calculation of Euclidean distance using the dist() function produces a lower triangular matrix with the pairwise comparisons (I’ve included the distance with the sample itself on the diagonal). The Euclidan distance values suggest that A and B are the most similar! Euclidean distance puts more weight on differences in species abundances than on difference in species presences. As a result, two samples not sharing any species could appear more similar (with lower Euclidean distance) than two samples which share species that largely differ in their abundances. There are other disadvantages as well, and in general, there is simply no perfect metric. For example, you may dislike the fact that Euclidean distance also has no upper bound, and so it becomes difficult to understand how similar two things are (i.e., the metric can only be understood in a relative way when comparing many things, Sample A is more similar to Sample B than Sample C, for example). You could use a Pearson correlation coefficient instead, since it has an upper bound of 1.0, but this metric is very senstive to outliers, which is also a disadvantage. Table 7.5: Species abundance versus species presence and Euclidean distance Sample A Sample B Sample C Species 1 0 1 0 Species 2 1 0 4 Species 3 1 0 8 dist(t(meu[2:4]), method=&quot;euclidean&quot;, diag=TRUE) A B C A 0.000000 B 1.732051 0.000000 C 7.615773 9.000000 0.000000 7.3.3 R functions There are a number of functions in R that can be used to calculate similarity and dissimilarity metrics. Since we are usually NOT just comparing two objects, sites or samples, these functions can help make your calculations much quicker when you are comparing many units. dist() offers a number of distance measures - e.g. euclidean,canberra and manhattan. The result is the distance matrix which gives the dissimilarity of each pair of objects, sites or samples. the matrix is an object of the class dist in R. vegdist() (library vegan) - default distance used in this function is Bray-Curtis distance, which is (in contrast to Euclidean distance) considered as more suitable for ecological data (it is a quantitative analog of Sørensen dissimilarity). dsvdis() (library labdsv) - offers some other indices than vegdist, e.g. ruzicka (Růžička, quantitative analogue of Jaccard) and roberts. For full comparison of dist, vegdist and dsvdis,see http://ecology.msu.montana.edu/labdsv/R/labs/lab8/lab8.html. dist.ldc() (library adespatial) - includes 21 dissimilarity indices described in Legendre &amp; De Cáceres (2013), twelve of which are not readily available in other packages. Note that Bray-Curtis dissimilarity is called percentage difference (method = “percentdiff”). By default this function returns an informative message as to whether the given dissimilarity index is Euclidean or not and whether it becomes Euclidean if square-rooted (as is the case of e.g. Bray-Curtis). designdist() (library vegan) - allows one to design virtually any distance measure using the formula for their calculation. daisy() (library cluster) - offers euclidean, manhattan and gower distance. distance() (library ecodist) - contains seven distance measures, but the function is more for demonstration (for larger matrices, the calculation takes rather long). 7.4 Cluster Analysis When we have a large number of things to compare, and examination of a matrix of similarlity or dissimilatiry metrics can be tedious or even impossible to do. One way to visualize the similarity among units is to use some form of cluster analysis. Clustering is the classification of data objects into discrete similarity groups (clusters) according to a defined distance measure. So we can constrast clustering, which assumes that units (e.g., sites, communities, species or genes) can be grouped into discrete categories based on similarity, with ordination, which treats the similarity between units as a continuous gradient (we’ll discuss ordination in section XX). We can use clustering to do things like discern whether there are one or two or three different communities in three or four or five sampling units. It is used in many fields, such as machine learning, data mining, pattern recognition, image analysis, genomics, systems biology, etc. Machine learning typically regards data clustering as a form of unsupervised learning. It is “unsupervised” because we are not guided by a priori ideas of which variables or samples belong in which clusters. “Learning” because the machine algorithm “learns” how to cluster. 7.4.1 Hierarchical clustering: groups are nested within other groups. Perhaps the most familiar type of clustering is hierarchical. There are two kinds: divisive and agglomerative. In the divisive method, the entire set of units is dividied into smaller and smaller groups. The agglomerative method starts with small groups of few units, and groups them into larger and larger clusters, until the entire data set is sampled (Pielou, 1984). Of course, once you have more than two units, you need some way to assess similarlity between the clusters. There are a couple of different methods here. Single linkage assigns the similairty between clusters to the most similar units in each cluster. Complete linkage uses the similarity between the most dissmilar units in each cluster, while average linkage aveages over all the units in each cluster (7.2). Figure 7.2: Different methods of determining similarity between clusters 7.4.1.0.1 Single Linkage Cluster Analysis Single linkage cluster analysis is one of the easiest to explain. It is hierarchical, agglomerative technique. We start by creating a matrix of similarity (or dissimilarity) indices between the units we want to compare. Then we find the most similar pair of samples, and that will form the 1st cluster. Next, we find either: (a) the second most similar pair of samples or (b) highest similarity between a cluster and a sample, or (c) most similar pair of clusters, whichever is greatest. We then continue this process until until there is one big cluster. Remember that in single linkage, similarity between two clusters = similarity between the two nearest members of the clusters. Or if we are comparing a sample to a cluster, the similarity is defined as, the similarity between sample and the nearest member of the cluster. cls=data.frame(a=c(5,6,34,1,12),b=c(10,1,2,32,4), c=c(1,59,32,3,4), d=c(2,63,1,2,45), e=c(4,3,4,90,2)) clsd=dist(t(cls), method=&quot;euclidean&quot;) clsd a b c d b 45.81484 c 53.82379 72.01389 d 73.73602 80.56054 51.57519 e 94.50397 58.41233 107.24738 114.91736 7.4.2 R functions Agglomerative approach (bottom-up) hclust() calculates hierarchical cluster analysis. Requires at least two arguments: d for distance matrix, and method for agglomerative algorithm, one of ward.D, ward.D2, single, complete, average (= UPGMA), mcquitty (= WPGMA), median (= WPGMC) or centroid (= UPGMC). Has it’s own plot function. and agnes() (library cluster) - contains six agglomerative algorithms, some not included in hclust. Divisive approach (top-down) diana() 7.4.3 How many clusters Drafty below 7.4.4 K-Means Clustering Partitional Clustering: A division of data objects into non-overlapping subsets (clusters) such that each data object is in exactly one subset Each cluster is associated with a centroid (center point) Each point is assigned to the cluster with the closest centroid Number of clusters, K, must be specified Choose the number of K clusters Select K points as the initial centroids Calculate the distance of all items to the K centroids Assign items to closest centroid Recompute the centroid of each cluster Repeat from (3) until clusters assignments are stable K-means has problems when clusters are of differing – Sizes – Densities – Non-globular shapes K-means has problems when the data contains outliers. 7.4.5 Fuzzy C-Means Clustering In contrast to strict (hard) clustering approaches, fuzzy (soft) clustering methods allow multiple cluster memberships of the clustered items. This is commonly achieved by assigning to each item a weight of belonging to each cluster. Thus, items at the edge of a cluster, may be in a cluster to a lesser degree than items at the center of a cluster. Typically, each item has as many coefficients (weights) as there are clusters that sum up for each item to one. 7.5 Exercise: Cluster analysis of isotope data we will read data into R. Download and import the dataset “Dataset_S1.csv” from Perkins et al. 2014 on the learn site in the lab folder. This data contains δ15N and δ13C signatures for species from different food webs. To read data into R one of the easiest options is to use the read.csv() function with the argument on a .csv file. These Comma Separated Files are one of your best options for reproducible research. They are human readable and easily handled by almost every time of software. Once you have successfully read your data file into R, take a look at it! Type mydata (or whatever you named your data object) to see if the data file was read in properly. Some datasets will be too large for this approach to be useful (the data will scroll right off the page). In that case, there are a number of commands to look at a portion of the dataset. You could use a command like names(mydata), but it has one obvious shortcoming. What is it? One of the best things to do is plot the imported data. Of course, this is not always possible with very large datasets, but this set should work. Use the plot function as plot(iso\\(N~iso\\)C, pch=16, xlab=“C,” ylab=“N”) to take a quick look. We are going to use this data set to see if a cluster analysis on δ15N and δ13C can identify the foodweb and trophic level of different species. That is we are going to “supervise” our unsupervised learning algorithm. Our first step is to create a dissimilarity matrix, but even before this, we must select that part of the data that we wish to use. We’ll be creating the dissimilarity matrix using just the δ15N and δ13C data, not the other components of the dataframe, so you will need to select and save just these two columns. In addition, our analysis will be affected by the missing data. So let’s get remove those rows with missing data right now using the complete.cases() function. The function returns a value of TRUE for every row in a dataframe that no missing values in any column. So newdat=mydata[complete.cases(mydata),], will be a new data frame with only complete row entries. Select and save only the C and N columns of this data, and you should be set to create a dissimilarity matrix. Create a dissimilarity matrix using dist() The function dist() will generate a matrix of the pairwise Euclidean distances between pairs of observations. Instead of “euclidean,” you can also use these distance measures of the dist() function “maximum,” “manhattan,” “canberra,” “binary” or “minkowski.” Check the help file for dist() by typing ?dist. Complete a hierarchical cluster analysis Now that you have a dissimilarity matrix, you complete a cluster analysis. The function hclust() will produce a data.frame that can be sent to the plot() function plotted to visualize the recommended clustering. The method used to complete the analysis is indicated below the graph. Please adjust the arguments of the function to complete a single linkage analysis (look at help(hclust)) to determine the method to do this). When you graph your cluster using plot(), you notice that there are many individual measurements, but there are only a few large groups. Does it look like there is an outlier? If so, you may want to remove this point from the data set, and then rerun the analysis. Remember to remove the point from the dataframe that has all the food chain info in it, otherwise you will have problems plotting later. When you examine the data set, you noted that there are 4 Food.chain designations. We will use the cutree() function to cut our cluster tree to get the desired number of groups, and then save the group numbers to a new column in our original dataframe. For example, newdata$clust&lt;- cutree(p,4). We can then plot the data using colours and symbols to see how well our clustering works plot(ndata\\(N~ndata\\)C, pch=as.numeric(ndata\\(Food.Chain), col=ndata\\)clust,xlab=“C,” ylab=“N,”main=“Four clusters”) legend(“topright,” legend=c(1:4),pch=1,col=c(1:4), bty=“n”) legend(“bottom,” legend=as.character(levels(iso$Food.Chain)[2:5]), pch=c(2:5), bty=“n”) If you are not happy with the success of this clustering algorithm you could try other variants. For example the “complete” linkage variant. BTW If you are feeling bored try a heatmap() display of the cluster analysis. This function works on the raw N and C data, but we have to send the data as a matrix rather than a dataframe, and we have to transpose it using the t() function as heatmap(as.matrix(t(net))). Clustering with kmeans() Let’s try a non-hierarchical cluster analysis on the same data. The kmeans() function requires that we select the required number of clusters ahead of time (4) as kclust=kmeans(ndata, 4), we can then save the assigned clusters to our dataframe as ndata\\(kclust=kclust\\)cluster, and plot in a similar way You can compare your clustering methods with a side by side plot. Just enter the command par(mfrow=c(1,2)), and then each plot statement. You can also compare by generating a table of the Food.chain and cluster designations for each method (e.g., table(ndata$Food.Chain, ndata $clust) Fuzzy clustering with the package cluster and fanny() To compete a fuzzy cluster analysis, you will need to install the package “cluster” on your computer, read the library into memory and then use the fanny() function Perkins, M.J., McDonald, R.A., van Veen, F.F., Kelly, S.D., Rees, G. and Bearhop, S., 2014. Application of nitrogen and carbon stable isotopes (δ15N and δ13C) to quantify food chain length and trophic structure. PloS one, 9(3), p.e93281. "],["ordination.html", "8 Ordination 8.1 Ordination as data reduction 8.2 Methods of Ordination 8.3 Principal Components Analysis (PCA)", " 8 Ordination While cluster analysis let’s us visualize multivariate data by grouping objects into dscrete categories, ordination uses continuous axes to help us accomplish the same task. While physicists grumble if space exceeds four dimensions, biologists typically grapple with dozens of dimensions (species and/or samples). In effect, we “order” this multivariate data in order to produce a low dimensional picture (i.e., a graph in 1-3 dimensions). Just like cluster analysis, we wil use similarity metrics to accomplish this. Also like cluster anlaysis, simple ordination is not a statistical test: it is a method of visualizing data. Figure 8.1: Synthetic axis rotation in ordination Figure 8.2: Data reduction ordination 8.1 Ordination as data reduction Essentially, we find axes in the data that explain a lot of variation, and rotate so we can use that axis as one of our dimensions of visual representation 8.1. Another way to think about it, is that we are going to summarize the raw data, which has many variables (p) byt a smaller set of derived (or synthetic or composite) variables, k 8.2. If the ordination is informative, it reduces a large number of original correlated variables to a small number of new uncorrelated variables. But it really is a bit of a balancing act between clarity of representation (and ease of understanding), and oversimiplication. We will lose information in this data reduction, and if that information is important, then we can make the multivariate data harder to understand! Also note that if the original variables are not correlated, then we won’t gain anything with ordinaton. 8.2 Methods of Ordination There are lots of different ways to perform an ordination, but most methods are based on extracting the eigenvalues of a similarity matrix. The four most commonly used methods are: Principle Component Analysis (PCA), which is the main eigenvector-based method, Correspondence Analysis (CA) which is used used on frequency data, Principle Coordinate Analysis (PCoA) which works on dissimilarity matrices, and Non Metric Multidimensional Scaling (NMDS) which is NOT an eigenvector method, instead it represents objects along a predetermined number of axes. Table 8.1: Domains of Application of Ordination Methods (adpated from Legnedre &amp; Lengendre 2012) Method Distance Variables Principal component analysis (PCA) Euclidean Quantitative data Correspondence analysis (CA) X^2 Non-negative, quantitiative or binary data; species frequencies or presence/absence data Principal coordinate analysis (PCoA), metric (multidimensional) scaling, classical scaling Any Quantitative, semiquantitative, qualitative, or mixed Nonmetric multidimensional scaling (nMDS) Any Quantitative, semiquantitative, qualitative, or mixed Legendre &amp; Legendre (2012) provide a nice summary of when you should use each method 8.1 8.3 Principal Components Analysis (PCA) Principal Components Analusis is probably the most widely-used and well-known of the “standard” multivariate methods. It was invented by Pearson (1901) and Hotelling (1933), and first applied in ecology by Goodall (1954) under the name “factor analysis” (NB “principal factor analysis” is a synonym of PCA). Like most ordination methods, PCA takes a data matrix of n objects by p variables, which may be correlated, and summarizes it by uncorrelated axes (principal components or principal axes) that are linear combinations of the original p variables. The first k components display as much as possible of the variation among objects. PCA uses Euclidean distance calculated from the p variables as the measure of dissimilarity among the n objects, and derives the best possible k dimensional (k &lt; p) representation of the Euclidean distances among objects. Figure 8.3: Selecting the synthetic axes in ordination We can think about this spatially. Objects are represented as a cloud of n points in a multidimensional space with an axis for each of the p variables. So the centroid of the points is defined by the mean of each variable, and the variance of each variable is the average squared deviation of its n values around the mean of that variable (i.e., \\(V_i= \\frac{1}{n-1}\\sum_{m=1}^{n}{(X_{im}-\\bar{X_i)}^2}\\)). The degree to which the variables are linearly correlated is given by their covariances \\(C_{ij}=\\frac{1}{n-1}\\sum_{m=1}^n{(X_{im}-\\bar{X_i})(X_{jm}-\\bar{X_j})}\\). The objective of PCA is to rigidly rotate the axes of the p-dimenional space to new positions (principal axes) that have the following properties: they are ordered such that principal axis 1 has the highest variance, axis 2 has the next highest variance etc, and the covariance among each pair of principal axes is zero (the principal axes are uncorrelated) 8.3 \\(1^{st}\\) Principle component PC axes are a rigid rotation of the original variables. Dimentionality reduction is the same as first rotating the data with the eigenvalues to be aligned with the principle components, then using only the components with the greatest eigenvalues. Let’s try an example. We’re going to use a sample dataset in R and the base R version of PCA to start exploring this data analysis technique. Get the iris dataset into memory by typing “data(“iris”). Take a look at this dataset using the head, str or summary functions. For a multivariate data set, you would like to take a look at the pairwise correlations. Remember that PCA can’t help us if the variables are not correlated. Let’s use the pairs() function to do this as: pairs(iris[1:4],main=“Iris Data,” pch=19, col=as.numeric(iris$Species)+1) data(&quot;iris&quot;) str(iris); summary(iris[1:4]) &#39;data.frame&#39;: 150 obs. of 5 variables: $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Sepal.Length Sepal.Width Petal.Length Petal.Width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 pairs(iris[1:4],main=&quot;Iris Data&quot;, pch=19, col=as.numeric(iris$Species)+1) The colours let us see the data for each species, the plots are the pairwise plotting of each pair of the 4 variables. Do you see any correlations? If there seem to be some correlations we might use PCA to visualize the 4 dimensional variable space. Let’s rush right in and use the prcomp() function to run a PCA on the numerical data in the iris dataframe. Save the output from the function to a new variable name so you can look at it when you type that name. The str() function will show you what the output object includes. If you use the summary() function, R will tell you what proportion of the total variance is explained by each axis. Do you notice anything? Hmmm, that’s a LOT of variance on axis one. Let’s examine the variance in the raw data. Use the apply() function to quickly calculate the variance in each of the numeric columns of the data as apply(iris[,1:3], 1, var). What do you see? Are the variances of each the columns comparable? pca &lt;- prcomp(iris[,1:4]) summary(pca) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 2.0563 0.49262 0.2797 0.15439 Proportion of Variance 0.9246 0.05307 0.0171 0.00521 Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 apply(iris[,1:4], 2, var) Sepal.Length Sepal.Width Petal.Length Petal.Width 0.6856935 0.1899794 3.1162779 0.5810063 If you look at the help menu, the notes for the use of prcomp() STRONGLY recommend standardizing the data. To do this there is a built in option. We just need to set scale=TRUE. Let’s try again with data standardization. Save your new PCA output to a different name. We’ll compare to the unstandardized data in a moment. Take a look at the summary. pca2 &lt;- prcomp(iris[,1:4], scale=TRUE) summary(pca2) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 1.7084 0.9560 0.38309 0.14393 Proportion of Variance 0.7296 0.2285 0.03669 0.00518 Cumulative Proportion 0.7296 0.9581 0.99482 1.00000 That variance is now a bit more evenly distributed… Text about standardization here. Now we need to determine how many axes to use to interpret our analysis. For 4 variables its easy enough to just look that the amount of variance. For larger numbers of variables a plot can be useful. The screeplot() function will output the variance explained by each of the principle component axes, and you can make a decision based on that (e.g., screeplot(pca2, type=“lines”)) Hate this…. How does this work exactly? The steps are 1. Standardize the data 2. Compute the variance-covariance matrix of the standardized data(same as correlation matrix for standardized data) 3. Calculate the eigenvalues of the variance-covariance matrix 4. Calculate the associated eigenvectors Then, the jth eigenvalue is the variance of the jth principle component and the sum of all the eigenvalues is the total variance explained. The proportion of variance explained by each component is the eigenvalue for the component divided by the total variance explained, while the loadings are the eigenvectors NB "],["optimization.html", "9 Optimization 9.1 Introduction 9.2 Fundamentals of Optimization 9.3 Regression 9.4 Iterative Optimization Algorithms 9.5 Calibration of Dynamic Models 9.6 Other Topics 9.7 References", " 9 Optimization 9.1 Introduction To improve is to make better; to optimize is to make best. More generally, optimization is the act of identifying the extreme (cheapest, tallest, fastest…) case over a collection of possible cases. You may recall studying optimization in introductory calculus. In that case you likely solved design problems, for which the collection of possibilities was specified as possible dimensions of a farmer’s fence, or of a box, or something similar. Optimization over design space (also called decision space) is a critical feature of many engineering tasks, and has a role in most areas of applied science, including applied biology. Examples include optimal manipulation of biological systems (e.g.~optimal harvesting, or optimal drug dosing), optimal design of biological systems (e.g.~robust synthetic genetic circuits). A complementary task is optimal experimental design, which aims to identify the ‘best’ experiment from a collection of possible designs. Model calibration, to be discussed further below, provides another example; here we seek the ‘best fit’ version of a model from a collection of possible options. Optimization can also be used to investigations of natural systems (i.e.~in `pure’ science), in cases where we have reason to believe that ‘nature’ presents us with an optimal version of a given phenomenon. The principle of entropy maximization (or energy minimization) can be used to justify nature of a wide variety of phenomena, from the shapes of soap bubbles (why are they round?) to the configuration of proteins [maybe add distributions: check Nelson book for examples]. Darwinian evolution provides another avenue for optimization. Assuming evolution has arrived at (or near) an optimum, we can apply optimization to understand a wide array of biological phenomena, from metabolic flux distribution (FBA), to brain wiring (ref needed: check Banga), to optimal foraging strategies (ref needed). This module will introduce optimization methods in R. Applications and illustrations will be drawn from a range of biological domains. The simple optimization tasks addressed in introductory calculus could be accomplished with paper-and-pencil calculations. As shown below, while the fundamental principles introduced by those exercises carry forward, most optimization tasks of interest in biology demand more extensive computational resources. 9.2 Fundamentals of Optimization Figure 1 illustrates some basics terminology associated with optimization. The graph of a function \\(f\\) of a single variable \\(x\\) is shown, over a domain \\([a,b]\\). In the context of optimization, we can think of each \\(x\\)-value in the interval \\([a,b]\\) as one possible scenario (fence measurement, harvesting rate, etc.). The function \\(f\\) maps those choices to some objective (e.g.~total area, long-term annual yield) that we wish to optimize (either maximize or minimize). The points labelled in the figure capture a central dichotomy in the theory of optimization. The global extrema (i.e. maximum, minimum) represent the results we wish to achieve. Figure 1: Extreme Values More generally, we define local extrema (max, min) as results that appear to be optimal if we restrict our attention only to ‘nearby’ possibilities (i.e.~\\(x\\)-values). There is an extensive theory of optimization methods; unfortunately most of these are dedicated to identifying local optima. These approaches cannot directly identify global optimum; at best they can identify candidate global optima, which then can be compared to identify the ‘best’ result. (A convenient special case occurs for functions in which every local optimum is a global optimum; these are called convex optimization problems. Unfortunately, they occur only rarely in addressing biological phenomena.) To illustrate these fundamentals, consider the following two academic examples that may be familiar form an introductory calculus course. These both rely on Fermat’s Theorem, which states that local extrema occur at points where the tangent line to a function’s graph is horizontal (so the derivative (i.e.~the slope of the tangent) is zero (Figure 2). Example 1. Identify the value of \\(x\\) for which the function \\(f(x) = x^2+3x-2\\) is minimized. Solution: Taking the derivative, we find \\(f&#39;(x) = 2x+3\\). To find the single point where the derivative is zero, we solve: \\(f&#39;(x) = 0 \\Leftrightarrow 2x+3 = 0\\Leftrightarrow x = -\\frac{3}{2} = -1.5\\). As shown in Figure 4, the single local minimum is the global miminum in this case, so \\(x=-1.5\\) is the desired solution. Example 2. Identify the value of \\(x\\) for which the function \\(f(x) = 3x^4-4x^3-54x^2+108x\\) is minimized. Solution: Taking the derivative, we find \\(f&#39;(x) = 12x^3-12x^2-108x+108 = 12(x-3)(x-1)(x+3)\\). To find the single point where the derivative is zero, we solve: \\(f&#39;(x) = 0 \\Leftrightarrow x = 3, 1, -3\\). As shown in Figure 4, two of these points are local minima. One (\\(x=-3\\)) is where the global minimum occurs. Figure 2: Fermat’s Theorem: local extrema occur at points where the tangent line is horizontal 9.3 Regression 9.3.1 Linear Regression As mentioned above, finding the ‘best fit’ from a family of models is a common optimization task in science. The simplest (and most frequently used) example is linear regression: determining a line of best fit through a given set of points. To illustrate, consider the dataset of \\((x,y)\\) pairs shown in Figure XYZ. Several lines are displayed in the figure. The optimization task here is to identify the ‘best’ line: the one that best captures the trend in the data. To specify this task mathematically, we need to agree on a measure of ‘quality of fit.’ We start by recognizing that the line will (typically) fail to pass through many of the points in the dataset. (In fact, the ‘best’ line, by the definition we’ll adopt, will typically fail to pass through any of the points in the dataset.) Thus, at each point \\(x_i\\) we can define an error, which is the difference between the observed \\(y\\)-value and the \\(y\\)-value predicted by the line. If we call the line \\(y=mx+b\\), then the error at \\(x_i\\) will be \\(y_i-(mx_i+b)\\). This definition of the error is not satisfactory: the errors associated with points above and below the line will have opposite signs, and so will cancel when we attempt to tally the total error. The problem could be addressed by taking the absolute value \\(|y_i-(mx_i+b)|\\), but it turns out that squaring the error is a more satisfactory solution for a number of theoretical reasons; we will follow this standard approach. This, we consider the squared error at \\(x_i\\) as \\((y_i-(mx_i+b))^2\\), and tally these together into the sum of squared errors (SSE): \\((y_1-(mx_1+b))^2 + (y_2-(mx_2+b))^2 + \\ldots (y_N-(mx_N+b))^2\\). We can now pose the model-fitting task as an optimization problem. For each line (that is, each assignment of numerical values to \\(m\\) and \\(b\\)), we associate a corresponding SSE. We seek the values of \\(m\\) and \\(b\\) for which the SEE is a global minimum. Figure 3: Example graph of \\(f(x) = x^2+3x-2\\) Example 3. Consider a simplified version of linear regression, in which we know that our model (line) should pass through the origin (0,0). That is, instead of lines \\(y=mx+b\\), we will consider only lines of the form \\(y=mx\\). (We thus have a single parameter to identify: the slope \\(m\\).) To keep the algebra as simple as possible we’ll take a tiny dataset consisting of just two points: (2,3) and (5.4), as indicated in Figure 5. The line passes through points \\((2,2m)\\) and \\((5, 5m)\\). In this simple case, the sum of squared errors is: \\[\\begin{equation*} \\mbox{SSE} = e_1+e_2 = (3-2m)^2+(5m-4)^2 \\end{equation*}\\] To apply Fermat’s theorem we take the derivative and identify any values of \\(m\\) for which it is zero: \\[\\begin{eqnarray*} \\frac{d}{dm} \\mbox{SSE} &amp;=&amp; 2(3-2m)(-2)+2(5m-4)(5) \\\\ &amp;=&amp; -4(3-2m) + 10(5m-4) = -12 +8m+50m-40 = -52+58m. \\end{eqnarray*}\\] The only local extremum (and hence only candidate for global minimum) is then \\(m=52/58 = 26/29\\). (Additional analysis can confirm this is indeed the global minimum.) The analysis in Example 3 can be extended to determine the general solution of the linear regression task. The solution formula is as follows: Linear regression formula The best fit line \\(y=mx+b\\) to the dataset \\((x_1, y_1)\\), \\((x_2, y_2)\\), , \\((x_n, y_n)\\) is given by \\[\\begin{eqnarray*} m&amp;=&amp; \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\\\ b&amp;=&amp; \\bar{y}-m\\bar{x}, \\\\ \\end{eqnarray*}\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) are the averages \\[\\begin{eqnarray*} \\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i \\ \\ \\ \\bar{y}=\\frac{1}{n} \\sum_{i=1}^n y_i \\end{eqnarray*}\\] Figure 4: Example graph of \\(f(x) = 3x^4-4x^3-54x^2+108x\\) Figure 5: Example 1: finding a line of best fit through the origin This formula is somewhat unwieldy, but is straightforward to implement. In R, the command lm implements this formula, as the following example illustrates. *****linear regression R exercise here**** 9.3.2 Nonlinear Regression Nonlinear regression is the task of fitting a nonlinear model through a dataset. The setup for this task is identical to linear regression. We begin by selecting a parameterized family of models (curves), and aim to identify the curves that minimizes the sum of squared errors when compared to the data. The family of models can be chosen based on an understanding of the mechanism that relates the input and output data. For example, if we are investigating the rate law for a single-substrate enzyme-catalysed reaction, we might choose the family of curves specified by Michaelis-Menten kinetics: \\(y = \\frac{V_{\\mbox{max}} S}{K_M+S}\\). Our goal would then be to idntify values for the parameters \\(V_{\\mbox{max}}\\) and \\(K_M\\) that minimize the sum of squared errors when comparing with the observed data. Regression via linearizing transformation In several important cases, the nonlinear regression task can be transformed into a linear regression task. In the case of Michaelis-Menten kinetics, several linearizing transformations have been proposed, including those by Eadie–Hofstee and Lineweaver–Burk. Another example commonly encountered in Biology is fitting exponential curves (e.g.~population growth or drug clearance). In those cases, a logarithmic modifies the data so that a linear trend is captured. data: \\(y \\sim e^{rt}\\) transform \\(y^{trans} \\sim \\ln (e^{rt}) = \\times \\ln(e^{rt}) = rt\\). Linear regression on the transformed data \\((t_i, y^{trans}_i)\\) then provides an estimate of the value of \\(r\\). Unfortunately, linearizing transformations are only available in a handful of special cases. In general, the nonlinear regression task must be addressed directly. An example of the procedure in R follows: We need to provide the starting values \\(V_m\\) and \\(K\\). \\(V_m\\) is the maximum value, the asymoptote of the curve. The value of \\(K_m\\) is equal to \\(S\\) when \\(V_m\\) is half-way (\\(\\frac12 V_m\\)). ## ## Formula: v ~ Vm * S/(K + S) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## K 0.4398016 0.0311612 14.11 8.1e-11 *** ## Vm 0.0054252 0.0001193 45.47 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0001727 on 17 degrees of freedom ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 4.666e-07 Below is the plot of the original points and the fitted curve using nls(). From that exercise, the reader might have the impression that nonlinear regression and linear regression are very similar exercises. That would be a false impression. Although the problem set-up is similar (chose family of curves, minimize SSE), the optimization task is different, and it’s ‘solution’ follow a very different protocol. As we saw above, the solution to the linear regression task can be stated as a general formula. For nonlinear regression, no such formula exists. Worse, there is no procedure (algorithm) that is guaranteed to find the solution! 9.4 Iterative Optimization Algorithms The best techniques for addressing the general nonlinear regression task are iterative global optimization routines. As we’ll cover below, these algorithms all follow a basic idea: we start with an initial ‘guess’ of what the solution (best-fit parameters) might be, and then take steps through the parameter space to improve the quality of the solution. In the exercise above, the nls command executed a particularly simple version of this kind of algorithm. That’s why the nls requires that the user supply an ‘initial guess.’ This function also allows optional arguments that specify the number of steps (iterations) that the algorithm will take, and also the specific algorithm to be used (from a pre-specified set of choices). 9.4.1 Gradient Descent Perhaps the simplest iterative optimization algorithm is gradient descent, which can be understood intuitively in terms of finding your way to a valley bottom in a think fog. The fog obscures your vision so that you can only detect changes in elevation in your immediate vicinity. To make your way to the valley bottom, it would be reasonable to take each step of your journey in the direction of steepest decline. This strategy, illustrated in figure XYZ, is guaranteed to lead to a local minimum, but cannot guarantee arrival at the lowest point: the global minimum. Mathematically, the local change in elevation is determined by evaluating the objective at points nearby the ‘current position,’ and then using those to determine the direction of steepest descent. (technically, this involves a linearization of the function at the current position, or equivalently a calculation of the ‘gradient vector.’) A ‘step’ is then taken in the direction, and the process is repeated form this new ‘current position.’ Beyond that basic idea, a number of details have to be specified: how long should each step be? How many steps should be taken? Or should there be some other ‘termination condition’ that will trigger the end of the journey? (Each of these involve a tradeoff: often of precision vs. execution time. For instance, small steps guarantee a smooth path down the steepest route (as water would follow), but can take a very long time to complete the journey. Termination conditions are often specified in terms of the local topography: the algorithm stops when the ‘current position’ is at a sufficiently flat point (no downhill direction detected). First, let’s work with a simple function with global minimum only. We start from different initial points and plot the iteration steps to see if they result in the same spot. It can be seen from the plot that the global minimum found are the same no matter where you start. Then let’s take a look at a more complex function which has local minimum and global minimum. It can be seen that different starting points give different results. Choice of starting point matters, and it will not always give the optimal value (global minimum). Below is the plot of the same function from a different perspective to better review the structure. The interactive plot below allows you play around with the function. The default optimization algorithm used by nls is the Gauss-Newton method, which is a generalization of Newton’s method for solving nonlinear equations (which may be familiar from an introductory calculus). This is a refinement of gradient descent in which the local curvature of the function is used to project the position of the bottom of the local valley assuming that the surface is shaped like a parabola. On parabolic surfaces, this achieves descent to the bottom in one iteration. On non-parabolic surfaces (i.e.~most surfaces), the results is a sequence of iterations, but often many fewer than would be required using gradient descent. Both gradient descent and Gauss-Newton method are design to reach the bottom of the valley in which the initial guess lies. If that’s a local minimum but not the global minimum, then the algorithm will not be successful. So, how does one select a ‘good’ initial guess? Unfortunately, there’s no general answer to this question. In many cases, one can use previous knowledge of the system under investigation to begin with a solid initial guess. (This is closely related to the choice of a prior distribution in Bayesian approaches.) If no such previous knowledge is available, sometimes a `guess’ is all we have. In those cases, we may have little confidence that the algorithm will arrive at a global minimum. The simplest way to gain some confidence of achieving a global minimum is the multi-start strategy: choose many initial guesses, and run the algorithm from each. This can be computationally expensive, but if the same minimum is reached from initial spread widely over the parameters space, one begins to gain confidence that this is the true global minimum. A number of methods have been developed to complement the multi-start approach. These are known as global optimization routines. They are also known as heuristic methods, because their performance cannot be guaranteed in general: there are no guarantees that they’ll find the global minimum, nor are there solid estimates of how many iterations will be required for them to carry out a satisfactory search of the parameter space. We will consider two commonly used methods: simulated annealing and genetic algorithms. 9.4.1.1 Simulated Annealing Simulated annealing is motivated by the process of annealing: a heat treatment by which the physical properties of metals can be altered. This is an iterative algorithm; as in gradient descent, the algorithm starts at an initial guess, and then steps from point to point in the search space. However, there is a random element to the process followed by simulated annealing: that means that the path followed from a particular initial condition won’t be repeated if the algorithm is run again from the same point. (Algorithms that incorporate randomness are often referred to as Monte Carlo methods, after the European gambling centre.) At each step, the algorithm begins by identify a candidate next position. (This point could be selected by a variant of the gradient descent step, or some other method). The value of the objective at this candidate point is then compared with the objective value at the current point. If the objective is lower at the new point (i.e.~this step takes the algorithm downhill), then the step is taken and a new iteration begins. If the value of the objective is larger at the candidate (i.e.~the step makes things worse), the step can still be taken, but only with a small probability. Both the size of the candidate steps and the probability of accepting ‘wrong’ (uphill) steps are tied to a cooling schedule: a decreasing `temperature’ profile: at high temperatures, large steps are considered and ‘wrong’ steps are taken frequently; as the temperature drops, only smaller steps are considered, and fewer ‘wrong’ steps are allowed. (By analogy, imagine a ping-pong ball resting on a table into which has been carved a rolling landscape. One strategy to move the ball to the lowest valley bottom is to shake the table to jostle it about. Mirroring simulated annealing, we would begin by applying violent shakes (high temperature) which would result in the ball bouncing across much of the table. By slowly reducing the severity of the shaking, the ball would settle into a local minimum at a valley bottom. The hope is that if the cooling schedule is well-chosen, the ball would have sampled many valleys, and would end up at the bottom of the lowest. Simulated annealing is often combined with a multi-start strategy to further ensure wise sampling of the search space. Let’s first solve the complicated function above using simulated annealing using the same initial points as above. ## $value ## [1] -4.906786 ## ## $par ## [1] 2.0000000 0.6014984 ## ## $counts ## [1] 52490 ## $value ## [1] -4.906786 ## ## $par ## [1] 2.0000000 0.6014984 ## ## $counts ## [1] 54649 ## $value ## [1] -4.906786 ## ## $par ## [1] 2.0000000 0.6014984 ## ## $counts ## [1] 53020 ## $value ## [1] -4.906786 ## ## $par ## [1] 2.0000000 0.6014984 ## ## $counts ## [1] 52325 ## $value ## [1] -4.906786 ## ## $par ## [1] 2.0000000 0.6014984 ## ## $counts ## [1] 52470 From the results, we can see that although the initial points are different, they ended up with the same optimized value. The plot below shows how simulated annealing works. The red line records the current minimum. The blue points represents function value. It can be seen that although the actual minimum is found, there is still a probability to jump around to make sure that is the global minimum. The example below looks like an egg carton, which has many local minima. Here we started from two different starting points and they resulted in the same final position. ## $value ## [1] -306.7205 ## ## $par ## [1] -76.14595 191.15515 ## ## $counts ## [1] 51555 ## $value ## [1] -306.7205 ## ## $par ## [1] -76.14595 191.15515 ## ## $counts ## [1] 52120 The plot below shows how simulated annealing works for the two initial points respectively. The red line records the current minimum. The blue points represents function value. Although they have different starting position, it can be seen from the plots the optimized values are the same. We next consider a heuristic algorithm in which multiple paths through the search space are followed simultaneously. 9.4.1.2 Genetic Algorithms Genetic algorithms are inspired by Darwinian evolution. The algorithm begins with the specification of a population of initial guess points. At each iteration of the algorithm, this population ‘evolves’ toward improved estimates of the global minimum. This ‘evolution’ step involves three substeps: selection, mutation, and cross-over. In the selection step, the population is pruned by removing a fraction that are not sufficiently fit (where fitness corresponds to the value of the objective function being minimized). Then, mutations are introduced into the remaining population by introducing small random perturbations to their position in the search space. Finally, a new generation is generated by crossing members of the current population; this can be done in several ways, the simplest is to generate crosses as averages of the numerical values of the two ‘parents.’ The expectation is that, through several generations, this process will lead to a population with high fitness (minimal objective) that has thoroughly explored the search space (and may consist of subpopulations at local minima as well as representatives at the global minimum. Genetic algorithms are a subest of the more general class of evolutionary algorithms all of which involve simultaneous ‘exploration’ of the search space through multiple paths. We use the complicated function example with genetic algorithm. The built-in ga() function maximizes the objective function. In this case, to get the global minimum, a negative sign needs to be added in front of the function.The larger the maximum iteration is, the closer the value is to the actual global minimum. The seed can also be set to different values, but the final optimized value would be the same. The plot shows how optimal value converges. Although it is shown as positive values, we need to manually add a negative sign. From the summary below, the fitness function value and the parameters are very close to what we got before using simulated annealing. ## ── Genetic Algorithm ─────────────────── ## ## GA settings: ## Type = real-valued ## Population size = 50 ## Number of generations = 3000 ## Elitism = 2 ## Crossover probability = 0.8 ## Mutation probability = 0.1 ## Search domain = ## x1 x2 ## lower -2 -2 ## upper 2 2 ## ## GA results: ## Iterations = 3000 ## Fitness function value = 4.900531 ## Solution = ## x1 x2 ## [1,] 1.99625 0.5972324 Example: maybe biexponential or not, depending on complexity of cost surface. 9.5 Calibration of Dynamic Models The principles of nonlinear regression carry over directly to calibtration of more complex models. In particular, in many domains of biology, dynamic models are used to describe the time-varying behaviour of systems (from biomolecular networks to cell-cell interactions to physiology to ecology). These models take many forms, but a commonly used formulation is a model based on ordinary differential equations (i.e. rate equations). These models are deterministic (cannot incorporate random effects) and assume that the dynamics occur in a spatially homogeneous environment (they capture spatially distributed phenomena). Despite these limitations, these models can describe a wide variety of dynamic behaviours, and so are useful starting points for investigations across biology. Ordinary differential equation models used in biology often take the form \\[\\begin{equation*} \\frac{d}{dt} {\\bf x}(t) = {\\bf f}({\\bf x}(t), {\\bf p}) \\end{equation*}\\] where components of the time-varying vector \\({\\bf x}(t)\\) are the states of the system (e.g.~population sizes, molecular concentrations), the components of vector \\({\\bf p}\\) are the model parameters: numerical values that represent fixed features of the system and its environment (e.g. interaction strengths, temperature, nutrient availability), and the vector-valued function \\({\\bf f}\\) describes the rate of change of the state variables. As a concrete example, consider the Lotka-Volterra equations, a classical model to describe interacting predator and prey populations [ref]: \\[\\begin{eqnarray*} \\frac{d}{dt} x_1(t) &amp;=&amp; p_1 x_1(t) - p_2 x_1(t) x_2(t)\\\\ \\frac{d}{dt} x_2(t) &amp;=&amp; p_3 x_1(t) x_2(t) - p_4 x_2(t)\\\\ \\end{eqnarray*}\\] Here \\(x_1\\) is the size of the prey population; \\(x_2\\) is the size of the predator population. The prey are presumed to have access to resources that allow exponential growth in the absence of predation (growth at rate \\(p_1 x_1(t)\\). Interactions between prey and predator populations (assumed to occur at rate \\(x_1(t) x_2(t)\\) lead to an decrease in the prey population and an increase in the predator population (characterized by parameters \\(p_2\\) and \\(p_3\\) respectively). Finally, the prey suffer an exponential decline in population size in the absence of prey (decay rate \\(p_4 x_2(t)\\)). A simulation of the model is shown in Figure. Note that simualtion of the model requires specification of (i) values for each of the model parameters, and (ii) initial conditions, i.e.~the size of each population at time \\(t=0\\). Figure XYZ shows a dataset the corresponds to observations of a predator-prey population system. To calibrate the model Lotka-Volterra to this dataset we seek values for the four parameters \\(p_1\\), \\(p_2\\), \\(p_3\\), and \\(p_4\\) for which simulations of the model provide the ‘best fit.’ As in the regression tasks described previously, the standard measure for quality of fit is the sum of squared errors. We thus proceed with a minimization task: for each simulation of the model, we compare with the dataset and determine the sum of squared errors. We aim to minimize this fit over the space of model parameters. There’s one additional feature we must account for: to specify a simulation we need numerical values for the model parameters and the initial conditions. Calling these initial conditions \\(p_5 = x_1(0)\\) and \\(p_6=x_2(0)\\), we are then pose our optimization problem as a search over a six-dimensional parameter space. In what follows, we illustrate the use of both simulated annealing and a genetic algorithm to calibrate this model. Matt: insert example 9.6 Other Topics 9.6.1 Optimal Control The behaviour of the Lotka-Volterra model described above depends on the values of the model parameters. Such models are often used to explore the effect of perturbations on a system. For instance, after successfully calibrating a model to given population’s dynamics, one could propose interventions that could alter the value of one of the model parameters, e.g.~by restricting access to resources required for growth of the prey population. We could then simulate the model under this altered value of \\(p_1\\) and thus generate predictions of the effect of this manipulation. Alternatively, we could use a model formulation that incorporates a time-varying perturbation. As an example, consider a version of the Lotka-Volterra equations that includes a term to describe removal (harvesting/culling) of the predator population: \\[\\begin{eqnarray*} \\frac{d}{dt} x_1(t) &amp;=&amp; p_1 x_1(t) - p_2 x_1(t) x_2(t)\\\\ \\frac{d}{dt} x_2(t) &amp;=&amp; p_3 x_1(t) x_2(t) - p_4 x_2(t) - u(t) x_2(t) \\end{eqnarray*}\\] Here \\(u(t)\\), called an input signal is a function that represents the effort exerted in removal of predators. With this input signal in place, we can now use the model to explore the consequences of a range of removal schedules. In particular, optimization can be used to identify the removal strategy that best achieves some performance goal. As a concrete example, consider the goal of maximizing harvest over some fixed time period (e.g.~a year). Let’s consider two cases. To begin, suppose that the harvesting rate must be fixed throughout the system behaviour. In this case, the harvesting rate is constant. Let’s call it \\(u(t) = u_0\\). We can then account for the total harvest by introducing a new state variable that tracks the progress of the harvest: \\[\\begin{eqnarray*} \\frac{d}{dt} x_1(t) &amp;=&amp; p_1 x_1(t) - p_2 x_1(t) x_2(t)\\\\ \\frac{d}{dt} x_2(t) &amp;=&amp; p_3 x_1(t) x_2(t) - p_4 x_2(t) - u(t) x_2(t)\\\\ \\frac{d}{dt} x_3(t) &amp;=&amp; u(t) x_2(t) \\end{eqnarray*}\\] Then the value of \\(x_3(t)\\) is the accumulated harvest from time zero to time \\(t\\). If our goal is to maximize the total harvest over a year, we aim to maximize \\(x_3(1)\\) (where \\(t\\) is in units of years) over the choice of the fixed harvesting effort \\(u_0\\). [Details of implementation; exercise] Let’s now consider the case in which we can change the harvesting effort through the year: we’ll allow the input \\(u(t)\\) to be a time-varying function. One would expect we’d be able to achieve a larger harvest in this case (compared with a fixed harvest rate), because there’s more freedom in the design of the harvesting strategy. We’ll see below that this is indeed true. However, the optimization task as currently posed is problematic. So far, we’ve been optimizing over functions (SSE, total harvest) that depend on a finite set of numerical values, and so we searched a finite-dimensional space for the extrema. We’re now faced with optimizing over any function \\(u(t)\\). This is what’s technically called an infinite-dimensional search space. The search for extrema in this case has been investigated in the theory of optimal control and closely related work in the calculus of variations. See (Lenhart and Workman, 2007) if you’d like to learn more. Here, we’ll restrict ourselves to a simple short-cut for solving this problem. We need to find a way to parameterize the set of possible input curves \\(u(t)\\). A common strategy is as follows: Sample-and-hold input parametrization We suppose that the input \\(u(t)\\) (harvesting effort) is a piece-wise constant function, which changes values only at pre-specified timepoints (e.g. monthly, as shown in figure XYZ). Then we can specify any such function as a set of 12 numerical values, one for each month: (\\(u_1\\), \\(u_2\\), \\(u_3\\), …, \\(u_{11}\\), \\(u_{12}\\)). As youd expect, the optimization task (which is now an optimal control task) can be solved by searching for the maximizer of total harvest over this 12-dimensional search space. More details in Lin et al. 2014. [Implementation]; note: easiest way to implement is as a series of separate simulations. Also note: solution is not differentiable at timepoints where \\(u\\) jumps. exercise: change number of sub-intervals, note increase/decrease in maximum achieved] 9.6.2 Uncertainty Analysis and Bayesian Approahces The regression tasks discussed above resulted in estimates of parameter values based on the provided datasets. Regression is usually followed by uncertainty analysis, which provides some measure of confidence in those estimated parameter values. For instance, in the case of linear regression, 95% confidence intervals on the estimates (best fit line slope and intercept) and corresponding confidence intervals on the model predictions are supported by extensive statistical theory, and can be easily generated in R. The plot below shows 95% confidence intervals on the model predictions. The confint() function finds 95% confidence intervals on the estimates (best fit line slope and intercept). ## 2.5 % 97.5 % ## (Intercept) -31.167850 -3.990340 ## speed 3.096964 4.767853 This can also be derived using the confidence interval formula: \\(\\text{estimate}\\pm \\text{critical value}\\times\\text{standard error}\\) ## lower upper ## 3.096964 4.767853 ## lower upper ## -31.16785 -3.99034 For nonlinear regression (including calibration of dynamic models), the theory is less helpful, but estimates of uncertainty intervals can be achieved (as discussed below in the section on optimal experimental design). ## Waiting for profiling to be done... ## 2.5% 97.5% ## K 0.379645262 0.508891354 ## Vm 0.005184292 0.005681209 ## lower upper ## 0.005173515 0.005676949 ## lower upper ## 0.3740573 0.5055459 An alternative approach to the regression task combines calibration and uncertainty in a single process: Bayesian methods. The basic idea behind Bayesian analysis (founded on Bayes Theorem, which may be familiar from elementary probability), is to start with an initial parameter estimate (analogous to the initial guess needed in nonlinear regression) and then use the available data to refine that estimate. The difference is that instead of the initial guess and refined estimate being single numerical values, they are distributions. In Bayesian terminology, we being with a prior distribution, which may be informed by expert knowledge (e.g.~a normal distribution centered at a good initial guess), or may be a wilde guess (e.g.~a uniform distribution over a wide range of possible values). Application of a Bayesian calibration scheme uses the available data to generate an improved description of the parameter value distributions, called the posterior distribution. A successful Bayesian calibration could take a `wild guess’ uniform prior and return a tightly-centered posterior, as in figure XYZ. Uncertainty can then be gleaned directly from the posterior, by reporting, e.g.~ a 95% confidence interval. One commonly cited concern with Bayesian techniques is the dependence of the result on the rather subjective selection of a prior. This is analogous to concerns about any bias introduced by choice of initial guess in the non-Bayesian (maximum likelihood-based) regression analysis described above. In both cases, multi-start techniques can be used to address this issue. Here we’ll consider a simple numerical implementation of a Bayesian approach: approximate Bayesian computation. This approach is based on a simple idea: the rejection method, in which we sample repeatedly from the prior distribution and reject all samples that do not satisfy a pre-specified tolerance for quality of fit. (This is reminiscent of the selection step in genetic algorithms: culling unfit members of a population.) For the Michaelis-Menten example, the estimated values for \\(K\\) and \\(V_m\\) are 0.4398016 and 0.0054252 respectively. Now we simulate numbers for \\(K\\) and \\(V_m\\), and accept the value if the distances between the simulated values and the true values are under the threshold. Here we randomly sample \\(K\\) and \\(V_m\\) 200000 times, it can be seen from the histogram that the values that occur most frequently are close to the estimated values derived from the non linear regression model. ## [1] 867 Typically, approximate Bayesian computation is implemented as an iterative method, in which a sequence of rejection steps is applied, withe the prior being refined at each step (generating, in essence, a sequence of posterior distributions, the last of which is considered to be the best description of the desired parameter estimates). [Implementation of ABC method from package; Michaelis-Menten and Lotka Volterra] 9.6.3 Optimal Experimental Design Experimental design involves choices about what experiments to perform, what measurements to take, and, for time-varying processes, when to take those measurements. Most experimental design exercises are carried out in the face of constraints on available resources (time, personnel, reagents, funding, etc.). The goal of experimental design is then to identify the ‘best’ experiments to execute, given the imposed resource constraints. In cases where we can express the ‘quality’ experimental results in terms of a numerical performance, and can likewsise express constraints numerically, we can then frame the experimental design task as an optimization task; this is optimal experimental design. There is a rich history of optimal experimental design for linear systems (Pukelsheim, 2006.). Here, we consider a more challenging task of optimal experimental design in a nonlinear setting. Consider the case in which a nonlinear regression has been applied to find a best fit model to some pre-existing data, along with an estimate of the quality of that fit. We then consider an experimental design task that targets the refinement of that estimate: the `best’ experimental results will be those that most shrink the confidence intervals when that data is included in the regression analysis. We quantify the degree of confidence in the parameter estimate with the Fisher Information Matrix (FIM) which is constructed from (i) the sensitivities of the model predictions to the values of the model parameters, and (ii) the variance in the data. The sensitivities account for how much effect a changing the value of model parameters affects the model predictions that will be used for calibration. If the parameter values have little influence over those predictions, then we can’t expect to have much confidence in estimate the parameter values based on tuning of those predictions. Likewise, if the dataset shows wide variation, then we’ll expect to have less confidence from tuning the model to agree with the observed behaviours. A simple measure of overall confidence is provided by the determinant of the Fisher information matrix. (This is referred to as D-optimality in a common alphabetic classification of related optimality objectives). This determinant is a estimate of the size of a 95% confidence ellipsoid that corresponds to a level set of the cost surface (Figure XYZ). [IMPLEMENT D-optimal design for simple model, e.g. Michaelis-Menton or Hill.] 9.6.4 Optimal distribution of intracellular metabolic fluxes (Flux Balance Analysis) We’ve been focusing on the use of iterative optimization algorithms to solve nonlinear optimization tasks. For some problems, such sophisticated (and computationally intensive) techniques are not need. One important class of such tasks is referred to as linear programming, in which we seek to find the optima of a linear objective function while in as search space that is bounded by linear constraints. An example: \\[\\begin{eqnarray*} \\mbox{maximize} \\ 3x_1 - 4x_2 \\mbox{subject to} \\ 2x_1 + 5x_2 &lt; 3 \\ \\mbox{and} \\ -6x_1 + 4x_2 &lt; -6 \\end{eqnarray*}\\] There are iterative algorithms for solving such problems, that are guaranteed to reach the solution after a finite number of steps (in contrast to the global optimization algorithms discussed earlier). These algorithms, such as the simplex method, step from vertex to vertex within the valid solution space until they arrive at the desired extremum. [Fig XYZ] [Implement solution to toy problem.] Linear programming task arise regularly in planning, scheduling, and other management tasks. One biological context in which they occur is prediction of the distribution of intracellular metabolic fluxes. Genomic analysis allows us to identify the suite of metabolic enzyme that can be expressed within a given cell, and thus identify the set of metabolic processes that may be active in that cell. If we think of these metabolic processes occurring in the context of balanced cell growth, we can safely assume that the net production rate of all metabolic intermediates is zero (i.e.~rate of production is balanced by rate of consumption). If we assign a label to the rate of each reaction (i.e.~the reaction flux) in an intracellular metabolic network, this balance condition imposes a set of linear constraints on the components of a flux vector. For example, consider an idealized cell with a simple metabolism as in figiure XYZ. The balance conditions at intermediate metabolites \\(s_1\\), \\(s_2\\), \\(s_3\\), and \\(s_4\\) gives rise to constraints: \\(v_1=v_3\\), \\(v_2=v_4\\), \\(v_3+v_4=v_5\\), \\(v_5=v_6+v_7\\). Next, consider the case in which the rates of uptake and secretion of (at least some) nutrients and products have been measured. In our case, suppose we have measured \\(v_1=1 uM/hr\\), \\(v_2=3 um/hr\\). …. Not surprisingly, these few measurements of fluxes outside the cell do not provide enough information to determine the values of all fluxes in the intracellular metabolic network. (This is described by saying that the system is underdetermined.) Nevertheless, there are cases in which we want to make such a prediction. To address such cases, the technique of Flux Balance Analysis imposes an assumption about the cell’s internal flux allocation. The assumption is that the cell regulates the internal fluxes to maximize some objective. In the case of microbial cells, this is most commonly presumed to be growth rate. With this objective in place, expressed typically in terms of maximizing biomass: written as a linear combination of particular intracellular reactions, the task of predicting all intracellular fluxes becomes a linear programming task. [implement example] 9.7 References Ashyraliyev, M., Fomekong‐Nanfack, Y., Kaandorp, J. A., &amp; Blom, J. G. (2009). Systems biology: parameter estimation for biochemical models. The FEBS journal, 276(4), 886-902. Lenhart, Suzanne, and John T. Workman. Optimal control applied to biological models. CRC press, 2007. Lin, Qun, Ryan Loxton, and Kok Lay Teo. “The control parameterization method for nonlinear optimal control: a survey.” Journal of Industrial and management optimization 10.1 (2014): 275-309. Beaumont, Mark A. “Approximate bayesian computation.” Annual review of statistics and its application 6 (2019): 379-403. Pukelsheim, Friedrich. Optimal design of experiments. Society for Industrial and Applied Mathematics, 2006. "]]
