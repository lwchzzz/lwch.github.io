[["index.html", "Further Git and GitHub 1 Prerequisites", " Further Git and GitHub Andrew Edwards 05/07/2021 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["intro.html", "2 Introduction", " 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2021) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],["literature.html", "3 Literature", " 3 Literature Here is a review of existing methods. "],["introduction-to-git-and-github.html", "4 Introduction to Git and GitHub 4.1 Motivation 4.2 Getting set up for the first time 4.3 Using Git and GitHub 4.4 Beyond the basics", " 4 Introduction to Git and GitHub 4.1 Motivation As a biology graduate student or a professional biologist in a university or government setting, why might you want to use Git and GitHub? And what are Git and GitHub anyway? Scientists (including students) are working far more collaboratively than in the past This involves both sharing code and writing up results There is a push towards open science – including your code as part of a scientific paper We have called this a TTT approach: Transparent – a clear and open way to show data, code, and results, enabling reproducibility Traceable – a clear link from database queries and code to final results (numbers, tables, and graphs in a document) Transferable – it should be feasible for another person to reproduce work and build upon it with a minimal learning curve Using Git and GitHub in your workflow greatly enables this, both when working alone and in a team. Git keeps track of the latest versions of your files, such as computer code or write up for results as you work on them. It also allows you to go back to any previous version of your files (this is ‘version control’). (It also does much more). GitHub is a website that hosts a ‘repository’ of your work code and enables users to easily collaborate. Your repositories can be either public or private. We use Git and GitHub extensively: to collaborate on writing code and producing documents (such as this entire document!). to easily share code publically for scientific papers, and update it if necessary. when working alone to retain a methodical workflow. Example application – Pacific Hake stock assessment Under a formal Agreement between the Canadian and US governments, a team of four of us (two from each country) conduct an annual stock assessment for Pacific Hake (Merluccius productus) off the west coast of Canada and the US. The assessment is used to manage the stock, which is of important ecological and economic value ($100 million export value in Canada). We fit complex population models to data to make projections about future health of the stock under different levels of catch. There is a very short turnaround (five weeks) between getting the final data, doing the analyses (model runs can take many hours of computer time) and submitting the assessment document, which is typically &gt;200 pages and contains numerous figures and tables available here. Prior to 2016, the document was assembled in Word, requiring lots of editing and amaglamating of files, often late at night. Now we share our code via GitHub, automate a lot of the document production using knitr (similar to Rmarkdown, covered in Module 2). So with four people constantly working on the same large document, we need to ensure we are keeping up-to-date with each other, can all produce the latest version, and have identical folder structures on each other’s computers. The alternative of emailing files back and forth is: very inefficient, prone to errors, just painful. What we can avoid Using GitHub it is easy to see what text/code collaborators have changed, avoiding things like the following, for which it hard to see where to get started: We may want to keep old versions of files (and email them back and forth), but without GitHub we can end up with a veritable gong show: We can avoid having to co-ordinate having only one person working on the latest version of a document, so we don’t get things like: Can avoid multiple versions of a file that then have to be carefully merged: While GoogleDocs, for example, is fine for collaborating on a short document, it isn’t suitable for sharing complex code, or complex documents that are somewhat automatically updated. GitHub advantages Say you’ve off on a two-week hike while your collaborators have been diligently working away and they have edited 15 new files of code in five folders, added four data sets, and created five new pages of text towards a manuscript. You can easily catch up with them (get all their changes onto your computer) with a few simple commands. You don’t even have to pester them to ask what they’ve done, as you can check it yourself. So rather than this conversation: You: “Hey, I’m back from my awesome trip and saw some bears. What have you been doing with the project?” Likely reply: “Glad you had fun. I’m busy on something else right now. Er, where were we at when you left?” You can have this one: You: “Hey, I’m back from my awesome trip and saw some bears. I went through your commits on GitHub and everything looks great. Shall I get on those Methods Issues you assigned me?” Likely reply: “Glad you had fun, looking forward to hearing about it. I’m busy on something else right now so, yes, resolving those issues will be great, thanks.” And the project keeps moving in an efficient way. By having code shared publically, it is easy to answer questions, such as this one I received: Rather than go searching on my laptop for the code that I hadn’t looked at for six months, I could click on the link the questioner sent and answer very quickly, with a simple link to the file I am referring to (there is no ambiguity): You can even ask who last edited a particular line of code/text (GitHub amusingly calls it ‘Blame’): You can properly keep track of (and discuss) ‘Issues’ to be thought about or fixed, rather than having things in emails that get forgotten: Important: You still have all your work locally on your computer. So if your internet access goes down or GitHub is unavailable (which of course will only happen when you have a deadline) you can still carry on with your work. Why this course? Delving into the Git and GitHub world online it can feel like you need a computer science degree to get started. This may not be surprising as Git was writting by the guy who wrote the operating system Linux, to help people collaborate on writing the operating system Linux. But it means that, for example, the second paragraph of the Wikipedia Git page says: “As with most other distributed version control systems, and unlike most client–server systems, every Git directory on every computer is a full-fledged repository with complete history and full version-tracking abilities, independent of network access or a central server.” Say what??? That is fairly incomprehensible to those without strong computer science backgrounds. The aim of this module is to introduce biologists to the world of Git and GitHub, while avoiding a lot of the technical details. However, once you have mastered the basics then it should be easier to delve deeper. Our target audience is: graduate level biology students biology faculty government scientists scientists in non-governmental organisations in fact anyone wanting to learn these tools This work is extended from lectures and exercises developed by Chris Grandin and myself as part of a Fisheries and Oceans Canada workshop. (Luckily Chris does have a computer science degree, and so was able to get some of us going with Git and GitHub several years ago). These tools are now widely used within our organisation. Computer language For sharing code, it doesn’t matter what language your code is in (R, Matlab, Python, C, …), as we will just be sharing text files. There is a learning curve, but once you get going you only really need a few main commands. Unfortunately the hardest bit is actually getting everything set up…. 4.2 Getting set up for the first time Before you start using Git you need to set up your computer to use it, and install a few other programs that are useful. This is a one-time setup and once it is done, you will be able to easily create new projects or join others in collaboration. We have tested the installations as much as feasible. If you have an issue then search the internet, as it may be due to some configuration on your particular computer. This module is for any operating system: Windows, MacOS, Linux or Unix. 4.2.1 What you will end up having installed These are programs/things you will need (instructions are on the next slides). Obviously skip any that you already have working. A GitHub account A text editor that isn’t Notepad Git on your computer Diffmerge or something similar for comparing changes to files (not completely necessary) Markdown Pad 2 or Chrome extension or something similar for viewing Markdown files (not completely necessary) 4.2.2 Get a GitHub account Sign up for GitHub: http://github.com If possible, choose a user name that will make sense to colleagues, e.g. andrew-edwards or cgrandin, not pink-unicorn. Desirable: attach a photo (headshot) to your profile. This makes it easy for collaborators to identify you. 4.2.3 Text Editor You must have a text editor that is aware of outside changes in a file. This is necessary because if you have a file open in the editor and you download an updated version of the file, you want the editor to ask you if you want to use the updated version. We know that Emacs, Xemacs and maybe Vim are okay, as is RStudiofor.R (and other) files. Notepad is not okay. But you can download and install Notepad++ which is fine: https://notepad-plus-plus.org/download/v7.3.3.html 4.2.4 Install the Git application on your machine See https://git-scm.com/downloads for downloading instructions for Windows, MAC and Linux/Unix It seems best to accept the default options, except NOT Notepad or Vim (unless you use Vim) as the default editor. 4.2.5 Git shell For this course we will use a simple git shell to type commands (rather than a point-and-click Graphical User Interface). This is for several reasons: Commands are the same across operating systems. It is easier to demonstrate (and remember) a few simple commands, rather than follow a cursor moving across a screen. Learning the text commands will give you a good understanding of how Git and GitHub work. It is easier to Google for help when you get stuck or want to learn about more advanced options. Commands are quick, and you can usually the up arrow (or ctrl-up-arrow) to retrieve recent commands, or auto-complete commands using . 4.2.6 Git shell, RStudio There are many Graphical User Interfaces that are available, as described at https://git-scm.com/downloads/guis. Many (but not all) biologists use R in RStudio for their analyses. There is Git functionality built in to RStudio that we (TODO: SOMEONE?) will demonstrate later. I use magit which works in the text editor emacs (which for years I have used for pretty much everything, such as editing files, running R, Matlab, etc.). But I would not have been able to learn magit without first knowing the Git commands from using the shell. For now we will stick with the Git shell for the aforementioned reasons. It will also give you a better understanding of Git and GitHub, and emphasise that you can use Git for any files, not just R code. 4.2.7 Powershell and posh-git Download a Powershell (a shell window in which you can type commands, presumably the ‘power’ part means it’s more powerful than a basic version) and then posh-git following the instructions at https://github.com/dahlbyk/posh-git Do the ‘Installation’ and ‘Using posh-git’ sections. If you don’t understand some options (I don’t!) just pick the simplest, usually the first. The next slides are from our course about three years ago (and were for Windows). So they may be out of date (though first one is recent tips from a colleague). [Maybe we should see https://upg-dh.newtfire.org/explainGitShell.html] 4.2.8 One-time authentication The first time you get set up or start using Git, there will be some one-timeauthentication to connect to your GitHub account. Follow any instructions. 4.2.9 Configure the Git application Windows Create a github directory, such as C:. It is fine to put it in a differentpath, but make sure there are no spaces or special charactersanywherein the fullpath. This is where you want to be saving your work that you are tracking with Git. TODO: Andy has to reinstall anyway and will write something here. Think it’s just following instructions. MAC Create the directory ~/github Enjoy a beverage TODO: Check with Luwen if it is that simple 4.2.10 Install the difftool The difftool will be used to examine differences between different versions of files and also to simplify merging of branches and collaborator’s code. There are many programs that can be used but for consistency we will use Diffmerge. It is nice to have but not essential if you have trouble installing it. Install Diffmerge: https://sourcegear.com/diffmerge/downloads.php The configuration for directing git to use Diffmerge will be done below. 4.2.11 “Cloning” the git-course repository On the GitHub webpage, sign into your account and navigate to: https://github.com/quantitative-biology/module-1-git Windows: Open the Git shell and run the following command to clone the repository (‘clone’ means copy all files in the repository to your computer): git clone https://github.com/quantitative-biology/module-1-git MAC: Open terminal and change to the GitHub directory: cd ~/github then run the clone command: git clone https://github.com/quantitative-biology/module-1-git You now have the files for the GitHub course on your computer 4.2.12 Copy the .gitignore file Git uses a configuration file for your account info, name to use when committing, aliases for commands, and other things. Open up the misc sub-directory in the git-module-1 directory and copy the file .gitconfig. For Windows, copy this file (overwrite the existing file) to: C:\\Users\\YOUR-COMPUTER-USER-NAME\\.gitconfig, where YOUR-COMPUTER-USER-NAME is your username on your computer, not your GitHub account name. For MAC, copy this file (overwrite the existing file) to: ~/.gitconfig 4.2.13 Edit the .gitconfig file Use your favourite editor to edit the new file (not the one ingit-course/misc). Change the [user] settings to reflect your information. Change the [difftool] and [diffmerge] directories so they point to the location where you have DiffMerge (if it installed okay).. For Windows the location should be: C:\\Program Files\\SourceGear\\Common\\DiffMerge\\sgdm.exe For MAC the location should be: /usr/local/bin/diffmerge If you could not install [difftool] or [diffmerge] then delete those lines in your .gitconfig file. 4.2.14 MAC only: make your output pretty On the MAC, change the ~/github directory and run the following command: git config –global color.ui.auto This will make your git output colored in a similar way to the Windows powershell version. 4.2.15 Markdown Pad Each project has an associated README.md file that appears on its GitHub homepage.The extension .md stands for Markdown and is just an ASCii text file that contains simple formatting (such as bold or italics). There are two options we have used to readmarkdown files, choose one: The Markdown Pad 2 editor/viewer which is easy to use: http://markdownpad.com. Just get the free version. The Chrome extension for markdown viewing: https://chrome.google.com/webstore/detail/markdown-viewer/ckkdlimhmcjmikdlpkmbgfkaikojcbjk?hl=en. 4.3 Using Git and GitHub This section also has a recorded lecture to demonstrate the main concepts and ideas. The video is available here TODO and the slides from the talk are here TODO, though the notes below mostly replicate the slides. 4.3.1 Definitions Let’s start with some definitions: Repository – essentially a directory containing all your files for a project (plus some files that Git uses). Git – a program that allows you to efficiently save ongoing versions of your files (`version control’). GitHub – a website that hosts your repositories so that you can easily share code and collaborate with colleagues. Basically, the idea is that you work on your files in a repository on your computer, use Git on your computer when you are happy to keep your changes, and use GitHub to easily share the files. Here you will learn the important steps: Creating – create a new repository on GitHub Cloning – copying it to your local computer Committing – the crux of working with Git Collaborating – efficiently work with colleagues Conflicts – fixing conflict changes (happens rarely) 4.3.2 Creating a new repository Sign into your GitHub account, click on the Repositories tab, and press the New button. Give your repository a name. Let’s call it test. Check Initialize this repository with a README. Leave Add .gitignore and Add a license set to None Click Create repository. You now have a new repository on the GitHub website. Next we will clone it onto your computer. 4.3.3 Cloning your new repository Copy the full URL (web address) of your test repository. Open the Git shell and navigate to your C://github directory (or whatever you called it when you created it in the setup instructions – it’s the place you are going to save all your Git repositories). Run the following command to clone your repository: git clone URL where URL is the url of your newly created repository (paste should work). You should now have a subdirectory called github/test on your computer. In Git shell, change to that directory (with cd test). So ‘clone’ is Git speak for copying something from GitHub onto your local computer. This example has just one file (README.md). But the process is the same for a repository with multiple files and multiple directories, and the complate file sturcture is fully preserved. Windows only: Storing your credentials When you are using the Git shell for the very first time on Windows, issue the following command: git config --global credential.helper wincred This means that you don’t have to repeatedly enter you GitHub password (just do it when you are first prompted). 4.3.4 Committing Create a new file, newFile.txt, in the github/test directory. Add a line of text at the start of the file and save it. Check the status of your (test) repository: git status It should say that you have an ‘Untracked file’ called newFile.txt. You want to tell Git to start tracking it, by using: git add. gitignore Type git status again. You should see that the file is listed as a ‘new file’ under ‘Changes to be commited.’ Let’s now ‘commit’ it: git commit -a -m \"Add newFile.txt.\" The commit message (in the quotes) should be a useful message saying what the commit encapsulates (more on that later). Push the commit to GitHub: git push Check (refresh) the GitHub webpage and see your commit and the uploaded file. What just happened? We just used three of the main Git commands: git add &lt;filename&gt; – tell Git to start keeping track of changes to this file. You only need to tell Git this once. git commit -a -m \"Message.\" – committing your changes, which means tell Git you are happy with your edits and want to save them. git push – this sends your commit to the GitHub website. You always have your files stored locally on your computer (as usual), even if you don’t add them or commit changes. When you push to GitHub then your colleagues can easily fetch (retrieve) them. Keyboard aliases (shortcuts) Now, git commit -a -m \"Message.\" is a bit much to type, so we have an alias for it: git com \"Message.\" This is defined in the .gitconfig file you installed in the git-setup instructions into C:\\Users\\YOUR-USER-NAME\\.gitconfig (for Windows). YOu can also add your own commands to that file. The -a means ‘commit all changes of files that Git is tracking,’ and -m is to include a message. Since we usually want to do both of these, git com \"Message.\" is a useful shortcut. But it is important to realise it is an alias if searching online for help. Similarly: git s – for git status git p – for git push git d – for git diff git f – for git fetch From now on we will mostly use the aliases. Use the full commands if the .gitconfig file didn’t work for you. Edit Readme.md Edit the Readme.md file. Add some simple comments describing the project such as: “A test repository for learning Git.” Look over the changes, commit them, and push them to your GitHub repository: git s git d (or git diff) – this gives a simple look at the differences between the last committed version and your current version (of all files; only one in this case) git com “Initial edit of Readme.md” git p (or git push) Refresh your GitHub web page and you should see your text (the Readme.md file is what is shown on the main page of your repo). If you got Diffmerge installed okay, instead of git diff you can do git difftool. This opens up, in turn, each file that changed since your last commit and shows you the differences. This is useful for changes that are more complex than can be easily see in the quick git d. 4.3.5 Exercise 1: create, edit and commit simpleText.txt Create a text file simpleText.txt in your local test repository. Add a line of text at the start and save it. Predict what git s will tell you, then type it in the Git shell to check. Add the file to the repository using the git commands: git add simpleText.txt git s – not necessary but useful to check you understand what is changing before you commit git com \"Adding simpleText.txt\" git p Add some more test to simpleText.txt then git com \"Message.\" and git p. Repeat this a few times to get the hang of it. git com frequently and git p occasionally (you do not have to push every commit), while intermittently doing git s and git d to understand what’s changing. Keep and eye on your commits by refreshing the GitHub page. In reality when writing code/text you won’t be committing quite so frequently, as your focus will be on the writing. Adding multiple files at once Often you add multiple files in a new directory. When you run git s, you will see a large list of Untracked files. They can be all added at once by simply adding the whole directory. 4.3.6 Exercise 2: multiple files Do the following, to get the idea of creating multiple files in a folder and committing that folder. Create a new directory in your test repository, using your normal method. Call it new-stuff. Add a few new test files to that directory called test1.txt, test2.txt, etc. Put some example text in one or more of them if you want. On the command line, check the status: git s You will see a listing showing the new-stuff/ directory in Untracked files. To add all the new files in preparation for a commit, issue the command: git add new-stuff/ Check the status of the repository again with git s It will now show all files in Changes to be committed Commit the changes: git com \"Added new-stuff directory.\" Push the changes to GitHub: git p Check your GitHub webpage and see your commit and that the files have been uploaded. That works no matter how many files are in your new-stuff directory. There could be a hundred and it’s the same command. Wildcard symbol * This is useful to know (no need to do it as part of the exercise): To add multiple files with similar names you can use the wildcard * symbol. You just added (told Git to keep track of) the new files in your new-stuff/ directory. If you add more new files to that directory, you will have to tell Git to track those also. This is because they are new – you haven’t told Git about them yet. Say you have 10 new files called idea1.txt, idea2.txt, …, idea 10.txt. Instead of typing git add new-stuff/idea1.txt git add new-stuff/idea2.txt etc. you can just use the wildcard *: git add new-stuff/idea*.txt or even just git add new-stuff/*.txt or git add new-stuff/*.*. The .gitignore file But what if you don’t want to add all the files that you create? Each repository can have a .gitignore file, in the root directory of the repository. Such a file has names of files (such as my-secret-notes.txt) or wildcard names (such as _*.pdf_ or _*.doc_ ) that will be completely ignored by Git. For an example, see https://github.com/pacific-hake/hake-assessment/blob/master/.gitignore, noting that the # can be used for comments. When sharing a repository with others, you want to share your code (for example, R, Python or Matlab code) and maybe data, but generally not share the output (such as figures that the code generates; more on this later). For reproducible research your colleague (or anyone) should be able to run your code to generate the results. Some programs you run may make temporary files that don’t need to be tracked by Git, the names of which should also be included in your .gitignore. When sharing code or collaborating you want to keep your repository as clean as possible and not clutter it up with files that other people don’t need. So when you run git s and see untracked files that you don’t want to be tracked, add them (or a suitable wildcard expression) to your .gitignore file so that they are not added inadvertently. This will also simplify your workflow (you don’t need to keep being reminded that you have untracked files). If you are on MacOS and you find that folders have a .DS_Store file in them, then create (and add and commit) a .gitignore file with .DS_Store as a line. Git Workflow You have now learnt the basics of using Git. By creating a public repository on GitHub you can now release your code to the world! You can also choose the private repository option when creating a repository, so that you can control who can see it. Go into Settings--Manage Access to add collaborators. 4.3.7 Collaborating Now we will show how to collaborate with colleagues, which is where the usefulness of Git will become more apparent. There are a few different ways to collaborate using Git and GitHub. We will focus on the following one since it is the simplest, and is what you need to collaborate with colleagues. Concept: there is a project where people contribute to a main repository that is considered the ‘master copy.’ Everyone clones directly from the creator’s repository. All collaborators push their commits to the main repository (the creator has to add them as collaborators once on GitHub). Since the creator has to grant permission, you won’t have just anyone contributing to (and maybe messing up your work), just your trusted collaborators. But you have to trust your team to not mess things up (more on that later!). Okay, so in the video we demonstrated the following: Kim creates new repo called collaborate (and clones it to her computer). Andy clones it also. On GitHub, Kim gives Andy ‘push access’ to her collaborate repo. Both do some edits (create some new simple text files). For Andy to get Kim’s updates (and vice versa), it was just: git fetch (or just git f) – fetches the latest version of the repository from GitHub onto your computer. Your local files have not yet changed (check them), but Git has the changes stored on your computer (?!?). git rebase – updates your local repository (the committed files on your computer) with the changes you have just fetched, merging both people’s work together. git p – pushes the merged changes back up to GitHub so that the other person can get them. That is the basic workflow. We also showed an example of git p not being allowed for Person A because there are recent commits on GitHub (by Person B) that Person A has not yet merged into their local version of the repository. Here is an example of the error message you get: While a bit lengthy, the error message is useful. It forces you to get the other person’s work before you push yours. You do this by: git f git rebase. So to be allowed to push, just fetch to get the new commits onto your computer, and then rebase to combine the commits into your local version. Then you can git push. Here is a full screenshot (‘g’ is just a shortcut for ‘git’). The green up arrow number 8 tells me I have 8 commits to push to GitHub. The yellow arrows I think of as just implying I need to do a rebase (before doing that I might browse through the other person’s commits on GitHub): After the rebase I was allowed to push and then everything is up to date. A bit more about git rebase Andy commits local changes, tries to git push but is told to first git fetch (to get Kim’s changes from GitHub). Andy does git fetch and then git rebase. What git rebase does is basically rewind to the last common commit that both people had, and then add one person’s commits and the others. Andy then does git push to push his commits to GitHub (from where Kim will fetch them when she’s ready). Providing there are no conflicts, this will work fine. Another option is to do a git merge, which basically creates a new commit that merges both people’s work together. Our groups used to use git merge and now use git rebase; some people don’t like git merge because it adds extra commits. For a more in-depth understanding see https://reflectoring.io/git-rebase-merge/ for one of the clearer explanations out there concerning rebase v merge. Note that the error in the above screenshot (when I could not git push) told me that I might want to do git pull. This is basically git fetch git merge in one command, but it seems preferable to do git fetch git rebase. Fixing a conflict A conflict happens when two people have edited the same line(s) of the same file. Conflicts happen relatively rarely and can be generally avoided by co-ordinating with collaborators so that you are working on different files. But, they will happen and you need to know how to resolve them. Git forces you to explicitly decide whose changes to keep – this is a good thing, since you want a human to make such a decision. In the video we demonstrated a conflict. Fixing a conflict The best approach I have found to fixing a conflict is the following: Trying git rebase will tell you there is a conflict. git rebase --abort – do this to abort the rebase attempt. git merge – this will tell you there is a conflict. Open the file(s) with the conflict and edit the text (see below). git add &lt;filename(s)&gt; – you have to then add the files that had the conflict (I am not sure why this is necessary, I just do it). git com \"&lt;message&gt;\" – in your commit message you can explain how you fixed the conflict. This is useful so that your collaborators know you have resolved a conflict (they can look at the commit to see if they are happy with it). The merge message will tell you which files are conflicting. Open those files one by one, and you will see the conflicted section bracketed like the following: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Line(s) of text/code which are currently in your file. ======= Line(s) of text/code which are trying to merge in, but conflict. &gt;&gt;&gt;&gt;&gt;&gt; origin/main where origin/main refers to the version you have fetched from GitHub. All you do is remove the line(s) of text that you do not want to keep (or edit the line(s) to be something else entirely), and remove the bracketing lines &lt;&lt;&lt;... and &gt;&gt;&gt;..., and the ====== line. Save each conflicted file and then (as mentioned previously): git add &lt;filename(s)&gt; git com &quot;Kept Kim&#39;s edits as more consistent with remaining text.&quot; git p 4.3.8 Exercise 3: collaborating on a single repository If you have a colleague available, try what we just did: Person 1 creates a new repository on GitHub and clone to their computer. Give the Person 2 ‘push access’ to the repository (on the repo page on GitHub: Settings – Manage access – Invite a collaborator) Person 2 clones to their computer Both create a simple text file (use different filenames), add some text and, as usual, add, commit, and push. git fetch and git rebase to get the other person’s file. Continue editing either file, committing, and pushing. If you get the push error (shown earlier), refresh GitHub repository site to see recent commits (click on the XX commits link). You can easily spot the other person’s recent commits. Click on one (the bold message) to see details. Purposefully create a conflict (both edit the same line of the same file). Resolve it as described earlier. In practice you won’t commit so frequently when working, but this is good to get the hang of it. Congratulations Congratulations, you now know the few basic commands and functionality needed to collaborate with Git and GitHub. It takes a bit of practice, but it is very powerful. 95% of the time, this is all you are doing: Change some code. git s git d git com &quot;My commit message&quot;` git p (the git s and git d are useful to check you have changed only what you think you have changed). If GitHub does not allow you to push: git fetch git rebase If conflicts, then git rebase --abort git merge fix the conflicts manually and then git add &lt;conflicted file(s)&gt; git com &quot;Message to explain what you did&quot; git p Change some code and repeat! The next section gives slightly more advanced background that should further improve your understanding (including why Git is useful even when not collaborating or sharing your code), plus tips for improving your workflow. 4.4 Beyond the basics Here are some Git and GitHub concepts and tips that go beyond the basics that we just covered. 4.4.1 Workflow tips Realise that you still edit and save your files in the usual way on your computer. If you don’t do Git commits you will still have the latest versions of your files on your computer (as you would if you weren’t using Git). So if you do get stuck with Git you can carry on working as normal (though you probably do want to try and fix it at some point). When collaborating: If working closely with others, when you start each day (or after a break) then make sure you are up to date and have all their commits. Refresh the GitHub page for you repository, and git fetch (or just git f)and git clone if needed. (To be safe you can git f and git s to check). We find it helpful to co-ordinate our work (Slack is useful for this, or use GitHub Issues for complex discussion – see below), so that if multiple people are working at the same time, you are at least not working on exactly the same parts, just to reduce conflicts. Commit fairly frequently and write helpful commit messages (so your colleagues get an idea of what you’ve done in each commit). Push less frequently, and don’t push code that doesn’t work – that will annoy your colleagues. And then they (and probably you) may both spend time fixing it. To see who last edited a particular piece of code, when viewing the file on GitHub click Blame (you can even click on the square icon to go to the previous commit that changed that line): GitHub Issues GitHub Issues are very useful for discussing issues with your repo. For our annual Pacific Hake assessment we have used them extensively over the years: The Issues tab lists our current ‘Open’ issues – we have 20, of which five (the most recently posted) are shown here. We are currently in-between assessments (and not working on it), so we have created Issues that we want to think about or deal with for next year. This avoids forgetting about ideas or losing them in old emails. Issues are intuitive to use. There is a bright green ‘New Issues’ button to create new ones, you give a title and then write some details, people can reply, you can assign people to look at them, and you can close them. In the above screenshot you can see that we have closed 815 issues (this was over several years). Useful tip: when doing a commit that refers to an Issue, if you refer to the Issue number (with #&lt;number&gt;) in your commit message, then after pushing that commit the Issue on GitHub will automatically mention and link to the commit: git com \"Add more options to fancy_function(), #21.\" will mention the commit when you look at the issue. You can even automatically close the issue by saying closes #21 in your commit message: git com \"Add more options to fancy_function(), closes #21.\" Issues are particularly useful to avoid cluttering up code with commented notes or ideas that you may easily not come back to, or avoiding endless emails that end up getting overlooked. You don’t have to fix an Issue to close it, you can decide not to pursue, but at least you have made a decision. (We also use Slack a lot to communicate, but moreso for quick questions or bouncing ideas around – Issues are better for stuff that you want to come back to at some point). You may receive emails regarding Issues, but if you use GitHub a lot you will see Notifications (the blue dot on the bell in the top-right corner when signed in on GitHub) and that will show you new Issues of repositories you are involved with, or if anyone has updated an Issue. GitHub organizations If you will frequently collaborate with colleagues, you can create an Organization on GitHub and invite collaborators to it (click on your GitHub photo in the top-right corner, Settings, Organizations). Then they will automatically have access to all repositories created under the Organization. You can choose the security settings. 4.4.2 So I’ve made some changes but don’t really want to keep them – git stash If you’ve changed some code but have not committed it, and then maybe got in a mess and just want to go back to your last commit, you can stash your changes git stash and to include a message (for your future self): git stash save &quot;Message&quot; This stashes them away such that they can be retrieved later This is handy. You may think you don’t want to keep those changes, but sometimes you may later wish you had kept them somehwere. Note this only for files that Git is tracking (i.e. files that have been added at some point). You can have multiple stashes, seen by doing: git stash list To retrieve the last stash: git stash pop TODO: check rules for other stashed things. TODO: Often you’re working on something but it’s not finished, but you want to push it. Stash, then make branch then copy files in. Tricky to do all with git, so do manually. TODO: see my README for details, and test what to do 4.4.3 Pull requests TODO 4.4.4 The power to go back With Git you can revert back to any previous state of your repository. This is very powerful, though slightly scary at first. Do this with your test repository, that should have some files in it from the earlier excercise: git s to make sure you are all up-to-date (commit and/or push if necessary). In File Explorer (or whatever you use) look at your repository, you should see all your files, including the new-stuff\\ directory. Look at the commit tab on GitHub for your test repo and click on the clipboard icon to copy the HASH number thingy to the clipboard . In Git shell: git checkout HASH (where HASH is the pasted HASH, or git co HASH using our Alias) Look at File Explorer again – your new-stuff directory should have … disappeared!! (If it hasn’t disappeared then open it – the test files, i.e. test1.r, test2.r, etc. should be gone, but your text editor may have saved backup versions; manually delete them plus the new-stuff/ directory.) You are now back to the very first version of your repo! Powerful and scary. Now, to get your files back to the most recent version you had committed: git checkout main (it used to be git checkout master, the names have recently changed). That’s it! Check that your files are back. All this means that you can revert to any previous commit in your repository. This is very reassuring. For example you have some complex code that you realise is now a complete mess and you want to go back to yesterday’s version of everything. In practice you rarely actually do this, but it’s very comforting to know that you can. Consequently, your workflow is less cluttered and more tractable than having to save multiple versions of the same files with dates in the filename, such as this nightmare: Retrieving older work in practice I think there are fancy ways that Git can replace a current file with a version from an earlier commit. But, in practice (especially since you rarely want to do this) it is a bit safer to do the following: Say you are up-to-date (git s says all is good), but your program my_code.R just isn’t working and you want to go back to the version you had yesterday at commit number abc123. git co abc123 (or git checkout abc123) to checkout the earlier commit, which includes the old version of my_code.R that you want get. Copy my_code.R to a new file my_code_old.R. In the shell you can just do this with cp my_code.R my_code_old.R. Do NOT edit my_code.R or make any changes, as you may end up with a scary DETACHED HEAD warning. git co main to checkout the latest version again. Since you have NOT done git add my_code_old.R, Git is not tracking my_code_old.R and so it is just sitting in your folder as normal. Now you can manually copy what you want from my_code_old.R into my_code.R to fix your problem. It could be the full file, or just some part of it. Then commit as normal. At some point you can delete my_code_old.R so it is not hanging around, but you don’t have to. (Though maybe make a note in it as to which commit it was from, in case you do need it again). 4.4.5 So how does Git do all this? By now you’re probably wondering how Git keeps track of everything. Git does not keep versions of code, it keeps commits. The commits are kept track of using a HASH key which is a generated 40-digit key in hexadecimal (base 16). The hashes are what you see on GitHub and in various places when you use Git shell. By stitching all the commits back together again, Git can recreate all your code. There is a hidden .git/ directory in each repository. Look at the .git/objects/ subdirectory. Each subdirectory name is the first two digits of a HASH. The rest of the digits of the HASH are the filenames in the subdirectory. You can basically think of the hashes as representing commits (apparently they can also be blobs and trees, whatever they might be). I think of the files in the subdirectories containing the differences between each commit. Because of these structures, Git can go back and rebuild any or all files at any commit, and even have different directory structures at each commit. Since Git is keeping track of differences between files, this all works best for plain ASCii (text) files, such as .R, .txt, .Rmd, etc. Git does work for binary files, such as .xls, .docx, .RData, but since changes to the files are not easily saved (Git essentially has to resave the whole file at each commit), this is not very efficient and may make your repository large. Such files will be fully resaved every time they are changed. Think of a binary file as something that you cannot open in a text editor and read (it does not contain simple ASCii letters and numbers). Exceptions: often you may have an image or photo or other type file that you need to share for a document, but it isn’t going to keep changing. So that’s fine to commit. An example of why you should not commit binary files: A collaborator was running some R code (and correctly committed the .R files so that I could run it), but also committed the results, which included .pdf, .png and .RData files, which can get quite large. But, these latter files got updated every time the code was run. So changing one line of the .R code (which Git deals with very efficiently), and running that code and committing, resulted in the new .pdf etc. files being fully saved (since Git cannot just save the difference from the last commit because they are binary files). Even if, say, one point changes on a figure in a graph in a .pdf file, Git has to save the whole new version. This ended up with .git/objects/pack (whatever that might be!) being 2.8Gb. I needed space quickly on my computer so just deleted four files in .git/objects/pack, which freed up 1.6Gb. Note that I still had the actual final versions of files (as you would if not using Git), but just not the full repository history. However, when I tried to later do some work and then commit I got lots of ‘fatal’ errors with scary messages like bad object HEAD and the awesomely titled You are on a branch yet to be born: I just had to start again from scratch (reclone I think). Take-home message: Don’t mess with the .git directory!! 4.4.6 Git terminology At some point you will likely need to search online for some help (often questions are posted and answered on the excellent ‘stackoverflow’ website). A bit more understanding of terminology will help you. Remember that Git keeps commits. Several of these commits have pointers to them that have special names: HEAD points to the commit you are currently on in the Git shell. main or master is the default branch when you set up a repository on GitHub (there are two names because of recent changes on GitHub). 4.4.7 Branching So far we have only worked on the main branch. Sometimes you want to create a new branch that branches off from the main branch. It’s bit like a tree branching, except that at some point you want your new branch to be merged back into main. For example, you may want to try adding some new code to your project, but don’t want to break what is already there. You may do this even if working alone, but it’s especially useful if you are collaborating, or if, say, you have an R packages hosted on GitHub that anyone may be downloading – you don’t want to annoy them by pushing experimental code that doesn’t work. So you would create a new branch, work on that new branch (i.e. commit changes to the new branch), and when you are happy with your new changes you can easily merge it all back into main. Working on a new branch When creating a new branch, your starting point is identical to the branch you were when you created the new one. In the Git shell navigate into your test repository: cd test Depending on your set up, you should see main indicated somewhere (if not do git s and it should say On branch main. Make sure you are up-to-date and have committed all changes (git s, and commit if necessary). Create a new branch called temp, this will be based off the latest commit of the main branch you are currently on: git checkout -b temp (We have an alias for that: git cb temp`). You will be automatically placed in the new branch called temp, and commits you make will now occur in that branch only. Make and commit some changes (e.g. add a new file) – these will now be on your temp branch. You can push to GitHub. The first time you try git p, the Git shell will tell you that you need to type the following so that future pushes go to the new branch: git --set-upstream origin BRANCH-NAME Check the GitHub webpage to see that your branch was pushed. You repository page (that will still be looking at your main branch) may tell you that there is a temp branch with more recent commits than main. If not then if you click on the main drop-down menu: it should give you the option to look at your new temp branch. (The ‘1 branch’ in the above image should also say ‘2 branches’). You can now view your new file in your new temp branch on GitHub. A graphical way to see and understand branching is to click on Insights–Network to see the Network graph. The Network Graph is a useful visualization tool, where each commit is shown as a point on the graph (the numbers along the top are the dates). You can hover your mouse over a commit to see who committed it and the commit message. You can click to see full details of the commit. The Network Graph is particularly useful if you or others are working on multiple branches, or to check details about merges. Okay, back in your Git shell you can easily switch back to your original main branch: git checkout main (or the alias git co main). You will see that the file you just added is gone, because it only exists in the temp branch at this moment. Imagine that in your temp branch you did several commits to create a new function in your code, or have added some new text to a report. Now you are happy with what you’ve done you want to merge it back into the main branch. To view all local branches: git branch There is an asterisk next to the branch you are currently in. To switch to another branch (main in our case): git checkout main To combine the changes from the temp branch: git rebase temp or git merge temp Now the file you created in the temp branch now appears in the main branch. All commits done in the temp branch will now be in the main branch as well. If there was a merge conflict, you must fix it at this point (see earleir). Once you’ve merged your temp branch into main, you don’t really need temp any more and so it is good protocol to delete to keep things tidy: git branch -d temp If you have unmerged changes in a branch, you will not be allowed to delete it, but Git shell will tell you the command to forcibly delete it: git branch -D temp Warning – you won’t be able to get any of those changes back once you do this. To remove a branch entirely from GitHub: git p origin --delete BRANCH-NAME 4.4.8 Undoing stuff If you make a commit followed by other commits, then realize you want to undo the earlier commit, you use revert: git revert HASH where HASH is the hash for the commit you want to undo. Remember that Git shell is smart enough that you only need the first five digits: git revert 1ef1d This actually creates a new commit with the automatic message Revert \"&lt;previous commit message&gt;\". Obviously, you have to be careful with this if you’re changing something that was a few commits back, as you might mess up your code. Undoing changes not yet committed If you’ve made a mess in your working directory and you want to change everything back to the way it was on the last commit: git reset --hard HEAD If you’ve messed up a single file and just want that one file to go back to the way it was on the last commit: git checkout HEAD &lt;filename_to_restore&gt; Warning – running these commands will delete the changes you have made. Since you have not committed any changes, they will be lost. Make sure you are certain you don’t need the changes before running these commands. If you aren’t sure if you need the changes again in the future, use git stash instead. Changing the commit message in the last commit If you make a commit then realize you want to change it (add more information, fix something that will confuse your colleagues, fix something that will confuse you tomorrow), you can change the commit message: git commit --amend -m \"Correct message.\" This only works on the last commit. If you already pushed the commit before realizing that the message needs modification, do this: git p --force after making the amendment to the commit message. "],["introduction-to-r-markdown.html", "5 Introduction to R Markdown 5.1 Motivation 5.2 Basic idea 5.3 Simple example 5.4 Output format 5.5 Further reading", " 5 Introduction to R Markdown 5.1 Motivation Say you get some tree data from a colleague, and spend a month writing lots of R code to analyse the data. Your code also produces beautiful figures and tables, that you then manually copy into a Word document. You have also written lots of text, and include specific calculated numbers in the text, such as the simple The average tree height was 10.1 m. Then your colleague sheepishly tells you that someone found an error in some of the data, and so you need to redo everything. Your heart sinks with the prospect of re-running all your code and making sure you manually copy the correct new figures into your document. Oh, and you need to redo the tables and check all the numbers in your text. The alternative modern approach is to use R Markdown. The idea is that you can generate a ‘dynamic report.’ You write code that contains a mixture of your R code and your write up. You can use this for short analyses, scientific manuscripts, or even a complete thesis. This introduction, aimed at biologists, will get you started with the basics. You will then be in a good position to learn more details from the RStudio introduction and the online Definitive Guide to R Markdown. A key concept is that everything is written as code. You do not have to manually point and click anything, or copy and paste figures between directories. So once you understand how something works or have figured out some formatting that you like, you can just copy that code and use it elsewhere. Example application A recent 328-page document we wrote in R Markdown is A reproducible data synopsis for over 100 species of British Columbia groundfish. For each of 113 species, we produced two pages of figures: For each species, the layout of the figures is identical (even to show no data when none are available). Producing each figure and manually inserting them into a Word document would be extremely tedious. Instead, the production of the document is automated using R Markdown. Furthermore, the work is transparent and traceable. Because the code produces the figures (they are not pasted in from somewhere), we can trace back from the R Markdown code to see the R code that: pulled the data from databases fit models generated plots. In particular, we intend to periodically update the document as new data become available. While still a lot of work, it is less daunting knowing that the code is already available. On a practical level, the report has allowed anyone to see the data available, and has consequently increased data transparency between Fisheries and Oceans Canada, the fishing industry, non-governmental organizations, and the public. This is admittedly a very advanced example with a ton of code (several new R packages) and work behind it, but the idea is to show you what is possible. 5.2 Basic idea In the above tree example, if using Word, for example, you would have a sentence that says The average tree height was 10.1 m The “10.1” is hard-wired into your text, and you typed it on from the value “10.1” that your R code calculated (in a variable you calculated, let’s say you called it avge_height). In your R Markdown document you have your R code and your text write up. You would equivalently have: The average tree height was `r avge_height` m Instead of “10.1” you refer directly to the variable avge_height that you have already calculated. When you ‘render’ your R Markdown document (convert it from code into .pdf, .html or other formats), it automatically fills in the avge_height value as “10.1.” The `r means that the next bit of code (until the next backtick) should be evaluated using R, and the result inserted. This is the basic idea. Then, when your colleague mentions the error (or, say, provides you with extra data) you can just re-run your code and the “10.1” will be automatically updated in your document. This concept extends to your tables and figures – they can all be automatically updated. 5.3 Simple example Here we will generate some data, show some of it in a table, plot it, and show the results of fitting a simple linear regression. Read through this and then you will download and run the code in the exercise. Generate data First we’ll need some libraries: library(kableExtra) library(dplyr) library(xtable) Now generate some data: set.seed(42) n &lt;- 50 # sample size x &lt;- 1:n y &lt;- 10*x + rnorm(n, 0, 10) n [1] 50 So we are showing our R code here (we can choose to hide it if we like), and it has been executed, yielding the printed output of the value of n (because of the final line of the code). We can also embed results from R within sentences, for example: We have a sample size of 50. This is done (as mentioned above) by the code: We have a sample size of `r n` Note that you need the space straight after the r. We can also automatically say that the maximum value of the data is 506.5564788, or round it to a whole number: the maximum value of the data is 507. These were done by: the maximum value of the data is `r max(y)` the maximum value of the data is `r round(max(y))` Show some of the data Let’s combine the data in a tibble (think of it as a data frame if you don’t know what that is): data &lt;- tibble(x, y) data # A tibble: 50 x 2 x y &lt;int&gt; &lt;dbl&gt; 1 1 23.7 2 2 14.4 3 3 33.6 4 4 46.3 5 5 54.0 6 6 58.9 7 7 85.1 8 8 79.1 9 9 110. 10 10 99.4 # … with 40 more rows (only the first 10 rows get printed here thanks to dplyr). To have a proper table, we can do kable(data[1:10,], caption = &quot;The first rows of my data.&quot;) Table 5.1: The first rows of my data. x y 1 23.70958 2 14.35302 3 33.63128 4 46.32863 5 54.04268 6 58.93875 7 85.11522 8 79.05341 9 110.18424 10 99.37286 (If you’re running the code separately the exact style may look different because of settings we have, but pretty much everything is tweakable with the kable and kableExtra packages). Plot then fit a regression data Now to plot the data: plot(x, y) To fit and then print the summary regression output from R: fit &lt;- lm(y ~ x) print(summary(fit)) Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -27.403 -4.366 -1.193 8.319 21.072 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.7071 3.2719 1.133 0.263 x 9.8406 0.1117 88.124 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 11.39 on 48 degrees of freedom Multiple R-squared: 0.9939, Adjusted R-squared: 0.9937 F-statistic: 7766 on 1 and 48 DF, p-value: &lt; 2.2e-16 And for a report we can produce a simple table (including a caption) of output and the regression fit: kable(coefficients(summary(fit)), caption = &quot;Linear regression fit.&quot;) Table 5.2: Linear regression fit. Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.707115 3.2719100 1.133012 0.2628368 x 9.840634 0.1116686 88.123530 0.0000000 And create a plot: plot(x, y) abline(fit, col=&quot;red&quot;) Now, let’s go back and change the data The big feature of dynamically generating reports is when you go back and change or update the input data. For example, changing the data in the above example and then re-running it to redo the report. The best way to demonstrate this is for you to do it in the following Exercise. 5.3.1 Exercise In R do library(rmarkdown) to make sure you have the rmarkdown package. If not then install it. Download this file onto your computer and put it where you want to work on this exercise. The file is an R Markdown (.Rmd) file that you can run by either clicking the knitr button in RStudio or doing rmarkdown::render(\"rmark-exercise.Rmd\") in R. Check that this has produced (‘rendered’) an .html file document that looks similar to what you see above (don’t worry if the styling is not identical, but the important content should be). Note that the .Rmd file is not repeating all the explanations that we gave above. Carefully read through the .Rmd file and compare it with what you see in the resulting .html. There are some comments in there, denoted by &lt;!-- comment --&gt; to help you, but you should be able to get the idea of how commands in the .Rmd file translate into output in the .html file. Copy the resulting rmark-exercise.html to rmark-exercise-orig.html, change n to 30 in rmark-exercise.Rmd, and re-render it. Compare the two .html files. You have done the same analyses but on different data. You changed one value, n, and consequently all the resulting calculations changed, and these are all updated in your new .html document! You have performed exactly the same analysis on your ‘new’ data, including creating tables and figures. No copy-and-pasteing of tables, or manually keeping track of which figure corresponds to which analysis. That last bit is the crux of R Markdown. Once you understand that then you should incorporate it into your workflow. You can easily run identical analyses on two data sets, or do 10 runs of a model with different parameters, and easily compare the output. You can even get a bit clever with your writing by including an R ifelse statement to somewhat automate the text. Here is the results of some code (that will be included in the file): So the maximum value of \\(y\\) is 507, which is greater than the special value of 400. The “greater than” or “less than” part is given by `r ifelse(max(y)&gt;400, paste(&quot;greater than&quot;), paste(&quot;less than&quot;))` But you have to be careful and think about all possibilities – what if \\(y=399.9\\)? 5.4 Output format To keep it simple, in the Exercise we have set the output to be .html. For scientific documents, .pdf are preferable. For this you will need LaTeX installed. For decades, LaTeX has been the standard typesetting system for writing mathematical papers. In 2002, Friedrich Leisch created ‘Sweave,’ to weave together R code for calculations and LaTeX code for writing up results into a .pdf file. (It was called Sweave because S was the precursor language to R, plus I expect Sweave sounded better than Rweave). Sweave motivated knitr (to knit together R code with text write ups) by Yihui Xie, which also allows html and other output formats. R Markdown was then created to allow simpler use of knitr, without learning lots of LaTeX commands. To create .pdf output, and use LaTeX for writing equations and customising your output, if you don’t have LaTeX installed it then see the simple instructions another book by Yhiui Xie et al., the R Markdown cookbook. 5.5 Further reading As we said, the idea of this module was to give you a simple introduction to using R Markdown. The Definitive Guide to R Markdown is highly recommended (and often where you end up when Googling for how to do something), but you do not need to understand all of it (I know that Pandoc is doing stuff behind the scenes, but have managed to not need much more than that). Chapter 2 goes through the basics, including many things that we glossed over above, such as the code between the --- at the top of rmark-exercise.Rmd. For me to explain it properly, I would have to refer to the guidebook anyway. In practice, once you have something working in the style you like, you can just copy those settings for each new project. It can be handy to keep a readme file of containing commands/options/tips that you use a lot. "],["introduction-to-multivariate-analysis.html", "6 Introduction to multivariate analysis 6.1 Multivariate resemblance 6.2 Cluster Analysis 6.3 Ordination", " 6 Introduction to multivariate analysis In this module we’ll be disccusing multivariate quantitative methods. Analyses such as linear regression, where we relate a response, y, to a predictor variable, x, are univariate techniques. If we have multiple responses, \\(y_1...y_n\\), and multiple predictors, \\(x_1...x_n\\), we need multivariate approaches. For example, we may wish to understand how both precipitation and soil type are related to plant community composition. In this question, we may be tracking the abundance of over a dozen different species and many different sites with different types of soil, precipitation and other environmental factors. You can easily see that this is not a situation that ordinary univariate approaches are designed to handle! There are many types of multivariate analysis, and in this module and the next, we will only describe some of the most common ones. We can think of these different types of analysis as laying at different ends of a spectrum of treating the data as discrete vs continuous, and relying on identfying a reponse variable a priori versus letting the “data tell us” about explanatory features, i.e., latent variables (Fig. 6.1. Figure 6.1: Types of multivariate analysis 6.1 Multivariate resemblance The starting point for a lot of the classic multivariate methods is to find metrics that describe how similar two individuals, samples, sites or species might be. A natural way to quantify similarity is to list those characters that are shared. For example, what genetic or morphological features are the same or different between two species? A resemblance measure quantifies similarity by adding up in some way the similarities and differences between two things. We can express the shared characters of objects as either: similarity (S), which quantifies the degree of resemblance or dissimilarity (D) which quantifies the degree of difference. 6.1.1 Binary Similarity metrics The simplest similarity metric just tallys the number of shared features. This is called a binary similarity metric, since we are just indicating a yes or no for each characteristic of the two things we wish to compare (Table 6.1). Table 6.1: List of shared attributes for two things Attribute Object 1 Object 2 Similarity Attribute 1 1 0 x Attribute 2 0 1 x Attribute 3 0 0 ✓ Attribute 4 1 1 ✓ Attribute 5 1 1 ✓ Attribute 6 0 0 ✓ Attribute 7 0 1 x Attribute 8 0 0 ✓ Attribute 9 1 1 ✓ Attribute 10 1 0 x We could also use a shared lack of features as an indicator of similarity. The simple matching coefficient uses both shared features, and shared absent features to quantify similarity as \\(S_m=\\frac{a+d}{a+b+c+d}\\), where a refers to the number of characteristics that object 1 possesses and b is the number that object 2 possesses and so on (see Table 6.2). Table 6.2: Summary of shared and absent atributes Object 1 Object 2 Present Absent Object 1 Present a b Object 2 Absent c d We can further categorize similarity metrics as symmetric, where we regard both shared presence and shared absence as evidence of similarity, the simple matching coefficient, \\(S_m\\) would be an example of this, or asymmetric, where we regard only shared presence as evidence of similarity (that is, we ignore shared absences).Asymmetric measures are most useful in analyzing ecological community data, since it is unlikely to be informative that two temperature zone communities lack tropical data, or that aquatic environments lack terrestrial species. The Jaccard index is an asymmetric binary similarity coefficient calculated as \\(S_J=\\frac{a}{a+b+c}\\), while the quite similar Sørenson index is given as \\(S_S=\\frac{2a}{2a+b+c}\\), and so gives greater weight to shared similarities. Both metrics range from 0 to 1, where a value of 1 indicates complete similarity. Let’s try an example. In the 70s, Watson (1974) compared the zooplankton species present in Lake Erie and Lake Ontario. We can use this information to compare how similar the communities in the two lakes were at this time. We can see that they shared a lot of species (Table 6.3)! Table 6.3: Species presence and absence in lake Erie and lake Ontario (data from from Watson 1974) species erie ontario 1 1 1 2 1 1 3 1 1 4 1 1 5 1 1 6 1 1 7 1 1 8 1 1 9 1 1 10 1 1 11 1 1 12 1 1 13 1 1 14 1 1 15 1 1 16 1 1 17 1 1 18 1 1 19 1 0 20 0 1 21 0 0 22 0 0 23 0 0 24 0 0 We can calculate the similarity metrics quite easily using the table() function, where 1 indicates presence and 0 indicates absence. I have stored the information from Table 6.3 in the the dataframe lksp. I’m just going grab the presences and absences, since I don’t need the species names for my calculation. tlake=table(lksp[,c(&quot;erie&quot;,&quot;ontario&quot;)]) tlake ontario erie 1 0 1 18 1 0 1 4 a=tlake[1,1] b=tlake[1,2] c=tlake[2,1] d=tlake[2,2] S_j=a/(a+b+c) S_j [1] 0.9 S_s=2*a/(2*a+b+c) S_s [1] 0.9473684 When a disimilarity or similarity metric has a finite range, we can simply convert from one to the other. For example, for similarities that range from 1 (identical) to 0 (completely different), dissimilarity would simply be 1-similarity. 6.1.2 Quantitative similarity &amp; dissimilarity metrics While binary similarity metrics are easy to understand, there are a few problems. These metrics work best when we have a small number of characteristics and we have sampled very well (e.g., the zooplankton in Lake Erie and Ontario). However, these metrics are biased against maximum similarity values when we have lots of charactersitics (or species) and poor sampling. In addition, we sometimes have more information than just a “yes” or “no” which we could use to further characterize similarity. Quantiative similarity and dissimilarity metrics make use of this information. Some examples of quantitative similarity metrics are: Percentage similarity (Renkonen index), Morisita’s index of similarity (not dispersion) and Horn’s index. Quantitive dissimilarity metrics are perhaps more commonly used. In this case, we often talk about the “distance” between two things. Distances are of two types, either dissimilarity, converted from analogous similarity indices, or specific distance measures, such as Euclidean distance, which doesn’t have a counterpart in any similarity index. There are many, many such metrics, and obviously, you should choose the most accurate and meaningful distance measure for a given application. Legendre &amp; Legendre (2012) offer a key on how to select an appropriate measure for given data and problem (check their Tables 7.4-7.6). If you uncertain, then choose several distance measures and compare the results. Euclidean Distance Perhaps the mostly commonly used, and easiest to understand, dissimilarity, or distance, measure is Euclidian distance. This metric is zero for identical sampling units and has no fixed upper bound. Euclidean distance in multivariate space is derived from our understanding of distance in a Cartesian plane. If we had two species abundances measured in two different samples, we could then plot the abundance of species 1 and species 2 for each sample on a 2D plane, and draw a line between them. This would be our Euclidean distance: the shortest path between the two points (Fig. 6.2). Figure 6.2: Euclidean Distance We know that to calculate this distance we would just use the Pythagorean theorem as \\(c=\\sqrt{a^2+b^2}\\). To generalize to n species we can say \\(D^E_{jk}=\\sqrt{\\sum^n_{i=1}(X_{ij}-X_{ik})^2}\\), where Euclidean distance between samples j and k, \\(D^E_{jk}\\), is calculated by summing over the distance in abundance of each of n species in the two samples. Let’s try an example. Given the species abundances in Table 6.4, we can calculate the squared difference in abundance for each species, and sum that quantity. Table 6.4: Species abundance and distance calculations for two samples sample j sample k \\((X_j-X_k)^2\\) Species 1 19 35 256 Species 2 35 10 625 Species 3 0 0 0 Species 4 35 5 900 Species 5 10 50 1600 Species 6 0 0 0 Species 7 0 3 9 Species 8 0 0 0 Species 9 30 10 400 Species 10 2 0 4 TOTAL 131 113 3794 Then all we need to do is to take the square root of the sum to obtain the Euclidean distance. Did you get the correct answer of 61.6? Of course, R makes this much easier, I can calculate Euclidan distance using the dist() function, after creating a matrix of the two columns of species abundance data from my original eu dataframe. dist(rbind(eu$j[1:10], eu$k[1:10]), method = &quot;euclidean&quot;) 1 2 61.59545 There are many other quantitative dissimilarity metrics. For example, Bray Curtis dissimilarity is frequently used by ecologists to quantify differences between samples based on abundance or count data. This measure is usually applied to raw abundance data, but can be applied to relative abundances. It is calculated as: \\(BC_{ij}=1-\\frac{C_{ij}}{S_{i}+S_{j}}\\), where \\(C_{ij}\\) is the sum over the smallest values for only those species in common between both sites, \\(S_{i}\\) and \\(S_{j}\\) are the sum of abundances at the two sites. This metric is directly related to the Sørenson binary similarity metric, and ranges from 0 to 1, with 0 indicating complete similarity. This is not at distance metric, and so, is not appropriate for some types of analysis. 6.1.3 Comparing more than two communities/samples/sites/genes/species What about the situation where we want to compare more than two communtiies, species, samples or genes? We can simply generate a dissimilarity or similarity matrix, where each pairwise comparison is given. In the species composition matrix below (Table 6.5), sample A and B do not share any species, while sample A and C share all species but differ in abundances (e.g. species 3 = 1 in sample A and 8 in sample C). The calculation of Euclidean distance using the dist() function produces a lower triangular matrix with the pairwise comparisons (I’ve included the distance with the sample itself on the diagonal). The Euclidan distance values suggest that A and B are the most similar! Euclidean distance puts more weight on differences in species abundances than on difference in species presences. As a result, two samples not sharing any species could appear more similar (with lower Euclidean distance) than two samples which share species that largely differ in their abundances. Table 6.5: Species abundance versus species presence and Euclidean distance sample A sample B sample C species 1 0 1 0 species 2 1 0 4 species 3 1 0 8 dist(t(meu[2:4]), method=&quot;euclidean&quot;, diag=TRUE) A B C A 0.000000 B 1.732051 0.000000 C 7.615773 9.000000 0.000000 There are other disadvantages as well, and in general, there is simply no perfect metric. For example, you may dislike the fact that Euclidean distance also has no upper bound, and so it becomes difficult to understand how similar two things are (i.e., the metric can only be understood in a relative way when comparing many things, Sample A is more similar to sample B than sample C, for example). You could use a Bray-Curtis dissimilarity metric, which is quite easy to interpret, but this metric will also confound differences in species presences and differences in species counts (Greenacre 2017). The best policy is to be aware of the advantages and disadvantages of the metrics you choose, and interpret your analysis in light of this information. 6.1.4 R functions There are a number of functions in R that can be used to calculate similarity and dissimilarity metrics. Since we are usually not just comparing two objects, sites or samples, these functions can help make your calculations much quicker when you are comparing many units. dist() offers a number of distance measures (e.g. euclidean,canberra and manhattan). The result is the distance matrix which gives the dissimilarity of each pair of objects, sites or samples. the matrix is an object of the class dist in R. vegdist() (library vegan). The default distance used in this function is Bray-Curtis distance, which is considered more suitable for ecological data. dsvdis() (library labdsv) Offers some other indices than vegdist (e.g. ruzicka (or Růžička), a quantitative analogue of Jaccard, and roberts. For full comparison of dist, vegdist and dsvdis,see http://ecology.msu.montana.edu/labdsv/R/labs/lab8/lab8.html. dist.ldc() (library adespatial) Includes 21 dissimilarity indices described in Legendre &amp; De Cáceres (2013), twelve of which are not readily available in other packages. Note that Bray-Curtis dissimilarity is called percentage difference (method = “percentdiff”). designdist() (library vegan) Allows one to design virtually any distance measure using the formula for their calculation. daisy() (library cluster) Offers euclidean, manhattan and gower distance. distance() (library ecodist) Contains seven distance measures, but the function is more for demonstration (for larger matrices, the calculation takes rather long). 6.2 Cluster Analysis When we have a large number of things to compare, an examination of a matrix of similarlity or dissimilatiry metrics can be tedious or even impossible to do. One way to visualize the similarity among units is to use some form of cluster analysis. Clustering is the grouping of data objects into discrete similarity categories according to a defined similarity or dissimilarity measure. We can contrast clustering, which assumes that units (e.g., sites, communities, species or genes) can be grouped into discrete categories based on similarity, with ordination, which treats the similarity between units as a continuous gradient (we’ll discuss ordination in section 6.3). We can use clustering to do things like discern whether there are one or two or three different communities in three or four or five sampling units. It is used in many fields, such as machine learning, data mining, pattern recognition, image analysis, genomics, systems biology, etc. Machine learning typically regards data clustering as a form of unsupervised learning, or from our figure above (Fig 6.1), as a technique that uses “latent” variables because we are not guided by a priori ideas of which variables or samples belong in which clusters. 6.2.1 Hierarchical clustering: groups are nested within other groups. Perhaps the most familiar type of clustering is hierarchical. There are two kinds: divisive and agglomerative. In the divisive method, the entire set of units is divided into smaller and smaller groups. The agglomerative method starts with small groups of few units, and groups them into larger and larger clusters, until the entire data set is sampled (Pielou, 1984). Of course, once you have more than two units, you need some way to assess similarlity between the clusters. There are a couple of different methods here. Single linkage assigns the similairty between clusters to the most similar units in each cluster. Complete linkage uses the similarity between the most dissmilar units in each cluster, while average linkage averages over all the units in each cluster (Fig. 6.3). Figure 6.3: Different methods of determining similarity between clusters Single Linkage Cluster Analysis Single linkage cluster analysis is one of the easiest to explain. It is hierarchical, agglomerative technique. We start by creating a matrix of similarity (or dissimilarity) indices between the units we want to compare. Then we find the most similar pair of samples, and that will form the 1st cluster. Next, we find either: (a) the second most similar pair of samples or (b) highest similarity between a cluster and a sample, or (c) most similar pair of clusters, whichever is greatest. We then continue this process until until there is one big cluster. Remember that in single linkage, similarity between two clusters = similarity between the two nearest members of the clusters. Or if we are comparing a sample to a cluster, the similarity is defined as the similarity between sample and the nearest member of the cluster. cls=data.frame(a=c(5,6,34,1,12),b=c(10,5,2,3,4), c=c(10,59,32,3,40), d=c(2,63,10,29,45), e=c(44,35,40,12,20)) clsd=dist(t(cls), method=&quot;euclidean&quot;) round(clsd,0) a b c d b 33 c 60 71 d 76 76 36 e 51 62 48 66 Figure 6.4: Example of using a dissimilarity matrix to construct a single-linkage cluster diagra 6.2.2 R functions Agglomerative approach (bottom-up) hclust() calculates hierarchical cluster analysis and has it’s own plot function. agnes() (library cluster) Contains six agglomerative algorithms, some not included in hclust. Divisive approach (top-down) diana() 6.2.3 How many clusters? The hiearchical methods just keep going until all objects are included (agglomerative methods), or are each in their own group (divisive methods). However, neither endpoint is very useful. How do we select the number of groups? There are metrics and techniques to make this decision more objective, however, in this introduction, we’ll just mention that, for hierarchical methods, you can determine the number of groups a given degree of similarity, or set the number of groups and find the degree of similarity that results in that number of groups. Let’s try. We’ll use the cutree() function that works on cluster diagrams produced by the hclust() function (Fig. @ref{fig:hclustfig)). If we set our dissimilarity threshold at 40, we find that there are three groups: a&amp;b, c&amp;d, and e in its own group. Figure 6.5: Cluster diagram produced by the function hclust with cut-off line at euclidean distance=40 for group membership a b c d e 1 1 2 2 3 6.2.4 Other clustering methods There are other means of clustering data of course. Partitional clustering is the division of data objects into non-overlapping subsets, such that each data object is in exactly one subset 6.2.4.1 K-means clustering In one version of this, k-means clustering, each cluster is associated with a centroid (center point), and each data object is assigned to the cluster with the closest centroid. In this method, the number of clusters, K, must be specified in advance. Our method is: Choose the number of K clusters Select K points as the initial centroids Calculate the distance of all items to the K centroids Assign items to closest centroid Recompute the centroid of each cluster Repeat from (3) until clusters assignments are stable K-means has problems when clusters are of differing sizes and densities, or are non-globular shapes. It is also very sensitive to outliers. 6.2.4.2 Fuzzy C-Means Clustering In contrast to strict (or hard) clustering approaches, fuzzy (soft) clustering methods allow multiple cluster memberships of the clustered items. This is commonly achieved by assigning to each item a weight of belonging to each cluster. Thus, items at the edge of a cluster, may be in a cluster to a lesser degree than items at the center of a cluster. Typically, each item has as many coefficients (weights) as there are clusters that sum up for each item to one. 6.2.5 Exercise: Cluster analysis of isotope data Our first step is to download and import the dataset “Dataset_S1.csv” from Perkins et al. 2014 (see url below). This data contains δ15N and δ13C signatures for species from different food webs. Unfortunately, this data is saved in an .xlsx file. To read data into R one of the easiest options is to use the read.csv() function with the argument on a .csv file. These Comma Separated Files are one of your best options for reproducible research. They are human readable and easily handled by almost every type of software. In contrast Microsoft Excel uses a propriatory file format, is not fully backwards compatible, and although widely used, is not human readable. As a result, we need special tools to access this file outside of Microsoft software products We’ll download the data set using download.file(), and read it using the R library openxlsx (see example below).Once you have successfully read your data file into R, take a look at it! Type iso (or whatever you named your data object) to see if the data file was read in properly. Some datasets will be too large for this approach to be useful (the data will scroll right off the page). In that case, there are a number of commands to look at a portion of the dataset. You could use a command like names(iso). One of the best things to do is plot the imported data. Of course, this is not always possible with very large datasets, but this set should work. Use the plot() function plotting δ15N vs δ13C to take a quick look. library(openxlsx) urlj=&quot;https://doi.org/10.1371/journal.pone.0093281.s001&quot; download.file(urlj, &quot;p.xlsx&quot;) iso=read.xlsx(&quot;p.xlsx&quot;) plot(iso$N~iso$C, col=as.numeric(as.factor(iso$Food.Chain)),xlim=c(-35, 0), pch=as.numeric(as.factor(iso$Species)), xlab=&quot;δ13C&quot;, ylab=&quot;δ15N&quot;) legend(&quot;topright&quot;, legend=unique(as.factor(iso$Food.Chain)),pch=1, col=as.numeric(unique(as.factor(iso$Food.Chain))), bty=&quot;n&quot;, title=&quot;Food chain&quot;) legend(&quot;bottomright&quot;,legend=as.character(unique(as.factor(iso$Species))), pch=as.numeric(unique(as.factor(iso$Species))), bty=&quot;n&quot;) Figure 6.6: Isotope data from Perkins et al (2014) We are going to use this data set to see if a cluster analysis on δ15N and δ13C can identify the foodweb. That is we are going to see if the latent variables identified by our clustering method match up to what we think we know about the data. Our first step is to create a dissimilarity matrix, but even before this, we must select that part of the data that we wish to use, just the δ15N and δ13C data, not the other components of the dataframe. In addition, our analysis will be affected by the missing data. So let’s get remove those rows with missing data right now using the complete.cases() function. The function returns a value of TRUE for every row in a dataframe that no missing values in any column. So niso=iso[complete.cases(mydata),], will be a new data frame with only complete row entries. The function dist() will generate a matrix of the pairwise Euclidean distances between pairs of observations. Now that you have a dissimilarity matrix, you complete a cluster analysis. The function hclust() will produce a data frame that can be sent to the plot() function plotted to visualize the recommended clustering. The method used to complete the analysis is indicated below the graph. Please adjust the arguments of the function to complete a single linkage analysis (look at help(hclust)) to determine the method to do this). str(iso) &#39;data.frame&#39;: 165 obs. of 7 variables: $ Replicate : num 1 2 3 4 5 6 7 8 9 10 ... $ Food.Chain : chr &quot;Wheat&quot; &quot;Wheat&quot; &quot;Wheat&quot; &quot;Wheat&quot; ... $ Species : chr &quot;Plant&quot; &quot;Plant&quot; &quot;Plant&quot; &quot;Plant&quot; ... $ Tissue : chr &quot;Leaf&quot; &quot;Leaf&quot; &quot;Leaf&quot; &quot;Leaf&quot; ... $ Lipid.Extracted: chr &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... $ C : num -30.1 -31.7 -30.1 -30.9 -31 ... $ N : num -3.47 -2.68 3.42 1.27 6.2 ... diso&lt;-dist((iso[,c(&quot;C&quot;,&quot;N&quot;)]), method=&quot;euclidean&quot;) p=hclust(diso,method=&quot;single&quot;) plot(p, cex=0.5,main=&quot;&quot;) Figure 6.7: Cluster figure of isotope data from Perkins et al. 2014) When you graph your cluster using plot(), you notice that there are many individual measurements, but there are only a few large groups. Does it look like there is an outlier? If so, you may want to remove this point from the data set, and then rerun the analysis. The row numbers are used as labels by default, so this is easy to do (niso=niso[-5,]). Remember to remove the point from the dataframe that has all the food chain info in it, otherwise you will have problems plotting later. When you examine the data set, you noted that there are 4 Food.chain designations. We will use the cutree() function to cut our cluster tree to get the desired number of groups (4), and then save the group numbers to a new column in our original dataframe. For example, iso$clust&lt;- cutree(p,4).We can then plot the data using colours and symbols to see how well our clustering works niso=iso[complete.cases(iso),] niso=niso[-5,] net=niso[,c(&quot;C&quot;,&quot;N&quot;)] diso&lt;-dist((net), method=&quot;euclidean&quot;) p=hclust(diso, method=&quot;single&quot;) plot(p,labels=FALSE, main=&quot;&quot;) Figure 6.8: Single linkage clustering on Perkins et al (2014) data with outlier removed niso$clust&lt;-cutree(p,k=4) plot(iso$N~iso$C, col=as.numeric(as.factor(niso$clust)),xlim=c(-35, 0), pch=as.numeric(as.factor(niso$Species)), xlab=&quot;δ13C&quot;, ylab=&quot;δ15N&quot;) legend(&quot;topright&quot;, legend=unique(as.factor(niso$clust)),pch=1, col=as.numeric(unique(as.factor(niso$clust))), bty=&quot;n&quot;, title=&quot;cluster&quot;) legend(&quot;bottomright&quot;,legend=as.character(unique(as.factor(niso$Species))), pch=as.numeric(unique(as.factor(niso$Species))), bty=&quot;n&quot;,title=&quot;Species&quot;) Figure 6.9: Data from Perkins et al (2014) data with grouping from single linkage clustering superimposed It doesn’t look like our cluster algorithm is matching up with our Food.chain data categories very well. Wheat- and Nettle-based distinguished, which makes sense when you consider that both of these plants use a C3 photosynthesis system. If you are not happy with the success of this clustering algorithm you could try other variants(.g., “complete” linkage) and a different number of groups. Let’s try a non-hierarchical cluster analysis on the same data to see if it works better. The kmeans() function requires that we select the required number of clusters ahead of time (we want 4, so kclust=kmeans(niso[,c(“C,” “N”)], 4)), we can then save the assigned clusters to our dataframe and plot in a similar way Figure 6.10: K means clustering on Perkins et al (2014) data It looks like kmeans has the same problem with distinguishing C3 plant-based foodwebs. But we still get three groups that roughly map onto our information about the data. 6.3 Ordination While cluster analysis let’s us visualize multivariate data by grouping objects into dscrete categories, ordination uses continuous axes to help us accomplish the same task. Physicists grumble if space exceeds four dimensions, while biologists typically grapple with dozens of dimensions (species and/or samples). In effect, we “order” this multivariate data in order to produce a low dimensional picture (i.e., a graph in 1-3 dimensions). Just like cluster analysis, we will use similarity metrics to accomplish this. Also like cluster anlaysis, simple ordination is not a statistical test: it is a method of visualizing data. Essentially, we find axes in the data that explain a lot of variation, and rotate so we can use that axis as one of our dimensions of visual representation(Fig. 6.12). Another way to think about it, is that we are going to summarize the raw data, which has many variables, p, by a smaller set of synthetic variables, k (Fig. 6.11). If the ordination is informative, it reduces a large number of original correlated variables to a small number of new uncorrelated variables. But it really is a bit of a balancing act between clarity of representation, ease of understanding, and oversimiplication. We will lose information in this data reduction, and if that information is important, then we can make the multivariate data harder to understand! Also note that if the original variables are not correlated, then we won’t gain anything with ordinaton. Figure 6.11: Ordination as data reduction. We summarize data with many variables (p) by a smaller set of derived or synthetic variables (k) Figure 6.12: Synthetic axis rotation in ordination There are lots of different ways to perform an ordination, but most methods are based on extracting the eigenvalues of a similarity matrix. The four most commonly used methods are: Principle Component Analysis (PCA), which is the main eigenvector-based method, Correspondence Analysis (CA) which is used used on frequency data, Principle Coordinate Analysis (PCoA) which works on dissimilarity matrices, and Non Metric Multidimensional Scaling (NMDS) which is not an eigenvector method, instead it represents objects along a predetermined number of axes. Table 6.6: Domains of Application of Ordination Methods (adpated from Legendre &amp; Legendre 2012) Method Distance Variables Principal component analysis (PCA) Euclidean Quantitative data Correspondence analysis (CA) X^2 Non-negative, quantitiative or binary data; species frequencies or presence/absence data Principal coordinate analysis (PCoA), metric (multidimensional) scaling, classical scaling Any Quantitative, semiquantitative, qualitative, or mixed Nonmetric multidimensional scaling (nMDS) Any Quantitative, semiquantitative, qualitative, or mixed Legendre &amp; Legendre (2012) provide a nice summary of when you should use each method Table 6.6 6.3.1 Principal Components Analysis (PCA) Principal Components Analysis is probably the most widely-used and well-known of the standard multivariate methods. It was invented by Pearson (1901) and Hotelling (1933), and first applied in ecology by Goodall (1954) under the name “factor analysis” (NB “principal factor analysis” is also a synonym of PCA). Like most ordination methods, PCA takes a data matrix of n objects by p variables, which may be correlated, and summarizes it by uncorrelated axes (principal components or principal axes) that are linear combinations of the original p variables. The first k components display as much as possible of the variation among objects. PCA uses Euclidean distance calculated from the p variables as the measure of dissimilarity among the n objects, and derives the best possible k-dimensional representation of the Euclidean distances among objects, where \\(k &lt; p\\) . We can think about this spatially. Objects are represented as a cloud of n points in a multidimensional space with an axis for each of the p variables. So the centroid of the points is defined by the mean of each variable, and the variance of each variable is the average squared deviation of its n values around the mean of that variable (i.e., \\(V_i= \\frac{1}{n-1}\\sum_{m=1}^{n}{(X_{im}-\\bar{X_i)}^2}\\)). The degree to which the variables are linearly correlated is given by their covariances \\(C_{ij}=\\frac{1}{n-1}\\sum_{m=1}^n{(X_{im}-\\bar{X_i})(X_{jm}-\\bar{X_j})}\\). The objective of PCA is to rigidly rotate the axes of the p-dimenional space to new positions (principal axes) that have the following properties: they are ordered such that principal axis 1 (or the principal component has the highest variance, axis 2 has the next highest variance etc, and the covariance among each pair of principal axes is zero (the principal axes are uncorrelated) (Fig. 6.13). Figure 6.13: Selecting the synthetic axes in ordination So our steps are to compute the variance-covariance matrix of the data, calculate the eigenvalues of this matrix and then calculate the associated eigenvectors. Then, the jth eigenvalue is the variance of the jth principle component and the sum of all the eigenvalues is the total variance explained. The proportion of variance explained by each component is the eigenvalue for the component divided by the total variance explained, while the loadings are the eigenvectors. Dimensionality reduction is the same as first rotating the data with the eigenvalues to be aligned with the principle components, then using only the components with the greatest eigenvalues. 6.3.2 Exercise: PCA on the iris data Let’s try an example. We’re going to use a sample dataset in R and the base R version of PCA to start exploring this data analysis technique. Get the iris dataset into memory by typing “data(“iris”). Take a look at this dataset using the head(), str() or summary() functions. For a multivariate data set, you would like to take a look at the pairwise correlations. Remember that PCA can’t help us if the variables are not correlated. Let’s use the pairs() function to do this data(&quot;iris&quot;) str(iris); summary(iris[1:4]) &#39;data.frame&#39;: 150 obs. of 5 variables: $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Sepal.Length Sepal.Width Petal.Length Petal.Width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 pairs(iris[1:4],main=&quot;Iris Data&quot;, pch=19, col=as.numeric(iris$Species)+1) Figure 6.14: correlation atrix for the iris data The colours let us see the data for each species, the plots are the pairwise plotting of each pair of the 4 variables (Fig. 6.14). Do you see any correlations? If there seem to be some correlations we might use PCA to visualize the 4 dimensional variable space. Let’s rush right in and use the prcomp() function to run a PCA on the numerical data in the iris dataframe. Save the output from the function to a new variable name so you can look at it when you type that name. The str() function will show you what the output object includes. If you use the summary() function, R will tell you what proportion of the total variance is explained by each axis. There is a problem though, let’s examine the variance in the raw data. Use the apply() function to quickly calculate the variance in each of the numeric columns of the data as apply(iris[,1:3], 1, var). What do you see? Are the variances of each the columns comparable? pca &lt;- prcomp(iris[,1:4]) summary(pca) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 2.0563 0.49262 0.2797 0.15439 Proportion of Variance 0.9246 0.05307 0.0171 0.00521 Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 apply(iris[,1:4], 2, var) Sepal.Length Sepal.Width Petal.Length Petal.Width 0.6856935 0.1899794 3.1162779 0.5810063 Using covariances among variables only makes sense if they are measured in the same units, and even then, variables with high variances will dominate the principal components. These problems are generally avoided by standardizing each variable to unit variance and zero mean as \\(X_{im}^{&#39;}=\\frac{x_{im}-\\bar{X_i}}{sd_i}\\) where sd is the standard deviation of variable i. After standardizaton, the variance of each variable is 1 and the covariances of the standardized variables are correlations. If you look at the help menu, the notes for the use of prcomp() STRONGLY recommend standardizing the data. To do this there is a built in option. We just need to set scale=TRUE. Let’s try again with data standardization. Save your new PCA output to a different name. We’ll compare to the unstandardized data in a moment. Take a look at the summary. p &lt;- prcomp(iris[,1:4], scale=TRUE) summary(p) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 1.7084 0.9560 0.38309 0.14393 Proportion of Variance 0.7296 0.2285 0.03669 0.00518 Cumulative Proportion 0.7296 0.9581 0.99482 1.00000 Now we need to determine how many axes to use to interpret our analysis. For 4 variables it is easy enough to just look that the amount of variance. For larger numbers of variables a plot can be useful. The screeplot() function will output the variance explained by each of the principle component axes, and you can make a decision based on that (e.g., screeplot(pca2, type=“lines”)). An ideal curve should be steep, then bend at an “elbow” — this is your cutting-off point — and after that flattens out. To deal with a not-so-ideal scree plot curve you can apply the Kaiser rule: pick PCs with eigenvalues of at least 1. Or you can select using the proportion of variance where the PCs should be able to describe at least 80% of the variance. It looks like synthetic axes 1 &amp; 2 explain most of the variation. So let’s plot those out. A PCA plot displays our samples in terms of their position (or scores) on the new axes. We can add information about how much variation each axis explains, and colour our points to match species identity. In this 2D representation of 4 dimensional space, it looks like species versicolor and viriginica group together (Fig. 6.15). Figure 6.15: PCA plot for the iris data We can also plot information about influence the various characterisitics are having on each of the axes. The eigenvectors used for the rotation give us this information. So let’s just print that out Table 6.7: Eigenvectors (or ‘loadings’) for each variable and synthetic axis PC1 PC2 PC3 PC4 Sepal.Length 0.52 -0.38 0.72 0.26 Sepal.Width -0.27 -0.92 -0.24 -0.12 Petal.Length 0.58 -0.02 -0.14 -0.80 Petal.Width 0.56 -0.07 -0.63 0.52 We can see that a lot of information is coming from the petal variables for PC1, but less from the sepal variables (Table ??). We can plot this out to show how strongly each variable affects each principle component (or synthetic axis). We can see that petal width and length are aligned along the PC1 axis, while PC2 explains more variation in sepal width (Fig @ref(fig: loadplot)). To interpret the variable plot remember that positively correlated variables are grouped close together (e.g., petal length and width). Variables with about a 90 angle are probably not correlated, while negatively correlated variables are positioned on opposite sides of the plot origin (~180 angle; opposed quadrants). However, the direction of the axes is arbitrary! The distance between variables and the origin measures the contribution of the variables to the ordination. A shorter arrow indicates its less importance for the ordination. Variables that are away from the origin are well represented. Avoid the mistake of interpreting the relationships among variables based on the proximities of the apices (tips) of the vector arrows instead of their angles in biplots. Another way to portray this imformation is to create a biplot which, in addition to the coordinates of our samples on the synthetic axes PC1 and PC2, also provides information about how the variables align along the synthetic axes. (Fig (fig:pactwo). I should note that I have used an arbitrary scaling to display the variable loadings on each axis. Some of the R packages will use a specific scaling that will emphasize particular parts of the plot, either preserving the Euclidean distances between samples or the correlations/covariances between variables (e.g., vegan). Figure 6.16: PCA plot for the iris data Principal components analysis assumes the relationships among variables are linear, so that the cloud of points in p-dimensional space has linear dimensions that can be effectively summarized by the principal axes. If the structure in the data is nonlinear (i.e., the cloud of points twists and curves its way through p-dimensional space), the principal axes will not be an efficient and informative summary of the data. For example, in community ecology, we might use PCA to summarize variables whose relationships are approximately linear or at least monotonic (e..g, soil properties might be used to extract a few components that summarize main dimensions of soil variation). However, in general PCA is generally not useful for ordinating community data because relationships among species are highly nonlinear. This nonlinearity can leaad to characterisitc artifacts, where, for example, community trends along environmental gradients appear as “horseshoes” in PCA ordinations because low species density at opposite extremes of an environmental gradiant appear relatively close together. 6.3.3 Principle Coordinates Analysis (PCoA) The PCoA method may be used with all types of distance descriptors, and so might be able to avoid sum problems of PCA. Although, a PCoA computed on a Euclidean distance matrix gives the same results as a PCA conducted on the original data 6.3.3.1 R functions for PCoA cmdscale()(stats) -base R, no package needed smacofSym() (library smacof) wcmdscale()(vegan) pco()(ecodist) pco()(labdsv) pcoa()(ape) dudi.pco()(ade4) 6.3.4 Nonmetric Multidimensional Scaling (NMDS) Like PCoA, the method of nonmetric multidimensional scaling (nMDS), produces ordinations of objects from any resemblance matrix. However, nMDS compresses the distances in a non-linear way and its algorithm is computer-intensive, requiring more computing time than PCoA. PCoA is faster for large distance matrices. This ordinaton method does not to preserve the exact dissimilarities among objects in an ordination plot, instead it represent as well as possible the ordering relationships among objects in a small and specified number of axes. Like PCoA, nMDS can produce ordinations of objects from any dissimilarity matrix.The method can also cope with missing values, as long as there are enough measures left to position each object with respect to a few others. nMDS is not an eigenvalue technique, and it does not maximise the variability associated with individual axes of the ordination. In this computational method the steps are: Specify the desired number m of axes (dimensions) of the ordination. Construct an initial configuration of the objects in the m dimensions, to be used as a starting point of an iterative adjustment process. (tricky: end result may depend on this. A PCoA ordination may be a good start. Otherwise, try many independent runs with random initial configurations) Try to position the objects in the requested number of dimensions in such a way as to minimize how far the dissimilarities in the reduced-space configuration are from being monotonic to the original dissimilarities in the association matrix The adjustment goes on until the stress value cannot be lowered, or until it reaches a predetermined low value (tolerated lack-of-fit). Most NMDS programs rotate the final solution using PCA, for easier interpretation. NMDS often achieves a less deformed representation of the dissimilarity relationships among objects than a PCoA in the same number of dimensions. We can use a Shephard plot to get information about the distortion of representation. A Shepard diagram compares how far apart your data points are before and after you transform them (ie: goodness-of-fit) as a scatter plot. On the x-axis, we plot the original distances. On the y-axis, we plot the distances output by a dimension reduction algorithm. A really accurate dimension reduction will produce a straight line. However since information is almost always lost during data reduction, at least on real, high-dimension data, so Shepard diagrams rarely look this straight. Let’s try this for the iris data. We can evaluate the quality of the NMDS solution by checking the Shephard plot as : stressplot(nMDS, main = “Shepard plot”). In addition to the original dissimilarity and ordination distance, the plot displays two correlation-like statistics on the goodness of fit. The nonmetric fit is given by \\(R^2\\), while he “linear fit” is the squared correlation between fitted values and ordination distances (Fig. ??). There is some deformation here, but in general the representation is not so bad. Loading required package: permute Loading required package: lattice This is vegan 2.5-7 Run 0 stress 0.03775523 Run 1 stress 0.0553735 Run 2 stress 0.04367521 Run 3 stress 0.04804007 Run 4 stress 0.03775525 ... Procrustes: rmse 1.133709e-05 max resid 3.616212e-05 ... Similar to previous best Run 5 stress 0.03775522 ... New best solution ... Procrustes: rmse 7.621845e-06 max resid 7.564749e-05 ... Similar to previous best Run 6 stress 0.05918357 Run 7 stress 0.06031974 Run 8 stress 0.03775524 ... Procrustes: rmse 6.321677e-06 max resid 2.684336e-05 ... Similar to previous best Run 9 stress 0.04355784 Run 10 stress 0.04367522 Run 11 stress 0.05059727 Run 12 stress 0.03775521 ... New best solution ... Procrustes: rmse 3.167733e-06 max resid 1.37392e-05 ... Similar to previous best Run 13 stress 0.05361269 Run 14 stress 0.05317214 Run 15 stress 0.04804014 Run 16 stress 0.03775526 ... Procrustes: rmse 1.217405e-05 max resid 5.425564e-05 ... Similar to previous best Run 17 stress 0.03775524 ... Procrustes: rmse 8.064894e-06 max resid 3.494195e-05 ... Similar to previous best Run 18 stress 0.04804008 Run 19 stress 0.03775522 ... Procrustes: rmse 3.889243e-05 max resid 0.0001678761 ... Similar to previous best Run 20 stress 0.06144867 *** Solution reached nMDS often achieves a less deformed representation of the dissimilarity relationships among objects than a PCoA in the same number of dimensions. But nMDS is a computer-intensive iterative technique exposed to the risk of suboptimum solutions. In comparison, PCoA finds the optimal solution by eigenvalue decomposition. 6.3.4.1 R functions for NMDS metaMDS() (vegan) isoMDS( ) (MASS) 6.3.5 Exercise: Ordination We are going to use the vegan package, and some built in data with it to run the nMDS. varespec is a data frame of observations of 44 species at 24 sites. We’ll calculate both an NMDS and a PCoA using the (cmdscale() function) on the bray-curtis distance matrix of these data. In each case, we will specify that we want 2 dimensions as our output. library(vegan) data(varespec) disimvar=vegdist(varespec, method = &quot;bray&quot;) nMDS &lt;- metaMDS(varespec, distance=&quot;bray&quot;, k=2) Square root transformation Wisconsin double standardization Run 0 stress 0.1843196 Run 1 stress 0.2212141 Run 2 stress 0.2209227 Run 3 stress 0.2085515 Run 4 stress 0.1825658 ... New best solution ... Procrustes: rmse 0.0416743 max resid 0.1520615 Run 5 stress 0.195049 Run 6 stress 0.2398057 Run 7 stress 0.1955836 Run 8 stress 0.2114447 Run 9 stress 0.1967393 Run 10 stress 0.2419376 Run 11 stress 0.1825658 ... New best solution ... Procrustes: rmse 6.745298e-05 max resid 0.0002312025 ... Similar to previous best Run 12 stress 0.1845801 Run 13 stress 0.2141967 Run 14 stress 0.2166093 Run 15 stress 0.1843196 Run 16 stress 0.1948413 Run 17 stress 0.2048307 Run 18 stress 0.2234893 Run 19 stress 0.1985582 Run 20 stress 0.1825658 ... Procrustes: rmse 1.255124e-05 max resid 3.837387e-05 ... Similar to previous best *** Solution reached PCoA &lt;- cmdscale(disimvar, k = 2, eig = T, add = T ) If we look at the object PCoA we see eigenvalues, one for each of the 20 sites, and the new 2-D coordinates for each site. We can plot the results as plot(PCoA$points). In fact, let’s plot the PCoA and the NMDS side by side to see if they differ, using the par(mfrow()) functions. In this case, our species are the variables and our sites/samples are the objects of our attention. There’s a lot of species, so we won’t draw the arrows, we’ll just show their position on the biplot. "],["optimization.html", "7 Optimization 7.1 Introduction 7.2 Fundamentals of Optimization 7.3 Regression 7.4 Iterative Optimization Algorithms 7.5 Calibration of Dynamic Models 7.6 Uncertainty Analysis and Bayesian Calibration 7.7 References", " 7 Optimization 7.1 Introduction To improve is to make better; to optimize is to make best. Optimization is the act of identifying the extreme (cheapest, tallest, fastest…) over a collection of possibilities. You may recall studying optimization in introductory calculus where you may have solved simple design problems over the possible sizes of fences, or boxes, or something similar. Optimization over design space (also called decision space) is a critical feature of many engineering tasks, and has a role in most areas of applied science, including biology. Examples include optimal manipulation of biological systems (e.g. optimal harvesting or optimal drug dosing) or optimal design of biological systems (e.g. robust synthetic genetic circuits). A complementary task is optimal experimental design, which aims to identify the ‘best’ experiment from a collection of possibilities. Model calibration, to be discussed further below, provides another example; here we seek the ‘best fit’ version of a model from a collection of possible options. Beyond its use in design, optimization is also used to investigate natural systems (i.e. in `pure’ science), in cases where ‘nature’ presents us with an optimal version of a given phenomenon. For example, the principle of entropy maximization (equivalently energy minimization) justifies a wide variety of phenomena, from the shapes of soap bubbles to the configuration of proteins. Darwinian evolution provides another mechanism for optimization. Assuming evolution has arrived at optimal designs, we can apply optimization to understand a variety of biological phenomena, from metabolic flux distribution, to brain wiring, to optimal foraging strategies. This module introduces optimization methods in R. Applications and illustrations are be drawn from a range of biological domains. The simple optimization tasks addressed by introductory calculus can be accomplished with paper-and-pencil calculations. As shown below, while the fundamental principles introduced by those exercises carry forward, most optimization tasks of interest in biology demand more extensive computational resources. 7.2 Fundamentals of Optimization Figure 1 illustrates some basics terminology associated with optimization. The graph of a function \\(f\\) of a single variable \\(x\\) is shown, defined over a domain \\([a,b]\\). In the context of optimization, we can think of each \\(x\\)-value in the interval \\([a,b]\\) as one possible scenario (e.g. fence length, harvesting rate, etc.). The function \\(f\\) maps those choices to some objective (e.g. total area, long-term annual yield) that we wish to optimize (either maximize or minimize). The global extrema (i.e. maximum and minimum) represent the results we wish to achieve. Figure 1: Extreme Values More generally, we define local extrema (maxima and minima) as cases that appear to be optimal if we restrict our attention only to ‘nearby’ possibilities (i.e. \\(x\\)-values). There is an extensive theory of optimization methods; unfortunately most of these are dedicated to identifying local optima. These approaches cannot directly identify global optimum – instead they identify candidate global optima, which then can be compared to identify the ‘best’ result. (A convenient special case occurs for problems where every local optimum is a global optimum; these are called convex optimization problems. Unfortunately, they occur only rarely in addressing biological phenomena.) 7.2.1 Fermat’s Theorem To illustrate these fundamentals, consider the following two academic examples that rely on basic calculus, specifically on Fermat’s Theorem, which states that local extrema occur at points where the tangent line to a function’s graph is horizontal, i.e. at points where the derivative (i.e. the slope of the tangent) is zero (Figure 2). Figure 2: Fermat’s Theorem: local extrema occur at points where the tangent line is horizontal. Some local extrema are also global extrema. Example 1. Identify the value of \\(x\\) for which the function \\(f(x) = x^2+3x-2\\) is minimized. Solution: Taking the derivative, we find \\(f&#39;(x) = 2x+3\\). To find the point(s) where the derivative is zero, we solve: \\(f&#39;(x) = 0 \\Leftrightarrow 2x+3 = 0\\Leftrightarrow x = -\\frac{3}{2} = -1.5\\). As shown in Figure 4, the single local minimum is the global miminum in this case, so \\(x=-1.5\\) is the desired solution. Figure 3: Graph of \\(f(x) = x^2+3x-2\\) (Example 1) Example 2. Identify the value of \\(x\\) at which the function \\(f(x) = 3x^4-4x^3-54x^2+108x\\) is minimized. Solution: Taking the derivative, we find \\(f&#39;(x) = 12x^3-12x^2-108x+108 = 12(x-3)(x-1)(x+3)\\). To find the point(s) where the derivative is zero, we solve: \\(f&#39;(x) = 0 \\Leftrightarrow x = 3, 1, -3\\). As shown in Figure 4, two of these points are local minima. One (\\(x=-3\\)) is where the global minimum occurs. Figure 4: Graph of \\(f(x) = 3x^4-4x^3-54x^2+108x\\) (Example 2) 7.3 Regression 7.3.1 Linear Regression As mentioned above, finding the ‘best fit’ from a family of models is a common optimization task in science. The simplest example of this task is linear regression: determining a line of best fit through a given set of points. To illustrate, consider the dataset of \\((x,y)\\) pairs shown in Figure XYZ, which we can label as \\((x_1, y_1)\\), \\((x_2, y_2)\\), ldots, \\((x_N, y_N)\\). Several lines are displayed. The optimization task here is to identify the ‘best’ line: the one that best captures the trend in the data. To specify this task mathematically, we need to decide on a measure of ‘quality of fit.’ We start by recognizing that the line will (typically) fail to pass through most of the points in the dataset. Thus, at each point \\(x_i\\) we can define an error, which is the difference between the observed \\(y\\)-value and the \\(y\\)-value on the line. If we call the line \\(y=mx+b\\), then the error at \\(x_i\\) can be defined as \\(e_i = y_i-(mx_i+b)\\). We then need to combine these errors into a single measure of teh quality of fit. This is typically done by squaring the errors and adding them together. (Squaring ensures that both under- and over-estimations contribute equally.) We define the sum of squared errors (SSE) as : \\((y_1-(mx_1+b))^2 + (y_2-(mx_2+b))^2 + \\cdots +(y_N-(mx_N+b))^2 = e_1^2+e_2^2+ \\cdots e_N^2\\). We can now pose the model-fitting task as an optimization problem. For each line (that is, each assignment of numerical values to \\(m\\) and \\(b\\)), we associate a corresponding SSE. We seek the values of \\(m\\) and \\(b\\) for which the SSE is a global minimum. Example 3. Consider a simplified version of linear regression, in which we know that our model (line) should pass through the origin (0,0). That is, instead of lines \\(y=mx+b\\), we will consider only lines of the form \\(y=mx\\). (We thus have a single parameter to identify: the slope \\(m\\).) To keep the algebra as simple as possible we’ll take a tiny dataset consisting of just two points: \\((x_1, y_1) = (2,3)\\) and \\((x_2, y_2)= (5.4)\\), as indicated in Figure 5. The line passes through points \\((2,2m)\\) and \\((5, 5m)\\). Figure 5: Example 1: finding a line of best fit through the origin In this simple case, the sum of squared errors is: \\[\\begin{equation*} \\mbox{SSE} = e_1^2+e_2^2 = (3-2m)^2+(4-5m)^2 \\end{equation*}\\] To apply Fermat’s theorem we take the derivative and identify any values of \\(m\\) for which it is zero: \\[\\begin{eqnarray*} \\frac{d}{dm} \\mbox{SSE} &amp;=&amp; 2(3-2m)(-2)+2(4-5m)(-5) \\\\ &amp;=&amp; -4(3-2m) - 10(4-5m) = -12 +8m+-40+50m = -52+58m. \\end{eqnarray*}\\] The only local extremum (and hence the only candidate for global minimum) is then \\(m=52/58 = 26/29\\). The analysis in Example 3 can be extended to determine the general solution of the linear regression task [Fairway, 2002]. The solution formula is as follows: Linear regression formula The best fit line \\(y=mx+b\\) to the dataset \\((x_1, y_1)\\), \\((x_2, y_2)\\), , \\((x_n, y_n)\\) is given by \\[\\begin{eqnarray*} m&amp;=&amp; \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\\\ b&amp;=&amp; \\bar{y}-m\\bar{x}, \\\\ \\end{eqnarray*}\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) are the averages \\[\\begin{eqnarray*} \\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i \\ \\ \\ \\bar{y}=\\frac{1}{n} \\sum_{i=1}^n y_i \\end{eqnarray*}\\] This formula is somewhat unwieldy, but is straightforward to implement. In R, the command lm implements this formula, as the following example illustrates. To plot your dataset, use the plot() command. In this example, we will use the cars dataset and plot a distance vs. speed graph. The code used will be plot(cars). plot(cars) By using the lm() command with the two variables we get the intercept and the slope of the line. The code used will be lm(dist~speed). attach(cars) lm(dist~speed) Call: lm(formula = dist ~ speed) Coefficients: (Intercept) speed -17.579 3.932 As shown above, the intercept is -17.579 and the slope of the line is 3.932. There are two ways to add the line of best fit to the plot. The first is using the command abline(-17.579, 3.932), and the second method is to use abline(lm(dist~speed)). attach(cars) The following objects are masked from cars (pos = 3): dist, speed plot(cars) abline(lm(dist~speed)) 7.3.2 Nonlinear Regression Nonlinear regression is the task of fitting a nonlinear model (e.g. a curve) to a dataset. The setup for this task is identical to linear regression. We begin by selecting a parameterized family of models (i.e. curves), and aim to identify the model (i.e. curve) that minimizes the sum of squared errors when compared to the data. The family of models can be chosen based on an understanding of the mechanism that relates the input and output data. For example, if we are investigating the rate law for a single-substrate enzyme-catalysed reaction, we might choose the family of curves specified by Michaelis-Menten kinetics: \\[\\begin{eqnarray*} y = \\frac{V_{\\mbox{\\tiny max}} S}{K_M+S} \\end{eqnarray*}\\] Our goal would then be to identify values for the parameters \\(V_{\\mbox{\\tiny max}}\\) and \\(K_M\\) that minimize the sum of squared errors when comparing with observed data. Regression via linearizing transformation In several important cases, the nonlinear regression task can be transformed into a linear regression task. In the case of Michaelis-Menten kinetics, several linearizing transformations have been proposed (e.g. Eadie–Hofstee and Lineweaver–Burk [Cho and Lim, 2018]). Another example commonly encountered in Biology is fitting exponential curves (e.g. population growth or drug clearance). In those cases, a logarithm modifies the data so that a linear trend is captured: e.g. \\(y = e^{rt}\\), after applying a logarithm becomes \\(y^{trans} = \\ln(y) = \\ln (e^{rt})= rt\\). Linear regression on transformed data \\((t_i, y^{trans}_i)\\) then provides an estimate of the value of \\(r\\). Unfortunately, linearizing transformations are only available in a handful of special cases. In general, the nonlinear regression task must be addressed directly. An example of the procedure in R follows: We need to provide the starting values \\(V_m\\) and \\(K\\). \\(V_m\\) is the maximum value, the asymoptote of the curve. The value of \\(K_m\\) is equal to \\(S\\) when \\(V_m\\) is half-way (\\(\\frac12 V_m\\)). Formula: v ~ Vm * S/(K + S) Parameters: Estimate Std. Error t value Pr(&gt;|t|) K 0.4398016 0.0311612 14.11 8.1e-11 *** Vm 0.0054252 0.0001193 45.47 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.0001727 on 17 degrees of freedom Number of iterations to convergence: 7 Achieved convergence tolerance: 4.666e-07 Below is the plot of the original points and the fitted curve using nls(). From that exercise, the reader might have the impression that nonlinear regression and linear regression are very similar exercises. That would be a false impression. Although the problem set-up is similar (chose family of curves, minimize SSE), the optimization task is different, and it’s ‘solution’ follow a very different protocol. As we saw above, the solution to the linear regression task can be stated as a general formula. For nonlinear regression, no such formula exists. Worse, there is no procedure (algorithm) that is guaranteed to find the solution! 7.4 Iterative Optimization Algorithms The best techniques for addressing the general nonlinear regression task are iterative global optimization routines. As we’ll cover below, these algorithms all follow a basic idea: we start with an initial ‘guess’ of what the solution (best-fit parameters) might be, and then take steps through the parameter space to improve the quality of the solution. In the exercise above, the nls command executed a particularly simple version of this kind of algorithm. That’s why the nls requires that the user supply an ‘initial guess.’ This function also allows optional arguments that specify the number of steps (iterations) that the algorithm will take, and also the specific algorithm to be used (from a pre-specified set of choices). 7.4.1 Gradient Descent Perhaps the simplest iterative optimization algorithm is gradient descent, which can be understood intuitively in terms of finding your way to a valley bottom in a think fog. The fog obscures your vision so that you can only detect changes in elevation in your immediate vicinity. To make your way to the valley bottom, it would be reasonable to take each step of your journey in the direction of steepest decline. This strategy, illustrated in figure XYZ, is guaranteed to lead to a local minimum, but cannot guarantee arrival at the lowest point: the global minimum. Mathematically, the local change in elevation is determined by evaluating the objective at points nearby the ‘current position,’ and then using those to determine the direction of steepest descent. (technically, this involves a linearization of the function at the current position, or equivalently a calculation of the ‘gradient vector.’) A ‘step’ is then taken in the direction, and the process is repeated form this new ‘current position.’ Beyond that basic idea, a number of details have to be specified: how long should each step be? How many steps should be taken? Or should there be some other ‘termination condition’ that will trigger the end of the journey? (Each of these involve a tradeoff: often of precision vs. execution time. For instance, small steps guarantee a smooth path down the steepest route (as water would follow), but can take a very long time to complete the journey. Termination conditions are often specified in terms of the local topography: the algorithm stops when the ‘current position’ is at a sufficiently flat point (no downhill direction detected). First, let’s work with a simple function with global minimum only. We start from different initial points and plot the iteration steps to see if they result in the same spot. It can be seen from the plot that the global minimum found are the same no matter where you start. Then let’s take a look at a more complex function which has local minimum and global minimum. It can be seen that different starting points give different results. Choice of starting point matters, and it will not always give the optimal value (global minimum). Below is the plot of the same function from a different perspective to better review the structure. The interactive plot below allows you play around with the function. The default optimization algorithm used by nls is the Gauss-Newton method, which is a generalization of Newton’s method for solving nonlinear equations (which may be familiar from an introductory calculus). This is a refinement of gradient descent in which the local curvature of the function is used to project the position of the bottom of the local valley assuming that the surface is shaped like a parabola. On parabolic surfaces, this achieves descent to the bottom in one iteration. On non-parabolic surfaces (i.e.~most surfaces), the results is a sequence of iterations, but often many fewer than would be required using gradient descent. Both gradient descent and Gauss-Newton method are design to reach the bottom of the valley in which the initial guess lies. If that’s a local minimum but not the global minimum, then the algorithm will not be successful. So, how does one select a ‘good’ initial guess? Unfortunately, there’s no general answer to this question. In many cases, one can use previous knowledge of the system under investigation to begin with a solid initial guess. (This is closely related to the choice of a prior distribution in Bayesian approaches.) If no such previous knowledge is available, sometimes a `guess’ is all we have. In those cases, we may have little confidence that the algorithm will arrive at a global minimum. The simplest way to gain some confidence of achieving a global minimum is the multi-start strategy: choose many initial guesses, and run the algorithm from each. This can be computationally expensive, but if the same minimum is reached from initial spread widely over the parameters space, one begins to gain confidence that this is the true global minimum. A number of methods have been developed to complement the multi-start approach. These are known as global optimization routines. They are also known as heuristic methods, because their performance cannot be guaranteed in general: there are no guarantees that they’ll find the global minimum, nor are there solid estimates of how many iterations will be required for them to carry out a satisfactory search of the parameter space. We will consider two commonly used methods: simulated annealing and genetic algorithms. 7.4.1.1 Simulated Annealing Simulated annealing is motivated by the process of annealing: a heat treatment by which the physical properties of metals can be altered. This is an iterative algorithm; as in gradient descent, the algorithm starts at an initial guess, and then steps from point to point in the search space. However, there is a random element to the process followed by simulated annealing: that means that the path followed from a particular initial condition won’t be repeated if the algorithm is run again from the same point. (Algorithms that incorporate randomness are often referred to as Monte Carlo methods, after the European gambling centre.) At each step, the algorithm begins by identify a candidate next position. (This point could be selected by a variant of the gradient descent step, or some other method). The value of the objective at this candidate point is then compared with the objective value at the current point. If the objective is lower at the new point (i.e.~this step takes the algorithm downhill), then the step is taken and a new iteration begins. If the value of the objective is larger at the candidate (i.e.~the step makes things worse), the step can still be taken, but only with a small probability. Both the size of the candidate steps and the probability of accepting ‘wrong’ (uphill) steps are tied to a cooling schedule: a decreasing `temperature’ profile: at high temperatures, large steps are considered and ‘wrong’ steps are taken frequently; as the temperature drops, only smaller steps are considered, and fewer ‘wrong’ steps are allowed. (By analogy, imagine a ping-pong ball resting on a table into which has been carved a rolling landscape. One strategy to move the ball to the lowest valley bottom is to shake the table to jostle it about. Mirroring simulated annealing, we would begin by applying violent shakes (high temperature) which would result in the ball bouncing across much of the table. By slowly reducing the severity of the shaking, the ball would settle into a local minimum at a valley bottom. The hope is that if the cooling schedule is well-chosen, the ball would have sampled many valleys, and would end up at the bottom of the lowest. Simulated annealing is often combined with a multi-start strategy to further ensure wise sampling of the search space. Let’s first solve the complicated function above using simulated annealing using the same initial points as above. $value [1] -4.906786 $par [1] 2.0000000 0.6014984 $counts [1] 52490 $value [1] -4.906786 $par [1] 2.0000000 0.6014984 $counts [1] 54649 $value [1] -4.906786 $par [1] 2.0000000 0.6014984 $counts [1] 53020 $value [1] -4.906786 $par [1] 2.0000000 0.6014984 $counts [1] 52325 $value [1] -4.906786 $par [1] 2.0000000 0.6014984 $counts [1] 52470 From the results, we can see that although the initial points are different, they ended up with the same optimized value. The plot below shows how simulated annealing works. The red line records the current minimum. The blue points represents function value. It can be seen that although the actual minimum is found, there is still a probability to jump around to make sure that is the global minimum. The example below looks like an egg carton, which has many local minima. Here we started from two different starting points and they resulted in the same final position. $value [1] -306.7205 $par [1] -76.14595 191.15515 $counts [1] 51555 $value [1] -306.7205 $par [1] -76.14595 191.15515 $counts [1] 52120 The plot below shows how simulated annealing works for the two initial points respectively. The red line records the current minimum. The blue points represents function value. Although they have different starting position, it can be seen from the plots the optimized values are the same. We next consider a heuristic algorithm in which multiple paths through the search space are followed simultaneously. 7.4.1.2 Genetic Algorithms Genetic algorithms are inspired by Darwinian evolution. The algorithm begins with the specification of a population of initial guess points. At each iteration of the algorithm, this population ‘evolves’ toward improved estimates of the global minimum. This ‘evolution’ step involves three substeps: selection, mutation, and cross-over. In the selection step, the population is pruned by removing a fraction that are not sufficiently fit (where fitness corresponds to the value of the objective function being minimized). Then, mutations are introduced into the remaining population by introducing small random perturbations to their position in the search space. Finally, a new generation is generated by crossing members of the current population; this can be done in several ways, the simplest is to generate crosses as averages of the numerical values of the two ‘parents.’ The expectation is that, through several generations, this process will lead to a population with high fitness (minimal objective) that has thoroughly explored the search space (and may consist of subpopulations at local minima as well as representatives at the global minimum. Genetic algorithms are a subest of the more general class of evolutionary algorithms all of which involve simultaneous ‘exploration’ of the search space through multiple paths. We use the complicated function example with genetic algorithm. The built-in ga() function maximizes the objective function. In this case, to get the global minimum, a negative sign needs to be added in front of the function.The larger the maximum iteration is, the closer the value is to the actual global minimum. The seed can also be set to different values, but the final optimized value would be the same. The plot shows how optimal value converges. Although it is shown as positive values, we need to manually add a negative sign. From the summary below, the fitness function value and the parameters are very close to what we got before using simulated annealing. ── Genetic Algorithm ─────────────────── GA settings: Type = real-valued Population size = 50 Number of generations = 3000 Elitism = 2 Crossover probability = 0.8 Mutation probability = 0.1 Search domain = x1 x2 lower -2 -2 upper 2 2 GA results: Iterations = 3000 Fitness function value = 4.900531 Solution = x1 x2 [1,] 1.99625 0.5972324 Example: maybe biexponential or not, depending on complexity of cost surface. 7.5 Calibration of Dynamic Models The principles of nonlinear regression carry over directly to calibtration of more complex models. In particular, in many domains of biology, dynamic models are used to describe the time-varying behaviour of systems (from biomolecular networks to cell-cell interactions to physiology to ecology). These models take many forms, but a commonly used formulation is a model based on ordinary differential equations (i.e. rate equations). These models are deterministic (cannot incorporate random effects) and assume that the dynamics occur in a spatially homogeneous environment (they capture spatially distributed phenomena). Despite these limitations, these models can describe a wide variety of dynamic behaviours, and so are useful starting points for investigations across biology. Ordinary differential equation models used in biology often take the form \\[\\begin{equation*} \\frac{d}{dt} {\\bf x}(t) = {\\bf f}({\\bf x}(t), {\\bf p}) \\end{equation*}\\] where components of the time-varying vector \\({\\bf x}(t)\\) are the states of the system (e.g.~population sizes, molecular concentrations), the components of vector \\({\\bf p}\\) are the model parameters: numerical values that represent fixed features of the system and its environment (e.g. interaction strengths, temperature, nutrient availability), and the vector-valued function \\({\\bf f}\\) describes the rate of change of the state variables. As a concrete example, consider the Lotka-Volterra equations, a classical model to describe interacting predator and prey populations [ref]: \\[\\begin{eqnarray*} \\frac{d}{dt} x_1(t) &amp;=&amp; p_1 x_1(t) - p_2 x_1(t) x_2(t)\\\\ \\frac{d}{dt} x_2(t) &amp;=&amp; p_3 x_1(t) x_2(t) - p_4 x_2(t)\\\\ \\end{eqnarray*}\\] Here \\(x_1\\) is the size of the prey population; \\(x_2\\) is the size of the predator population. The prey are presumed to have access to resources that allow exponential growth in the absence of predation (growth at rate \\(p_1 x_1(t)\\). Interactions between prey and predator populations (assumed to occur at rate \\(x_1(t) x_2(t)\\) lead to an decrease in the prey population and an increase in the predator population (characterized by parameters \\(p_2\\) and \\(p_3\\) respectively). Finally, the prey suffer an exponential decline in population size in the absence of prey (decay rate \\(p_4 x_2(t)\\)). A simulation of the model is shown in Figure. Note that simualtion of the model requires specification of (i) values for each of the model parameters, and (ii) initial conditions, i.e.~the size of each population at time \\(t=0\\). Figure XYZ shows a dataset the corresponds to observations of a predator-prey population system. To calibrate the model Lotka-Volterra to this dataset we seek values for the four parameters \\(p_1\\), \\(p_2\\), \\(p_3\\), and \\(p_4\\) for which simulations of the model provide the ‘best fit.’ As in the regression tasks described previously, the standard measure for quality of fit is the sum of squared errors. We thus proceed with a minimization task: for each simulation of the model, we compare with the dataset and determine the sum of squared errors. We aim to minimize this fit over the space of model parameters. There’s one additional feature we must account for: to specify a simulation we need numerical values for the model parameters and the initial conditions. Calling these initial conditions \\(p_5 = x_1(0)\\) and \\(p_6=x_2(0)\\), we are then pose our optimization problem as a search over a six-dimensional parameter space. In what follows, we illustrate the use of both simulated annealing and a genetic algorithm to calibrate this model. Matt: insert example 7.6 Uncertainty Analysis and Bayesian Calibration The regression tasks discussed above resulted in estimates of parameter values based on the provided datasets. Regression is usually followed by uncertainty analysis, which provides some measure of confidence in those estimated parameter values. For instance, in the case of linear regression, 95% confidence intervals on the estimates (best fit line slope and intercept) and corresponding confidence intervals on the model predictions are supported by extensive statistical theory, and can be easily generated in R. The plot below shows 95% confidence intervals on the model predictions. The confint() function finds 95% confidence intervals on the estimates (best fit line slope and intercept). 2.5 % 97.5 % (Intercept) -31.167850 -3.990340 speed 3.096964 4.767853 This can also be derived using the confidence interval formula: \\(\\text{estimate}\\pm \\text{critical value}\\times\\text{standard error}\\) lower upper 3.096964 4.767853 lower upper -31.16785 -3.99034 For nonlinear regression (including calibration of dynamic models), the theory is less helpful, but estimates of uncertainty intervals can be achieved (as discussed below in the section on optimal experimental design). Waiting for profiling to be done... 2.5% 97.5% K 0.379645262 0.508891354 Vm 0.005184292 0.005681209 lower upper 0.005173515 0.005676949 lower upper 0.3740573 0.5055459 An alternative approach to the regression task combines calibration and uncertainty in a single process: Bayesian methods. The basic idea behind Bayesian analysis (founded on Bayes Theorem, which may be familiar from elementary probability), is to start with an initial parameter estimate (analogous to the initial guess needed in nonlinear regression) and then use the available data to refine that estimate. The difference is that instead of the initial guess and refined estimate being single numerical values, they are distributions. In Bayesian terminology, we being with a prior distribution, which may be informed by expert knowledge (e.g.~a normal distribution centered at a good initial guess), or may be a wilde guess (e.g.~a uniform distribution over a wide range of possible values). Application of a Bayesian calibration scheme uses the available data to generate an improved description of the parameter value distributions, called the posterior distribution. A successful Bayesian calibration could take a `wild guess’ uniform prior and return a tightly-centered posterior, as in figure XYZ. Uncertainty can then be gleaned directly from the posterior, by reporting, e.g.~ a 95% confidence interval. One commonly cited concern with Bayesian techniques is the dependence of the result on the rather subjective selection of a prior. This is analogous to concerns about any bias introduced by choice of initial guess in the non-Bayesian (maximum likelihood-based) regression analysis described above. In both cases, multi-start techniques can be used to address this issue. Here we’ll consider a simple numerical implementation of a Bayesian approach: approximate Bayesian computation. This approach is based on a simple idea: the rejection method, in which we sample repeatedly from the prior distribution and reject all samples that do not satisfy a pre-specified tolerance for quality of fit. (This is reminiscent of the selection step in genetic algorithms: culling unfit members of a population.) For the Michaelis-Menten example, the estimated values for \\(K\\) and \\(V_m\\) are 0.4398016 and 0.0054252 respectively. Now we simulate numbers for \\(K\\) and \\(V_m\\), and accept the value if the distances between the simulated values and the true values are under the threshold. Here we randomly sample \\(K\\) and \\(V_m\\) 200000 times, it can be seen from the histograms that the values that occur most frequently are close to the estimated values derived from the non linear regression model. [1] 867 Typically, approximate Bayesian computation is implemented as an iterative method, in which a sequence of rejection steps is applied, withe the prior being refined at each step (generating, in essence, a sequence of posterior distributions, the last of which is considered to be the best description of the desired parameter estimates). After 100000 simulations, it can be seen from the histograms that the most frequent values are close to the estimated values derived from the non linear regression model. 7.7 References Ashyraliyev, M., Fomekong‐Nanfack, Y., Kaandorp, J. A., &amp; Blom, J. G. (2009). Systems biology: parameter estimation for biochemical models. The FEBS journal, 276(4), 886-902. Beaumont, Mark A. “Approximate bayesian computation.” Annual review of statistics and its application 6 (2019): 379-403. Cho, Yong-Soon, and Hyeong-Seok Lim. “Comparison of various estimation methods for the parameters of Michaelis-Menten equation based on in vitro elimination kinetic simulation data.” Translational and clinical pharmacology 26.1 (2018): 39-47. Fairway, Julien, (2002) Practical Regression and Anova using R. https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf "]]
